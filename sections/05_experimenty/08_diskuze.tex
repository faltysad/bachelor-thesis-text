\section{Diskuze}
\label{sec:vae_model_discussion}

So far, all of our work on autoencoders and variational autoencoders has been limited
to a latent space with two dimensions. This has helped us to visualize the inner work‐
ings of a VAE on the page and understand why the small tweaks that we made to the
architecture of the autoencoder helped transform it into a more powerful class of net‐
work that can be used for generative modeling.

Let’s now turn our attention to a more complex dataset and see the amazing things
that variational autoencoders can achieve when we increase the dimensionality of the
latent space.