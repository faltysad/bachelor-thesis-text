\section{Model využívající latentních proměnných}
\label{sec:latent_variable_model}
Čím komplexnější jsou závislosti mezi dimenzemi generativního modelu, tím složitější je jeho trénování.

Pro jednoduchost je uvedena ilustrační úloha generativního modelování obrazových dat ručně psaných číslic 0-9.

Intuitivně je  vhodnou strategií, aby model nejprve učinil rozhodnutí o tom, jakou číslici se snaží generovat.
Takové rozhodnutí nazýváme \emph{latentní proměnnou}. Tedy, před tím, než model začne generovat obrázek, náhodně vybere vzorek $\emph{z}$\footnote{Později je ukázáno přesné složení tohoto vzorku, viz \autoref{sec:vae_encoder}.} z množiny $[0, \dots, 9]$ a při generovaní obrázku se ujistí, že všechny vygenerované pixely souhlasí s charakteristikami této číslice.
V tomto kontextu \emph{latentní} znamená, že pro danou číslici, kterou model vygeneroval, nemusí být latentnímu modelu nutně známa třída původního vstupu (tj. \emph{konkrétní číslice, která byla na vstupu, např. 9})\footnote{Latentní proměnná \emph{může} zachycovat, a u variačního autoenkodéru zachycuje, reprezentaci číslice, kterou model obdržel na vstupu, v nižší dimenzionalitě.}. To by muselo být odvozeno například počítačovým viděním. \cite{Doersch2021}

Libovolný model využívá latentních proměnných, pokud platí, že ve vysokodimenzionálním prostoru $\mathcal{Z}$ existuje vektor latentních proměnných $z$, ze kterého lze podle \emph{nějaké} hustoty pravděpodobnosti $P(z)$, definované skrze $\mathcal{Z}$, jednoduše vzorkovat $\emph{X}$.
Dále, existuje množina deterministických funkcí $f(z;\theta)$, parametrizovaných vektorem $\theta$ v nějakém prostoru $\Theta$, kde $f: \mathcal{Z} \times \Theta \rightarrow \mathcal{X}$. Byť je $f$ deterministická, je-li $z$ náhodné a $\theta$ neměnné, pak $f(z;\theta)$ je náhodná proměnná z prostoru $\mathcal{X}$.
Cílem pak je optimalizovat $\theta$ tak, aby bylo možné vzorkovat $z$ z $P(z)$ a aby, s vysokou pravděpodobnostní, každý výstup $f(z; \theta)$ byl podobný příslušným $X$ ze vstupního datasetu. \cite{Doersch2021}

Tedy maximalizovat pravděpodobnost každého $X$ z trénovací množiny skrze celý generativní proces dle:
\begin{equation}\label{eq:maximum_likelihood}
    P(X) = \int_{}^{} P(X\mid z;\theta)P(z)dz.
\end{equation}

Kde $f(z;\theta)$ je nahrazeno distribucí $P(X\mid z;\theta)$, která umožňuje explicitně vyjádřit závislost $X$ na $z$ (z věty o celkové pravděpodobnosti, viz \cite{Murphy2022}). \cite{Doersch2021}

\subsection{Maximum likelihood}
\label{sec:maximum_likelihood}
Intuitivní myšlenka za tímto principem, nazývaným \emph{maximum likelihood}, je, že pokud model s vysokou pravděpodobností vyprodukuje množinu vzorků z trénovací množiny, tak je také \textbf{pravděpodobné}, že vyprodukuje \textbf{podobné vzorky} které nebyly součástí trénovací množiny (a naopak, je  \textbf{nepravděpodobné} že vyprodukuje \textbf{odlišné vzorky vůči vzorkům v trénovací množině}). \cite{Doersch2021}

U variačních autoenkodérů je toto výstupní rozdělení pravděpodobnosti často voleno jako normální, tedy například $P(X\mid z\theta) = \mathcal{N}(X\mid f(z;\theta), \sigma^2 * I)$
\footnote{Tedy, má střední hodnotu $f(z;\theta)$ a kovarianci rovnu jednotkové matici $I$ krát \emph{nějaká} skalární hodnota $\sigma$. \cite{Doersch2021}}. \cite{Doersch2021}

Obecně (a zejména na počátku trénovací fáze) model nebude generovat výstupy které jsou \emph{identické} k libovolnému $X$.
V důsledku toho, že u variačního autoenkodéru volíme toto výstupní rozdělení jako normální, lze provést algoritmus gradientního sestupu (případně jinou optimalizační techniku) za účelem zvýšení $P(x)$ tím, že donutíme $f(z;\theta)$ blížit se k $X$ pro nějaké $z$ (a tím pádem iterativně zvyšovat pravděpodobnost, že model vygeneruje vzorek podobný vzorku z trénovací množiny). \cite{Doersch2021}

Důležitou vlastností pro možnost provést tuto optimalizaci tedy je, aby $P(X \mid z)$ bylo \textbf{vyčíslitelné a spojité v $\theta$} \footnote{Výstupní rozdělení nemusí byt normální (ale např. alternativní rozdělení).}. \cite{Doersch2021}