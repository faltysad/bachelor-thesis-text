\section{Strojové učení}
\subsection{Algoritmus strojového učení}
\label{sec:machine_learning_algorithm}
Definici algoritmu strojového učení výstižně shrnuje následující definice \cite[str. 2]{Mitchell1997}:
\blockquote{\emph{A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.}}

Algoritmy strojového učení lze obecně rozdělit na čtyři třídy – učení s učitelem (\emph{supervised learning}),
učení bez učitele (\emph{unsupervised learnaing}), kombinaci učení s učitelem a učení bez učitele (\emph{semi-supervised learning}) a posilovaného učení (\emph{reinforcement learning}).

Toto dělení vychází ze zkušenosti $E$, resp. míry, do jaké má algoritmus strojového učení \textbf{povoleno} interagovat s datovou sadou (resp. jakou možnost taková datová sada nabízí). \cite{Goodfellow2016}

Pro předmět této práce budou blíže představeny pouze oblasti \textbf{a učení s učitelem} a \textbf{učení bez učitele}.
\subsection{Učení s učitelem}
\label{sec:supervised_learning}
Zkušenost $E$ algoritmů učení bez učitele vychází z datové sady, která může mít celou řadu vlastností a zároveň je \textbf{předem známá klasifikace každého datového bodu do definovaných tříd}.
Na základě této asociace je natrénovaný algoritmus schopen provést přiřazení dosud neznámého objektu do jedné z tříd definovaných v trénovací sadě dat.

Učení s učitelem zahrnuje pozorování několika příkladů náhodného vektoru $\mathbf{x}$ a asociované hodnoty (či vektoru) $\mathbf{y}$.
Následuje učení se predikovat $\mathbf{y}$ z $\mathbf{x}$, často na základě odhadu $p(\mathbf{y}\mid\mathbf{x})$.

Samotný termín \emph{učení s učitelem} vychází ze situace, kdy je cílová třída $\mathbf{y}$ poskytnuta jakýmsi instruktorem či učitelem, který systému strojového učení ukazuje očekávané chování. \cite{Goodfellow2016}
\subsection{Učení bez učitele}
Ve vztahu k definici (viz \autoref{sec:machine_learning_algorithm}) lze říci, že zkušenost $E$ algoritmů učení bez učitele vychází z datové sady, která může mít celou řadu vlastností.
Cílem trénování algoritmů učení bez učitele je \textbf{naučit se užitečné a charakteristické vlastnosti o struktuře vstupní datové sady}.
V kontextu hlubokého učení (\autoref{sec:deep_learning}) je pak obvyklým cílem algoritmu naučit se celé rozdělení pravděpodobnosti, které generuje původní datovou sadu (ať už explicitně – např. odhad hustoty, či implicitně – např. úlohy syntézy dat a odstranění šumu).
Mezi další techniky strojového učení bez učitele se, mimo jiné, řadí shluková analýza.

Obecně lze říct, že učení bez učitele zahrnuje pozorování několika příkladů náhodného vektoru $\mathbf{x}$ na základě kterého se snaží implicitně či explicitně \emph{naučit} rozdělení pravděpodobnosti $p(\mathbf{x})$, případně \emph{užitečné vlastnosti} tohoto pravděpodobnostního rozdělení.

Na rozdíl od \autoref{sec:supervised_learning}, název \emph{učení bez učitele} napovídá, že v procesu učení není zapojen žádný \emph{instruktor} či \emph{učitel}, a tak systém strojového učení sám musí vyvodit smysl a užitečné vlastnosti předložené datové sady. \cite{Goodfellow2016}
\subsection{Perceptron}
\subsection{Hebbovské účení}
\subsection{Umělé neuronové sítě}
\subsection{Universal approximation theorem}
\label{sec:universal_approximation_theorem}
% https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/ And what we is that if the net is too small, it just can’t reproduce the function we want. But above some size, it has no problem—at least if one trains it for long enough, with enough examples. And, by the way, these pictures illustrate a piece of neural net lore: that one can often get away with a smaller network if there’s a “squeeze” in the middle that forces everything to go through a smaller intermediate number of neurons. (It’s also worth mentioning that “no-intermediate-layer”—or so-called “perceptron”-networks can only learn essentially linear functions—but as soon as there’s even one intermediate layer it’s always in principle possible to approximate any function arbitrarily well, at least if one has enough neurons, though to make it feasibly trainable one typically has some kind of regularization or normalization.)

\subsection{Vícevrstvý Perceptron}
\label{sec:multilayer_perceptron}

\subsection{Dopředná umělá neuronová síť}
\label{sec:feedforward_nn}
\subsection{Gradient descent}
\label{sec:gradient_descent}
\subsection{Backpropagation}
\subsection{Hluboké učení}
\label{sec:deep_learning}
\subsection{Konvoluční sítě}
\label{sec:cnn}

\subsection{No free lunch theorem for machine learning}
\cite{Goodfellow2016}
\subsection{Regularizace}


