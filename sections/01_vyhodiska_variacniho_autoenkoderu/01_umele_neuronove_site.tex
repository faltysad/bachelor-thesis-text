\section{Umělé neuronové sítě}
\subsection{Perceptron}
\subsection{Hebbovské účení}
\subsection{Universal approximation theorem}
\label{sec:universal_approximation_theorem}
% https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/ And what we is that if the net is too small, it just can’t reproduce the function we want. But above some size, it has no problem—at least if one trains it for long enough, with enough examples. And, by the way, these pictures illustrate a piece of neural net lore: that one can often get away with a smaller network if there’s a “squeeze” in the middle that forces everything to go through a smaller intermediate number of neurons. (It’s also worth mentioning that “no-intermediate-layer”—or so-called “perceptron”-networks can only learn essentially linear functions—but as soon as there’s even one intermediate layer it’s always in principle possible to approximate any function arbitrarily well, at least if one has enough neurons, though to make it feasibly trainable one typically has some kind of regularization or normalization.)

\subsection{Vícevrstvý Perceptron}
\label{sec:multilayer_perceptron}

\subsection{Dopředná umělá neuronová síť}
\label{sec:feedforward_nn}
\subsection{Gradient descent}
\label{sec:gradient_descent}
\subsection{Backpropagation}
\subsection{Strojové učení}
\label{sec:machine_learning}
\subsection{Hluboké učení}
\label{sec:deep_learning}
\subsection{Konvoluční sítě}
\label{sec:cnn}

