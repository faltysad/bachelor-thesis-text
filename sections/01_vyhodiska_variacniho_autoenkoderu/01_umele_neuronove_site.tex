\section{Strojové učení}
\subsection{Algoritmus strojového učení}
\label{sec:machine_learning}
Definici algoritmu strojového učení výstižně shrnuje následující definice \cite[str. 2]{Mitchell1997}:
\blockquote{\emph{A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.}}

Algoritmy strojového učení lze obecně rozdělit na čtyři třídy – učení s učitelem (\emph{supervised learning}),
učení bez učitele (\emph{unsupervised learnaing}), kombinaci učení s učitelem a učení bez učitele (\emph{semi-supervised learning}) a posilovaného učení (\emph{reinforcement learning}).

Toto dělení vychází ze zkušenosti $E$, resp. míry, do jaké má algoritmus strojového učení \textbf{povoleno} interagovat s datovou sadou (resp. jakou možnost taková datová sada nabízí). \cite{Goodfellow2016}

Pro předmět této práce budou blíže představeny pouze oblasti \textbf{učení bez učitele} a \textbf{a učení s učitelem}.
\subsection{Učení bez učitele}
Zkušenost $E$ algoritmů učení bez učitele vychází z datové sady, která může mít celou řadu vlastností.
Cílem algoritmů učení bez učitele je trénování \textbf{naučit se užitečné a charakteristické vlastnosti o struktuře vstupní datové sady}.
V kontextu hlubokého učení (\autoref{sec:deep_learning}) je pak obvyklým cílem algoritmu naučit se celé rozdělení pravděpodobnosti, které generuje původní datovou sadu (ať už explicitně – např. odhad hustoty, či implicitně – např. úlohy syntézy dat a odstraněni šumu).
Mezi další techniky strojového učení bez učitele se, mimo jiné, řadí shluková analýza.
\subsection{Učení s učitelem}
\subsection{Perceptron}
\subsection{Hebbovské účení}
\subsection{Umělé neuronové sítě}
\subsection{Universal approximation theorem}
\label{sec:universal_approximation_theorem}
% https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/ And what we is that if the net is too small, it just can’t reproduce the function we want. But above some size, it has no problem—at least if one trains it for long enough, with enough examples. And, by the way, these pictures illustrate a piece of neural net lore: that one can often get away with a smaller network if there’s a “squeeze” in the middle that forces everything to go through a smaller intermediate number of neurons. (It’s also worth mentioning that “no-intermediate-layer”—or so-called “perceptron”-networks can only learn essentially linear functions—but as soon as there’s even one intermediate layer it’s always in principle possible to approximate any function arbitrarily well, at least if one has enough neurons, though to make it feasibly trainable one typically has some kind of regularization or normalization.)

\subsection{Vícevrstvý Perceptron}
\label{sec:multilayer_perceptron}

\subsection{Dopředná umělá neuronová síť}
\label{sec:feedforward_nn}
\subsection{Gradient descent}
\label{sec:gradient_descent}
\subsection{Backpropagation}
\subsection{Hluboké učení}
\label{sec:deep_learning}
\subsection{Konvoluční sítě}
\label{sec:cnn}

\subsection{No free lunch theorem for machine learning}
\cite{Goodfellow2016}
\subsection{Regularizace}


