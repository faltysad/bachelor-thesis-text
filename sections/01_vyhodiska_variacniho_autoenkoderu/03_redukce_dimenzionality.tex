\section{Redukce dimenzionality}
\label{sec:dimensionality_reduction}

Do oblasti redukce dimenzionality patří celá řada technik pro práci s vysokodimenzionálními daty.
Cílem této oblasti je porozumět tvaru dat, se kterými pracuje.
Typickou úlohou redukce dimenzionality dat je sestrojit \textbf{nízkodimenzionální reprezentaci}, která zachytí \emph{většinu významu} původní, vysokodimenzionální, reprezentace.
Tento jev nazýváme hledáním \textbf{salientních vlastností} původní sady dat. \cite{Phillips2021}

Redukci dimenzionality lze chápat jako úlohu učení bez učitele, ve které se algoritmus strojového učení učí mapování z vysokodimenzionálního prostoru $\textbf{\emph{x}} \in \mathbb{R}^D$ do nízkodimenzionálního latentního prostoru $\textbf{\emph{z}} \in \mathbb{R}^L$. \cite{Murphy2022}

Princip redukce dimenzionality je pro zavedení variačního autoenkodéru velice důležitý, jelikož enkodér modul variačního autoenkodéru je nucen redukovat dimenzionalitu vstupních dat, a ponechat tak pouze nositele salientních vlastností. Tento proces detailněji popisuje \autoref{sec:vae_encoder}.

\subsection{\textquote{The curse of dimensionality}}
S rostoucím počtem vstupních proměnných exponenciálně roste počet vzorků dat nutný pro \emph{libovolně přesnou} aproximaci dané funkce.
V důsledku je tedy s rostoucí dimenzí vstupních dat značně degradováno chování většiny algoritmů (strojového učení).
Tento problém je známý pod termínem \textbf{curse of dimensionality}. \cite{Bellman1957}

I proto došlo ke vzniku oblasti zvané \emph{feature engineering}.
\subsection{Selekce vlastností dat}

Selekce vlastností dat (\emph{feature engineering}) je oblast, která se zabývá disciplínou volby vlastností dat, které budou použity při trénování modelu.
Pro automatizovanou selekci vlastností existuje celá řada technik.
Výběr podprostoru, který \emph{nejlépe} reprezentuje vlastnosti původních dat je NP-těžký kombinatorický problém.
Tyto techniky dokonce často vyhodnocují každou vstupní proměnnou nezávisle, což může vést ke zkresleným závěrům o jejich významnosti – naopak je běžné, že proměnné začínají vykazovat určitou míru významnosti \textbf{až při vzájemném využití}. \cite{Stanczyk2015}

Výše stanovené důvody vedly ke vzniku další disciplíny, a to \textbf{extrakce vlastností}, o níž pojednává \autoref{sec:feature_extraction}. 

\subsection{Extrakce vlastností}
\label{sec:feature_extraction}
Cílem extrakce vlastností (\emph{feature extraction}) je najít reprezentaci vstupních dat, která je vhodná pro algoritmus strojového učení, který se chystáme využít (jelikož původní reprezentace může být z mnoha důvodů nevhodná – například vysokodimenzionální).
Typicky tak musí dojít k redukci dimenzionality vstupních dat. \cite{Liu1998}

K extrakci nových vlastností lze dojít mnoha způsoby.
Jedna z technik založená na hledání lineárních kombinací původních vstupních vlastností, se nazývá analýza hlavních komponent a představuje ji \autoref{sec:pca}.

Pro nelineární redukci dimenzionality lze využít technik tzv. \emph{manifold learning}, viz \cite[kap. 5.11.3.]{Goodfellow2016}, kterých využívají i variační autoenkodéry pracující s vysokodimenzionálními daty.


\subsection{Analýza hlavních komponent (PCA)}
\label{sec:pca}
Jednou z nejrozšířenějších metod pro redukci dimenzionality je analýza hlavních komponent (\emph{PCA} \footnote{Principal component analysis}).

V principu se jedná o nalezení lineární a ortogonální projekce vysokodimenzionálních dat $\textbf{\emph{x}} \in \mathbb{R}^D$ do nízkodimenzionálního podprostoru  $\textbf{\emph{z}} \in \mathbb{R}^L$,
tak aby tato nízkodimenzionální reprezentace byla \emph{dobrou aproximací} původních dat.

Na PCA analýzu lze pohlížet jako na algoritmus učení bez učitele, který se učí reprezentaci vstupních dat. Tato reprezentace má dvě kritéria \cite{Goodfellow2016}:

\begin{itemize}
    \item Reprezentace vstupních dat má menší dimenzi než původní vstupní reprezentace.
    \item Elementy reprezentace mezi sebou nemají žádnou lineární korelaci.
\end{itemize}

Pro formální zavedení PCA analýzy je odkázáno na \cite{Murphy2022}.

