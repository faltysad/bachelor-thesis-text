\section{Autoenkodér}
Autoenkodér je typ umělé neuronové sítě se schopností učit se efektivní reprezentace vstupních dat bez učitele. Umělá neuronová síť Autoenkodéru má symetrickou strukturu a skrytou vrstvu \emph{h}, která popisuje \emph{kód} použitý pro reprezentaci vstupu.
Architekturu Autoenkodéru (viz \ref{fig:basic_autoencoder_structure}) lze principiálně rozdělit na dvě části – kódovací funkci $h = f(x)$, resp. \textbf{enkodér}
a dekódovací funkci $r = g(h)$, resp. \textbf{dekodér}.
Hovoříme tedy o typu umělé neuronové sítě s \emph{enkodér-dekodér} moduly.
Výstupem enkodéru je \textbf{kód} vstupu $\emph{h}$. Výstupem dekodéru je \textbf{rekonstrukce} vstupu \emph{r}.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[round/.style={circle, draw, minimum size=10mm, node distance=25mm, on grid}]
        \node[round](h){h};
        \node[round](x)[below left of=h]{x};
        \node[round](r)[below right of=h]{r};
        
        \draw[-Triangle] (x) -- node [above] {$f$} (h);
        \draw[-Triangle] (h) -- node [above] {$g$} (r);
    \end{tikzpicture}
    \caption{Obecná struktura Autoenkodéru. Ze vstupu $\emph{x}$ je enkodérem vytvořen kód $\emph{h}$} (funkce $\emph{f}$). Tento kód je následně dekodérem přetaven na rekonstrukci $\emph{r}$ (funkce $\emph{g}$).
    \label{fig:basic_autoencoder_structure}
\end{figure}

Obecnou strukturu (viz \ref{fig:basic_autoencoder_structure}) lze reprezentovat dopřednou umělou neuronovou sítí.
Jejím cílem je \textbf{rekonstruovat vstupní data na výstupní vrstvě}. Počet vstupů je tak totožný s počtem neuronů ve výstupní vrstvě umělé neuronové sítě (tedy $x$ a $r$ mají stejnou dimenzi).
\emph{h} může mít \emph{menší} či \emph{větší} dimenzi – volba dimenze \emph{h} se odvíjí od požadovaných vlastností Autoenkodéru.
Obecná architektura modulů umělé neuronové sítě Autoenkodéru je zachycena v \ref{fig:autoencoder}.


\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \node[trapezium,
            draw,
            text=black,
            trapezium angle=20,
            trapezium stretches=true,
            minimum height=2.5cm,
            minimum width=5cm,
            rotate=270] (encoder) at (1, 0) {Enkodér};
        
        \node[
            preaction={clip, postaction={draw=black, line width=0.3mm,}},
            minimum height=5cm,
            left=of encoder.center,
        ] (input) at (0, 0) {$\mathbf{x}$};

        \node[
            preaction={clip, postaction={draw=black, line width=0.3mm,}},
            minimum height=18.5mm,
            right=of encoder.north,
        ] (h)  {$\mathbf{h}$}; 

        \node[trapezium,
            draw,
            text=black,
            trapezium angle=20,
            trapezium stretches=true,
            minimum height=2.5cm,
            minimum width=5cm,
            rotate=90] (decoder) at (6, 0) {Dekodér};

        \node[
            preaction={clip, postaction={draw=black, line width=0.3mm,}},
            minimum height=5cm,
            right=of decoder.center,
        ] (output) at (7, 0) {$\mathbf{\hat{x}}$};

        \draw[-Triangle] (input) -- (encoder);
        \draw[-Triangle] (encoder) -- (h);
        \draw[-Triangle] (h) -- (decoder);
        \draw[-Triangle] (decoder) -- (output);
    \end{tikzpicture}
    \caption{Jednotlivé moduly architektury umělé neuronové sítě Autoenkodéru. }
    \label{fig:autoencoder}
\end{figure}

Autoenkodér je trénován k rekonstrukci jeho vstupů.
Pokud by se Autoenkodér naučil jednoduše určit $\mathbf{\emph{x}} = g(f(\mathbf{\emph{x}}))$ pro každé $\emph{x}$, získali bychom \emph{identitu}, která není patřičně užitečná.
Proto je při trénování zavedena řada omezení, jejichž účelem je zabránit možnosti naučení Autoenkodéru perfektně kopírovat vstupní data.

\subsection{Historický pohled}
Vícevrstvý Perceptron \autoref{sec:multilayer_perceptron} je univerzálním aproximátorem \autoref{sec:universal_approximation_theorem} – tedy historicky nalézá uplatnění zejména v klasifikačních úlohách učení s učitelem.
Sofistikovaný algoritmus se schopností trénování Vícevrstvého Perceptronu s větším počtem skrytých vrstev stále schází, a to zejména v důsledku problému mizejícího gradientu (\emph{vanishing gradient problem}).
Až příchod algoritmu gradientního sestupu \autoref{sec:gradient_descent}, který adresuje problém mizejícího gradientu v aplikacích s použitím konvolučních sítí \autoref{sec:cnn} a úloh učení se bez učitele, značí počátek moderních metod hlubokého učení.
V oblasti hlubokého učení \autoref{sec:deep_learning} dochází k emergenci a vývoji řady technik pro řešení úloh učení se bez učitele.
V této kapitole je popsána pouze jedna z nich – architektura umělé neuronové sítě založené na \emph{enkodér-dekodér} modulech: Autoenkodér.
Autoenkodéry byly poprvé představeny jako způsob pro předtrénování umělých neuronových sítí (formou automatizované extrakce vlastností \emph{feature extraction}). 
Později Autoenkodéry nalézají uplatnění zejména v úloháh redukce dimenzionality \autoref{sec:dimensionality_reduction} či fůzi vlastností (\emph{feature fusion}).


Nedávné teoretické propojení Autoenkodéru a Modelů využívajících latentní proměnných \autoref{sec:latent_variable_models} však vedlo ke vzniku zcela nové architektury neuronové sítě kombinující charakter redukce dimenzionality Autoenkodéru se statistickými metodami odvozování.
To vyneslo Autoenkodéry na popředí v oblasti generativního modelování – této architektuře je věnována kapitola \autoref{chap:vae}.

Byť Autoenkodéry vznikly v kontextu hlubokého učení, není pravidlem že všechny modely Autoenkodéru obsahují vícero skrytých vrstev. Následuje rozdělení Autoenkodérů dle struktury umělé neuronové sítě.

\subsection{Mělké autoenkodéry}
\label{sec:shallow_autoncoder}
\subsection{Autoenkodér s neúplnou skrytou vrstvou}
\label{sec:undercomplete_autoencoder}
Autoenkodér s neúplnou skrytou vrstvou (\emph{Undercomplete autoencoder}) je Autoenkodér, jehož dimenze kódu (\emph{h}) je menší, než dimenze vstupu.
Tuto skrytou vrstvu \emph{h} nazýváme \textbf{bottleneck}. Bottleneck je způsob, kterým se Autoenkodér s neúplnou skrytou vrstvou učí kompresované reprezentaci znalostí. 
V důsledku bottleneck vrstvy je Autoenkodér nucen zachytit pouze ty stěžejní vlastnosti trénovacích dat, které následně budou použity pro rekonstrukci.

Trénovací proces Neuplného autoenkodéru je popsán jako minimalizace ztrátové funkce:

\begin{equation}
    L(\mathbf{x}, g(f(\mathbf{x}))),
\end{equation}

kde $L$ je ztrátová funkce, penalizující $g(f(\mathbf{x}))$ za \emph{rozdílnost} vůči $\mathbf{x}$ (např. \emph{střední kvadratická chyba}).


\begin{figure}[H]
    \centering
    \begin{neuralnetwork}[height=4]
        \tikzstyle{input neuron}=[neuron, circle, draw=black, fill=white];
        \tikzstyle{hidden neuron}=[neuron, circle, draw=black, fill=white];
        \tikzstyle{output neuron}=[neuron, circle, draw=black, fill=white];
      
        \inputlayer[count=4, bias=false, title=Enkodér, text=\xin]
      
      
        \hiddenlayer[count=2, bias=false, title=Kód $\emph{h}$]
        \linklayers
      
        \outputlayer[count=4, title=Dekodér, text=\xout]
        \linklayers
      
      \end{neuralnetwork}
    \caption{Jednoduchá architektura umělé neuronové sítě Autoenkodéru s neúplnou skrytou vrstvou. Skrytá vrstva představuje \emph{bottleneck}.}
    \label{fig:autoencoder_bottleneck}
\end{figure}
\subsubsection{Od Analýzy hlavních komponent po Autoenkodér}
Máme-li linéární dekodér (Autoenkodér používá pouze linéárni aktivační funkce) a jako ztrátová funkce $L$ je použita \emph{střední kvadratická chyba},
pak se Neuplný autoenkodér naučí stejný \emph{vektorový prostor}, který by byl výsledkem Analýzy hlavních komponentů \autoref{sec:pca}.
V tomto speciálním případě lze ukázat, že Autoenkodér trénovaný na úloze kompresované reprezentace znalostí jako vedlejší efekt provedl Analýzu hlavních komponentů.

Důležitým důsledkem tohoto jevu je, že \textbf{Autoenkodéry} s nelineární kódovací funkcí $f$
a nelineární dekódovací funkcní $g$ \textbf{jsou schopny učit se obecnější generalizaci}
než u Analýzy hlavních komponent.

Na druhou stranu, má-li Autoenkodér k dispozici příliš mnoho kapacity,
může se naučit kopírovat vstupní data na výstupní vrstvu bez extrakce užitečných (charakteristických) vlastností o rozdělení vstupních dat.

\subsubsection{Problém s naučením pouhého identického zobrazení}
\label{sec:identity}
Extrémním případem je teoretický scénář, ve kterém je Autoenkodér složen z kódu (\emph{h}) o jedné vrstvě a velmi výkonného enkodéru.
Takový Autoenkodér by se mohl naučit reprezentovat každý vstup $x_i$ kódem \emph{i}.
Dekodér by se pak tyto indexy mohl naučit mapovat zpátky na hodnoty konkrétních trénovacích vzorků dat.
Tento příklad se v praxi běžně nenaskytne, nicméne jasně ilustruje,
jak může Autoenkodér při úloze kopírování vstupu na výstupní vrstvu selhat naučit se užitečné vlastnosti o vstupních datech, jsou-li restrikce při učení příliš nízké.
Proto je třeba Autoenkodéry regularizovat.

Dále tedy budou představeny přístupy k architekturám Autoenkodérů \textbf{s využitím regularizace}.
\subsection{Autoenkodér s rozšířenou skrytou vrstvou}
\label{sec:overcomplete_autoencoder}
% TODO: Zapsat toto matematicky
Autoenkodér s rozšířenou skrytou vrstvou (\emph{Overcomplete Autoencoder}) je Autoenkodér, jehož počet neuronů ve skrýté vrstvě je větší než počet neuronů vstupní (a výstupní) vrstvy.

\subsection{Hlubuký autoenkodér}
V sekci \autoref{sec:undercomplete_autoencoder} a \autoref{sec:overcomplete_autoencoder} byly představeny Autonkodéry s jednovrstvým enkodérém a s jednovrstvým dekodérem.
Existují však i Autoenkodéry, jejichž enkodér a dekodér moduly jsou vícevrstvé, tedy mají hloubky vyšší než jedna.

Hluboký autoenkodér (\emph{Deep} autoenkodér), je Autoenkodér s netriviální hlobkou skryté vrstvy (\emph{kód}) $\textbf{\emph{h}}$.

\begin{figure}[H]
    \centering
    \begin{neuralnetwork}[height=6]
        \tikzstyle{input neuron}=[neuron, circle, draw=black, fill=white];
        \tikzstyle{hidden neuron}=[neuron, circle, draw=black, fill=white];
        \tikzstyle{output neuron}=[neuron, circle, draw=black, fill=white];
      
        \inputlayer[count=6, bias=false, text=\xin]
        \hiddenlayer[count=4, bias=false]
        \linklayers
        \hiddenlayer[count=2, bias=false]
        \linklayers
        \hiddenlayer[count=4, bias=false]
        \linklayers
        \outputlayer[count=6, text=\xout]
        \linklayers
      \end{neuralnetwork}
    \caption{Deep Autoenkodér.}
    \label{fig:stacked_autoencoder}
\end{figure}

Z netriviální hloubky dopředné umělé neuronové sítě plyne \autoref{sec:universal_approximation_theorem},
který garantuje, že dopředná umělá neuronová síť s alespoň jednou skrytou vrstvou dokáže aproximovat (\emph{libovolně přesně}) jakoukoliv funkci (za předpokladu dostatečného počtu neuron skryté vrstvy).

Nicméně u mělkých enkodérů, které jsou rovněž dopřednou sítí, neexistuje možnost představit libovolná omezení a regularizační prvky (například řídkost \emph{kódu} $\textbf{\emph{h}}$ – viz \autoref{sec:sparse_autoencoder}).
A tedy nelze zamezit problému naučiení pouhé identity \autoref{sec:identity}. Narozdíl od mělkých autoenkoderů \autoref{sec:shallow_autoncoder},
však mohou Hluboké autoenkodéry (\emph{libovolně přesně}) aproximovat jakékoliv mapování vstupu na kód (\emph{opět s předpokladem dostatečného počtu neuronů skryté vrstvy}).

S netriviální hloubkou se rovněž pojí významná redukce výpočetních nákladů spojených s reprezentací některých funkcí. 
V důsledku možnosti efektivnější reprezentace takových funkcí dochazí i k zmenšení nároků na velikost množiny trénovacích dat.


\subsubsection{Stacked autoenkodér}
Běžnou strategií pro trénování Hlubokého autoenkodéru je hladově předtrénovat (\emph{greedy pretraining}) model individuálním natrénováním většího počtu Mělkých autoenkodérů (představených v sekci \autoref{sec:shallow_autoncoder}), které jsou následně vloženy za sebe.
Takto složený Autoenkodér nazýváme Stacked autoenkodér.



\subsection{Řídký autoenkodér}
\label{sec:sparse_autoencoder}
Řídká reprezentace dat (\emph{sparsity}) ve strojovém učení znamená, že většina hodnot daného vzorku je nulová. 
Motivací pro řídkou reprezentaci dat ve strojovém učení je napodobení chování buněk v primární zrakové oblasti (\emph{V1}) mozku savců.
Konkrétně schopnosti odhalit a uložit efektivní kódovací strategie pozorovaných vjemů. 

Pro sestrojení Řídkého autoenkodérů je tedy nutné představit omezení (regularizační prvek) hodnot aktivací neuronů ve skryté (kódovací) vrstvě $\textbf{\emph{h}}$ (resp. počtu aktivních neuronů ve skrýte vrstvě).

Řídký autoenkodér (\emph{Sparse Autoecoder}) je Autoenkodér, jehož ztrátová funkce je rozšířena o penalizaci řídkosti kódovací vrstvy $\textbf{\emph{h}}$ (tzv. \emph{sparsity penalty}) vztahem $\Omega(\textbf{\emph{h}})$:

\begin{equation}
    L(\mathbf{x}, g(f(\mathbf{x}))) + \Omega(\textbf{\emph{h}}),
\end{equation}

kde $\Omega$ je \textbf{regularizační prvek}, jehož cílem je přiblížit hodnoty aktivací neuronů kódovací vrstvy k cílové hodnotě (a zabránit přeučení).
Chceme tak penalizovat neurony kódovací vrstvy, které se aktivují příliš často.

Běžně lze $\Omega$ stanovit následovně. Mějme Bernoulliho náhodnou proměnou $\emph{i}$ modelující aktivace neuronů skrýte (kódovací) vrstvy – můžou tedy nastat dva stavy: neuron skryté vrstvy je buď aktivován, nebo není aktivován.
Pro konkrétní vstup $\emph{x}$ dostaneme:

\begin{equation}
    \hat{p_i} = \frac{1}{|S|}\sum_{x \in S}^{}f_i(x),
\end{equation}

kde $ f = (f_1, f_2, \dots, f_c)$, $c$ je počet neuronů skryté (kódovací) vrstvy a $\hat{p_i}$ je průměrná aktivační hodnota neuronu skryté vrstvy (resp. střední hodnota příslušného Bernoulliho schématu).

Dále mějme $p$ jako cílové rozdělení aktivací.
Kullback-Leibnerova divergence mezi náhodnou proměnnou $i$ a $p$ pak udává rozdíl obou rozdělení:

\begin{equation}
    KL(p \parallel \hat{p_i}) = p \log \frac{p}{\hat{p_i}} + (1 - p) \log \frac{1 - p}{1 - \hat{p_i}}.
\end{equation}

Výsledný penalizační prvek $\Omega$ pro Řídký autoenkodér má tedy následující podobu:
\begin{equation}
    \Omega_{ŘAE}(W, b; S) = \sum_{i=1}^{c}KL(p \parallel \hat{p_i}),
\end{equation}

kde průměrná hodnota aktivací $\hat{p_i}$ závisí na parametrech enkodéru a množině trénovacích dat $S$. 

Přičtením tohoto penalizačního prvku ke ztrátové funkci (a následnou minimalizací celkové ztrátové funkce) je Autoenkodér nucen \textbf{omezit počet aktivních neuronů v skrýté (kódovací) vrstvě}.
V důsledku tohoto omezení pak každý neuron skryté vrstvy reprezentuje nějakou \textbf{salientní vlastnost} vstupních dat (a rovněž je zamezeno naučení pouhé identity \autoref{sec:identity}).

\subsection{Denoising autoenkodér}
Denoising autoenkodér je autoenkodér, který na vstupu obdrží poškozená vstupní data
a při trénování je jeho předpovědět originální vstup bez poškození, a ten na výstupní vrstvě vrátit.

Autoenkodéry běžně minimalizují funkci ve tvaru:
\begin{equation}
    L(\mathbf{x}, g(f(\mathbf{x}))),
\end{equation}

kde $L$ je ztrátová funkce penalizující $g(f(x))$ za odlišnost od $\mathbf{x}$ (např. Euklidovská norma jejich rozdílů).
Jak ale bylo ukázáno v \autoref{sec:identity}, to umožňuje $f \circ g$ naučit se být pouhou identitu.


Z toho důvodu Denoising autoenkodér (\emph{Denoising Autoencoder}) minimalizuje funkci:
\begin{equation}
    L(\textbf{\emph{x}}, g(f(\tilde{\textbf{\emph{x}}}))),
\end{equation}

kde $\tilde{\textbf{\emph{x}}}$ je kopií $\textbf{\emph{x}}$ která byla úmyslně poškozena procesem $C(\tilde{x} | x)$ (\emph{corruption process}),
který reprezentuje podmíněné rozdělení pravděpodobnosti poškozenných vzorků $\tilde{x}$ v závislosti na vzorku vstupních dat $x$.

Denoising autoenkodér se pak učí \textbf{rozdělení rekonstrukce} $p_{reconstruct}$($\mathbf{x}|\mathbf{\tilde{x}}$),
které je odhadnuto z trénovacích dvojic následovně:

\begin{enumerate}
    \item Zvolit trénovací vzorek $\emph{\textbf{x}}$ z množiny trénovacích dat
    \item Vygenerovat poškozenou verzi zvoleného vzorku ($\tilde{\textbf{\emph{x}}}$) procesem $C$
    \item Použít dvojice ($\emph{\textbf{x}}$, $\tilde{\textbf{\emph{x}}}$) jako množinu trénovacích dat pro odhadnutí rozdělení rekonstrukce Denoising autoenkodéru $p_{reconstruct}(\textbf{x} | \tilde{\textbf{x}}) = p_{decoder}(\textbf{\emph{x}}|\textbf{\emph{h}})$, kde $p_{decoder}$ je výstupem funkce dekodéru $g(\textbf{\emph{h}})$
\end{enumerate}

Při trénování Denoising autoenkodéru jsou funkce $\emph{f}$ a $\emph{g}$ nuceny zachytit implicitní strukturu $p_{data}(\textbf{\emph{x}})$.

Trénovací procedura Denoising autoenkodéru lze schematicky znázornit následovně (viz \ref{fig:denoising_autoencoder}):

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[round/.style={circle, draw, minimum size=10mm, node distance=25mm}]
        \node[round](h){$h$};
        \node[round](x_tilde)[below left of=h]{$\tilde{x}$};
        \node[round](l)[below right of=h]{$L$};
        \node[round](x)[below right of=x_tilde]{$x$};
        
        \draw[-Triangle] (x_tilde) -- node [above] {$f$} (h);
        \draw[-Triangle] (h) -- node [above] {$g$} (l);
        \draw[-Triangle] (x) -- node [below left] {$C(\tilde{x} | x)$} (x_tilde);
        \draw[-Triangle] (x) -- (l);
    \end{tikzpicture}
    \caption{DAE}
    \label{fig:denoising_autoencoder}
\end{figure}

Denoising autoenkodér se tedy musí naučit toto poškození odstranit a rekonstruovat tak původní vstup (namísto pouhého naučení se identitě).

Denoising Autoenkodéry jsou příkladem hned dvou jevů:
\begin{itemize}
    \item Emergence užitečných vlastností o vstupních datech jako výsledek minimalizace chyby rekonstrukce
    \item Schopnosti modelů s vysokou kapacitou/rozšířenou skrytou vrstvou fungovat jako Autoenkodér, \textbf{za předpokladu že je jim zabráněno naučit se identické zobrazení vstupních dat}
\end{itemize}

\subsection{Contractive autoenkodér}
Přehnaná citlivost na \emph{drobné rozdíly} ve vstupních datech by mohla vést k architektuře Autoenkodéru, která pro velmi podobné vstupy generuje odlišné kódy.

Contractive autonekodér (\emph{CAE}), je Autoenkodér, který je při trénování omezen regularizačním prvkem,
který vynucuje aby derivace kódů ve vztahu k jejich vstupu byly co možná nejmenší.
Tedy \textbf{dva \emph{podobné} vstupy musí mít vzájemně \emph{podobné} kódy}.
Přesněji je dosaženo lokální invariance na přípustně malé změny vstupních dat.

Citlivost na \emph{drobné rozdíly} ve vstupních datech lze měřit pomocí Frobeniovy normy $\lVert \cdot \rVert_F$ Jacobiho matice enkodéru ($J_f$):
\begin{equation}
    \lVert J_f(x) \rVert^2_F = \sum_{j=1}^{d}\sum_{i=1}^{c} \left( \frac{\partial f_i}{\partial x_j} (x) \right) ^2 .
\end{equation}

Čím vyšší je tato hodnota, tím více bude kód nestabilní s ohledem na \emph{drobné rozdíly} ve vstupních datech.
Z této metriky je následně sestaven \textbf{regularizační prvek} který je připočten k hodnotě ztrátové funkce Contractive Autoenkodéru:

\begin{equation}
    \Omega_{CAE} (W, b, S) = \sum_{x \in S}^{} \lVert J_f(x) \rVert^2_F .
\end{equation}

Výsledkem je tedy Autoenkodér, jehož dva (lokálně) \emph{podobné} vstupy musejí mít i \emph{podobný} kód.
Z Contractive autoenkodéru lze rovněž vzorkovat nové výstupy.
Z takto naučeného modelu Autoenkodéru lze generovat nové instance dat:
Jakobián (\emph{Jacobiho determinant}) enkodéru je (jako \emph{drobný šum}) přičten ke kódu vstupu.
Takto modifikovaný kód je poté dekodérem přetaven na výstup a dostáváme nový vzorek dat.


\subsection{Stochastický autoenkodér}
Struktura stochastického autoenkodéru se vůči struktuře představené v \ref{fig:basic_autoencoder_structure} liší reprezentací enkodér a dekodér modulů.
V stochastickém autoenkodéru jsou enkodér a dekodér moduly reprezentovány rozdělením pravděpodobností (nejedná se tedy pouze o funkce, ale moduly zahrnují i určitou míru šumu).
Výstup těchto modulů tedy obdržíme \textbf{výběrem z příslušného rozdělení pravěpodobnosti}.

Mějme skrytou vrstvu (\emph{kód}) $\textbf{\emph{h}}$. Obecně má pro enkodér toto rozdělení podobu $p_{enkodér}(\textbf{\emph{h}}\mid\textbf{\emph{x}})$ a pro dekodér $p_{dekodér}(\textbf{\emph{x}}\mid\textbf{\emph{h}})$.
Modifikací základní struktury autoenkodéru (\ref{fig:basic_autoencoder_structure}) tedy dostáváme:

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[round/.style={circle, draw, minimum size=10mm, node distance=25mm, on grid}]
        \node[round](h){h};
        \node[round](x)[below left of=h]{x};
        \node[round](r)[below right of=h]{r};
        
        \draw[-Triangle] (x) -- node [left] {$p_{enkodér}(\textbf{\emph{h}}\mid\textbf{\emph{x}})$} (h);
        \draw[-Triangle] (h) -- node [right] {$p_{dekodér}(\textbf{\emph{x}}\mid\textbf{\emph{h}})$} (r);
    \end{tikzpicture}
    \caption{Obecná struktura Autoenkodéru. Ze vstupu $\emph{x}$ je enkodérem vytvořen kód $\emph{h}$} (funkce $\emph{f}$). Tento kód je následně dekodérem přetaven na rekonstrukci $\emph{r}$ (funkce $\emph{g}$).
    \label{fig:stochastic_autoencoder_structure}
\end{figure}


V tradiční dopředné umělé neuronové síti \autoref{sec:feedforward_nn} je běžnou strategií pro návrh výstupní vrstvy definování (\emph{výstupního}) rozdělení pravděpodobnosti $p(\textbf{\emph{y}} \mid \textbf{\emph{x}})$. Pro návrh ztrátové funkce pak minimalizace záporného logaritmu věrohodnosti $-\log p(\textbf{\emph{y}}\mid\textbf{\emph{x}})$.
V takové architektuře je $\textbf{\emph{x}}$ vektor vstupních dat a  $\textbf{\emph{y}}$ vektor cílových proměnných, které se umělá neuronová síť snaží předpovědět (např. štítků jednotlivých tříd).

S architekturou autoenkodérů se však pojí jeden zásadní rozdíl. V autoenkodéru \textbf{je $\textbf{\emph{x}}$ jak vstupní, tak cílová proměnná}.

Dekodér pak lze interpretovat jako modul poskytující podmíněné rozdělení $p_{dekodér}(\textbf{\emph{x}} \mid \textbf{\emph{h}})$.
Autoenkodér lze trénovat minimalizací $-\log p_{dekodér}(\textbf{\emph{x}} \mid \textbf{\emph{h}})$.
Konkrétní podoba ztrátové funkce se odvíjí od požadovaných vlastností autoenkodéru a od přesné podoby modulu dekodéru
(pokud hodnoty $\textbf{\emph{x}} \in \mathbb{R}$, pak jsou pro parametrizaci normálního rozdělení použity linéarní výstupní jednotky,
tedy $-\log p_{dekodér}(\textbf{\emph{x}} \mid \textbf{\emph{h}})$ vrací \emph{střední kvadtratickou chybu}).

Stochastický autoenkodér tedy \textbf{generalizuje kódovací funkci} $f(x)$ na \textbf{kódovací rozdělení pravděpodobnosti} $p_{enkodér}(\textbf{\emph{h}}\mid\textbf{\emph{x}})$.

Enkodér a dekodér moduly stochastického autoenkodéru tedy lze považovat za \textbf{modely využívající latentní proměnné} (\emph{latent variable models}, viz \autoref{sec:latent_variable_models}).
Model využívající latentní proměnné značíme $p_{model}(\textbf{\emph{h}}, \textbf{\emph{x}})$.

\textbf{Stochastický enkodér} je pak definován následovně:
\begin{equation}
    p_{enkodér}(\textbf{\emph{h}}\mid\textbf{\emph{x}}) = p_{model}(\textbf{\emph{h}}, \textbf{\emph{x}})
\end{equation}

a \textbf{stochastický dekodér} následovně:
\begin{equation}
    p_{dekodér}(\textbf{\emph{x}}\mid\textbf{\emph{h}}) = p_{model}(\textbf{\emph{x}}, \textbf{\emph{h}})
\end{equation}



\subsection{Další typy autoenkoderů}
\subsection{Taxonomie autoenkodérů}
Bylo představeno několik tříd Autoenkoderů (rozdělení dle navržení ANN sítě, rozdělení dle regularizačního prvku) a následně popsáno několik dalśích druhú AE.
Zde je jejich (nevyčerpávající) taxonomie.

\tikzset{
	basic/.style  = {draw, text width=5cm, rectangle},
	root/.style   = {basic, thin, align=center},
	level-2/.style = {basic, thin, align=center, text width=4.5cm},
	level-3/.style = {basic, thin, align=center, text width=3.75cm}
}
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[
			level 1/.style={sibling distance=13em, level distance=6em},
			edge from parent/.style={->,solid,black,thick,sloped,draw}, 
			edge from parent path={(\tikzparentnode.south) -- (\tikzchildnode.north)},
		>=latex, node distance=1.5cm, edge from parent fork down]
    
    \node[root] {\textbf{Autoenkodér}}
        child {node[level-2] (c1) {\textbf{Dle kompozice skryté vrstvy}}}
        child {node[level-2] (c2) {\textbf{Dle formulace regularizačního prvku}}}
        child {node[level-2] (c3) {\textbf{Dle kategorie TBA}}};

    \begin{scope}[every node/.style={level-3}]
        \node [below of = c1, xshift=40pt] (c11) {Autoenkodér s neúplnou skrytou vrstvou};
        \node [below of = c11] (c12) {Autoenkodér s rozšířenou skrytou vrstvou};
        \node [below of = c12] (c13) {Hluboký autoenkodér};
        
        \node [below of = c2, xshift=40pt] (c21) {Řídký autoenkodér};
        \node [below of = c21] (c22) {Denoising autoenkodér};
        \node [below of = c22] (c23) {Contractive autoenkodér};
        \node [below of = c23] (c24) {Stochastický enkodér-dekodér};
        
        \node [below of = c3, xshift=40pt] (c31) {item 3-1};
        \node [below of = c31] (c32) {item 3-2};
        \node [below of = c32] (c33) {item 3-3};
        \node [below of = c33] (c34) {item 3-4};
        \node [below of = c34] (c35) {item 3-5};
    \end{scope}

    \foreach \value in {1,...,3}
        \draw[->] (c1.195) -- (c1\value.west);

    \foreach \value in {1,...,4}
        \draw[->] (c2.195) -- (c2\value.west);
      
    \foreach \value in {1,...,5}
        \draw[->] (c3.195) -- (c3\value.west);
		  
	\end{tikzpicture}
	\caption{Autoenkodéry rozděleny dle charakteristik zpracování kódovací vrstvy}
	\label{fig:autoencoder_taxonomy}
\end{figure}

\subsection{Využítí Autoenkodéru}

\begin{itemize}
    \item Mapování vysokorozměrných dat do 2D pro vizualizaci
    \item Učení se abstraktních vlastností o vstupních datech bez učitele, pro následné využití v supervizovaných úlohách
    \item Komprese
\end{itemize}