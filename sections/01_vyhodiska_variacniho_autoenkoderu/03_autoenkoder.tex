\section{Autoenkodér}
Autoenkodér je typ umělé neuronové sítě se schopností učit se efektivní reprezentace vstupních dat bez učitele. Umělá neuronová síť Autoenkodéru má symetrickou strukturu a skrytou vrstvu \emph{h}, která popisuje \emph{kód} použitý pro reprezentaci vstupu.
Architekturu Autoenkodéru (viz \ref{fig:basic_autoencoder_structure}) lze principiálně rozdělit na dvě části – kódovací funkci $h = f(x)$, resp. \textbf{enkodér}
a dekódovací funkci $r = g(h)$, resp. \textbf{dekodér}.
Hovoříme tedy o typu umělé neuronové sítě s \emph{enkodér-dekodér} moduly.
Výstupem enkodéru je \textbf{kód} vstupu $\emph{h}$. Výstupem dekodéru je \textbf{rekonstrukce} vstupu \emph{r}.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[round/.style={circle, draw, minimum size=10mm, node distance=25mm, on grid}]
        \node[round](h){h};
        \node[round](x)[below left of=h]{x};
        \node[round](r)[below right of=h]{r};
        
        \draw[-Triangle] (x) -- node [above] {$f$} (h);
        \draw[-Triangle] (h) -- node [above] {$g$} (r);
    \end{tikzpicture}
    \caption{Obecná struktura Autoenkodéru. Ze vstupu $\emph{x}$ je enkodérem vytvořen kód $\emph{h}$} (funkce $\emph{f}$). Tento kód je následně dekodérem přetaven na rekonstrukci $\emph{r}$ (funkce $\emph{g}$).
    \label{fig:basic_autoencoder_structure}
\end{figure}

Obecnou strukturu (viz \ref{fig:basic_autoencoder_structure}) lze reprezentovat dopřednou umělou neuronovou sítí.
Jejím cílem je \textbf{rekonstruovat vstupní data na výstupní vrstvě}. Počet vstupů je tak totožný s počtem neuronů ve výstupní vrstvě umělé neuronové sítě (tedy $x$ a $r$ mají stejnou dimenzi).
\emph{h} může mít \emph{menší} či \emph{větší} dimenzi – volba dimenze \emph{h} se odvíjí od požadovaných vlastností Autoenkodéru.
Obecná architektura modulů umělé neuronové sítě Autoenkodéru je zachycena v \ref{fig:autoencoder}.


\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \node[trapezium,
            draw,
            text=black,
            trapezium angle=20,
            trapezium stretches=true,
            minimum height=2.5cm,
            minimum width=5cm,
            rotate=270] (encoder) at (1, 0) {Enkodér};
        
        \node[
            preaction={clip, postaction={draw=black, line width=0.3mm,}},
            minimum height=5cm,
            left=of encoder.center,
        ] (input) at (0, 0) {$\mathbf{x}$};

        \node[
            preaction={clip, postaction={draw=black, line width=0.3mm,}},
            minimum height=18.5mm,
            right=of encoder.north,
        ] (h)  {$\mathbf{h}$}; 

        \node[trapezium,
            draw,
            text=black,
            trapezium angle=20,
            trapezium stretches=true,
            minimum height=2.5cm,
            minimum width=5cm,
            rotate=90] (decoder) at (6, 0) {Dekodér};

        \node[
            preaction={clip, postaction={draw=black, line width=0.3mm,}},
            minimum height=5cm,
            right=of decoder.center,
        ] (output) at (7, 0) {$\mathbf{\hat{x}}$};

        \draw[-Triangle] (input) -- (encoder);
        \draw[-Triangle] (encoder) -- (h);
        \draw[-Triangle] (h) -- (decoder);
        \draw[-Triangle] (decoder) -- (output);
    \end{tikzpicture}
    \caption{Jednotlivé moduly architektury umělé neuronové sítě Autoenkodéru. }
    \label{fig:autoencoder}
\end{figure}

Autoenkodér je trénován k rekonstrukci jeho vstupů.
Pokud by se Autoenkodér naučil jednoduše určit $\mathbf{\emph{x}} = g(f(\mathbf{\emph{x}}))$ pro každé $\emph{x}$, získali bychom \emph{identitu}, která není patřičně užitečná.
Proto je při trénování zavedena řada omezení, jejichž účelem je zabránit možnosti naučení Autoenkodéru perfektně kopírovat vstupní data.


\subsection{Autoenkodér s neúplnou skrytou vrstvou}
Autoenkodér s neúplnou skrytou vrstvou (\emph{Undercomplete autoencoder}) je Autoenkodér, jehož dimenze kódu (\emph{h}) je menší, než dimenze vstupu.
Tuto skrytou vrstvu \emph{h} nazýváme \textbf{bottleneck}. Bottleneck je způsob, kterým se Autoenkodér s neúplnou skrytou vrstvou učí kompresované reprezentaci znalostí. 
V důsledku bottleneck vrstvy je Autoenkodér nucen zachytit pouze ty stěžejní vlastnosti trénovacích dat, které následně budou použity pro rekonstrukci.

Trénovací proces Neuplného autoenkodéru je popsán jako minimalizace ztrátové funkce

\begin{equation}
    L(\mathbf{x}, g(f(\mathbf{x}))),
\end{equation}

kde $L$ je ztrátová funkce, penalizující $g(f(\mathbf{x}))$ za \emph{rozdílnost} vůči $\mathbf{x}$ (např. \emph{střední kvadratická chyba}).


\begin{figure}[H]
    \centering
    \begin{neuralnetwork}[height=4]
        \tikzstyle{input neuron}=[neuron, circle, draw=black, fill=white];
        \tikzstyle{hidden neuron}=[neuron, circle, draw=black, fill=white];
        \tikzstyle{output neuron}=[neuron, circle, draw=black, fill=white];
      
        \inputlayer[count=4, bias=false, title=Enkodér, text=\xin]
      
      
        \hiddenlayer[count=2, bias=false, title=Kód $\emph{h}$]
        \linklayers
      
        \outputlayer[count=4, title=Dekodér, text=\xout]
        \linklayers
      
      \end{neuralnetwork}
    \caption{Triviální architektura umělé neuronové sítě autoenkodéru. Skrytá vrstva představuje \emph{bottleneck}. Jedná se o tzv. \emph{shallow undercomplete Autoenkodér}.}
    \label{fig:autoencoder_bottleneck}
\end{figure}
\subsubsection{Od Analýzy hlavních komponent po Autoenkodér}
Máme-li linéární dekodér (Autoenkodér používá pouze linéárni aktivační funkce) a jako ztrátová funkce $L$ je použita \emph{střední kvadratická chyba},
pak se Neuplný autoenkodér naučí stejný \emph{vektorový prostor}, který by byl výsledkem Analýzy hlavních komponentů \autoref{sec:pca}.
V tomto speciálním případě lze ukázat, že Autoenkodér trénovaný na úloze kompresované reprezentace znalostí jako vedlejší efekt provedl Analýzu hlavních komponentů.

Důležitým důsledkem tohoto jevu je, že \textbf{Autoenkodéry} s nelineární kódovací funkcí $f$
a nelineární dekódovací funkcní $g$ \textbf{jsou schopny učit se obecnější generalizaci}
než u Analýzy hlavních komponent.

Na druhou stranu, má-li Autoenkodér k dispozici příliš mnoho kapacity,
může se naučit kopírovat vstupní data na výstupní vrstvu bez extrakce užitečných (charakteristických) vlastností o rozdělení vstupních dat.

\subsubsection{Problém s naučením identity}
Extrémním případem je teoretický scénář, ve kterém je Autoenkodér složen z kódu (\emph{h}) o jedné vrstvě a velmi výkonného enkodéru.
Takový Autoenkodér by se mohl naučit reprezentovat každý vstup $x_i$ kódem \emph{i}.
Dekodér by se pak tyto indexy mohl naučit mapovát zpátky na hodnoty konkrétních trénovacích vzorků dat.
Tento příklad se v praxi běžně nenaskytne, nicméne jasně ilustruje,
jak může Autoenkodér při úloze kopírování vstupu na výstupní vrstvu selhat naučit se užitečné vlastnosti o vstupních datech, jsou-li restrikce při učení příliš nízké.
Proto je třeba Autoenkodéry regularizovat.

\subsection{Hlubuký autoenkodér}
Hluboký autoenkodér (\emph{stacked} autoenkodér)

\begin{figure}[H]
    \centering
    \begin{neuralnetwork}[height=6]
        \tikzstyle{input neuron}=[neuron, circle, draw=black, fill=white];
        \tikzstyle{hidden neuron}=[neuron, circle, draw=black, fill=white];
        \tikzstyle{output neuron}=[neuron, circle, draw=black, fill=white];
      
        \inputlayer[count=6, bias=false, text=\xin]
        \hiddenlayer[count=4, bias=false]
        \linklayers
        \hiddenlayer[count=2, bias=false]
        \linklayers
        \hiddenlayer[count=4, bias=false]
        \linklayers
        \outputlayer[count=6, text=\xout]
        \linklayers
      \end{neuralnetwork}
    \caption{Stacked Autoenkodér.}
    \label{fig:stacked_autoencoder}
\end{figure}


\subsection{Řídký autoenkodér}

\subsection{Denoising autoenkodér}
\subsection{Contractive autoenkodér}

\subsection{Stochastický enkodér dekodér}

\subsection{Taxonomie autoenkodérů}

\subsection{Historický pohled}
Vícevrstvý Perceptron \autoref{sec:multilayer_perceptron} je univerzálním aproximátorem \autoref{sec:universal_approximation_theorem} – tedy historicky nalézá uplatnění zejména v klasifikačních úlohách učení s učitelem.
Sofistikovaný algoritmus se schopností trénování Vícevrstvého Perceptronu s větším počtem skrytých vrstev stále schází, a to zejména v důsledku problému mizejícího gradientu (\emph{vanishing gradient problem}).
Až příchod algoritmu gradientního sestupu \autoref{sec:gradient_descent}, který adresuje problém mizejícího gradientu v aplikacích s použitím konvolučních sítí \autoref{sec:cnn} a úloh učení se bez učitele, značí počátek moderních metod hlubokého učení.
V oblasti hlubokého učení \autoref{sec:deep_learning} dochází k emergenci a vývoji řady technik pro řešení úloh učení se bez učitele.
V této kapitole je popsána pouze jedna z nich – architektura umělé neuronové sítě založené na \emph{enkodér-dekodér} modulech: Autoenkodér.
Autoenkodéry byly poprvé představeny jako způsob pro předtrénování umělých neuronových sítí (formou automatizované extrakce vlastností \emph{feature extraction}). 
Později Autoenkodéry nalézají uplatnění zejména v úloháh redukce dimenzionality \autoref{sec:dimensionality_reduction} či fůzi vlastností (\emph{feature fusion}).


Nedávné teoretické propojení Autoenkodéru a Modelů využívajících latentní proměnných \autoref{sec:latent_variable_models} však vedlo ke vzniku zcela nové architektury neuronové sítě kombinující charakter redukce dimenzionality Autoenkodéru se statistickými metodami odvozování.
To vyneslo Autoenkodéry na popředí v oblasti generativního modelování – této architektuře je věnována kapitola \autoref{chap:vae}.

Byť Autoenkodéry vznikly v kontextu hlubokého učení, není pravidlem že všechny modely Autoenkodéru obsahují vícero skrytých vrstev. Následuje rozdělení Autoenkodérů dle struktury umělé neuronové sítě.