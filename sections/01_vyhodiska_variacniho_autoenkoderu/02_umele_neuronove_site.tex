\section{Umělá neuronová síť}
\label{neural_network}
Umělá neuronová síť je model strojového učení inspirovaný přírodou.
Zdá se být intuitivní, že chceme-li napodobit lidskou inteligenci, měli bychom se pro inspiraci podívat na architekturu lidského mozku.
V průběhu času se však konstrukce umělých neuronových sítí začala jejich přírodnímu protějšku podstatně vzdalovat.
Řada architektur umělých neuronových sítí tvoří biologicky nerealistický model, byť z této původní myšlenky vychází (stejně tak jako letadla vycházejí z přírodního vzoru létajících ptáků, pohybem svými křídly se ve skutečnosti ve vzduchu neodrážejí). \cite{Geron2019}

Představení kompletních principů umělých neuronových sítí není východiskem variačního autoenkodéru, nýbrž svým obsahem pokrývají několik monografií – pro úvod například \cite{Chollet2017}, \cite{Geron2019}.
V této sekci tedy budou představeny pouze stěžejní techniky využívané autoenkodéry.
\subsection{Anatomie umělé neuronové sítě}
Proces trénování umělé neuronové sítě z pravidla zahrnuje následující objekty \cite{Chollet2017}:
\begin{itemize}
    \item \emph{Vrstvy} ze kterých je následně složena \emph{síť} (resp. \emph{model})
    \item \emph{Vstupní data} (a případně jejich \emph{cílové třídy})
    \item \emph{Ztrátová funkce}, která slouží jako signál zpětné vazby použití pro učení modelu
    \item \emph{Optimizér}, který modifikuje parametry umělé neuronové sítě (např. váhy)
\end{itemize}

\subsection{Perceptron}
\label{sec:perceptron}
Jedna z nejjednodušších architektur umělé neuronové sítě (a zároveň \emph{model umělého neuronu}) představena v \cite{Rosenblatt1957} inspirována principy Hebbovského učení \cite{Hebb1949}.
Perceptron přichází s důležitým principem \textbf{numerických hodnot vstupů, výstupů} (oproti pouhým binárním hodnotám) – tzv \emph{linear treshold unit, LTU} a \textbf{vah} mezi jednotlivými neurony. 
Perceptron byl později kritizován \cite{Minsky1969} za jeho neschopnost řešit triviální problémy (např. \emph{XOR} klasifikace), což eventuálně vedlo ke krátkodobé stagnaci konekcionismu.

Jak se ale ukázalo, některé z těchto limitací lze vyřešit uspořádáním více Perceptronů za sebe do vrstev. \cite{Rumelhart1987}

Takto uspořádaná umělá neuronová síť se nazývá Vícevrstvý Perceptron.

\subsection{Vícevrstvý Perceptron}
Vícevrstvý Perceptron je tvořen umělou neuronovou sítí, která se skládá z jedné vstupní vrstvy, \textbf{jedné nebo více skrytých vrstev} LTU jednotek, a jedné výstupní vrstvy rovněž složené z LTU jednotek.
Součástí každé vrstvy (vyjma výstupní) je tzv. \textbf{bias} neuron který je \textbf{plně propojený} s další vrstvou sítě. \cite{Geron2019}

Pokud má umělá neuronová síť dvě a více skrytých vrstev, nazýváme ji \textbf{hlubokou neuronouvou sítí}.

Jak již víme, Vícevrstvý Perceptron adresuje limitace Perceptronu (viz \autoref{sec:perceptron}).
Článek \cite{Rumelhart1987} ale představil i další revoluční myšlenku – algoritmus \textbf{zpětné propagace}
\footnote{Algoritmus zpětné propagace byl ve skutečnosti nezávisle objeven více výzkumníky, počínaje \cite{Werbos1974}. Převzato z \cite{Geron2019}.} (\emph{backpropagation}).

Algoritmus zpětné propagace lze velmi zjednodušeně interpretovat následovně: Pro každou trénovací instanci je vypočten výstup každého jejího neuronu každé jednotlivé vrstvy.
Poté je změřena výstupní chyba celé sítě (například střední kvadratická chyba, MSE) a vypočten podílem, jakým každý neuron poslední skryté vrstvy přispěl k hodnotě každého neuronu výstupní vrstvy.
Následně je stejným způsobem měřeno, jak moc byla ovlivněna hodnota jednotlivých neuronů poslední skryté vrstvy hodnotami neuronů předchozí skryté vrstvy – a podobný proces se opakuje než algoritmus dosáhne vstupní vrstvy. \cite{Geron2019}

Průchod algoritmu sítí lze intuitivně popsat následovně: Pro každou trénovací instanci algoritmus zpětné propagace nejprve učiní predikci cílové hodnoty – tzv. \emph{forward pass}).
Poté změří chybu této predikce. Následně zpětně projde každou vrstvu sítě za účelem změření míry přispění k této chybě každým propojením (a její váhou) – tzv. \emph{reverse pass}.
Závěrem algoritmus \emph{lehce} upraví váhy jednotlivých propojení za účelem snížení chyby – tzv. \emph{Gradient Descent step}. \cite{Geron2019}

Pro správný chod algoritmu autoři \cite{Rumelhart1987} do architektury Vícevrstvého Perceptronu zanesli další zásadní změnu – jako tzv. \textbf{aktivační funkci} využili logistickou regresi (zcela běžné je využití i dalších aktivačních funkcí, například ReLU).