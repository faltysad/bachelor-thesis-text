\section{Umělá neuronová síť}
\label{sec:neural_network}
Umělá neuronová síť je model strojového učení inspirovaný přírodou.
Zdá se být intuitivní, že chceme-li napodobit lidskou inteligenci, měli bychom se pro inspiraci podívat na architekturu lidského mozku.
V průběhu času se však konstrukce umělých neuronových sítí začala jejich přírodnímu protějšku podstatně vzdalovat.
Řada architektur umělých neuronových sítí tvoří biologicky nerealistický model, byť z této původní myšlenky vychází (stejně tak jako letadla vycházejí z přírodního vzoru létajících ptáků, pohybem svými křídly se ve skutečnosti ve vzduchu neodrážejí). \cite{Geron2019}

V této sekci jsou představeny pouze stěžejní techniky využívané autoenkodéry. Detailní úvod do neuronových sítí nabízí \textcite{Chollet2017, Geron2019}.

\textbf{Model variačního autoenkodéru} je složen z dvou modulů - \textbf{enkodér} modul a \textbf{dekodér modul}.
Tyto moduly jsou detailněji později popisuje \autoref{chap:vae}.
Nicméně je důležité předeslat, že jsou implementovány formou \textbf{umělých neuronových sítí} a pro účely jejich sestavení tato sekce prezentuje potřebné principy.

\subsection{Architektura umělé neuronové sítě}
\label{sec:neural_network_architecture}
Pojmem \textbf{architektura} rozumíme \textbf{celkovou strukturu} umělé neuronové sítě – počet neuronů a způsob jakým jsou tyto neurony mezi sebou propojeny, jejich aktivační funkce a podobně.

Většina umělých neuronových sítí je organizována do skupin neuronů zvaných \textbf{vrstvy}.
Architektury umělých neuronových sítí poté uspořádávají tyto vrstvy do zřetězené struktury, kde každá vrstva je funkcí předcházející vrstvy. \cite{Goodfellow2016}

Mezi hlavní architektonická rozhodnutí při návrhu umělé neuronové sítě patří volba \textbf{hloubky sítě} a \textbf{šířky každé z vrstev}.

Proces trénování umělé neuronové sítě z pravidla zahrnuje následující objekty \cite{Chollet2017}:
\begin{itemize}
    \item \emph{Vrstvy} ze kterých je následně složena \emph{síť} (resp. \emph{model}).
    \item \emph{Vstupní data} (a případně jejich \emph{cílové třídy}).
    \item \emph{Ztrátová funkce}, která slouží jako signál zpětné vazby použití pro učení modelu.
    \item \emph{Optimizér}, který modifikuje parametry umělé neuronové sítě (např. váhy).
\end{itemize}

\subsection{Dopředná umělá neuronová síť}
\label{sec:feedforward_nn}
Dopředná umělá neuronová síť je velmi podstatná pro návrh (hlubokých) modelů strojového učení.
Cílem dopředné umělé neuronové sítě je aproximovat nějakou funkci $f*$.
\footnote{Například pro klasifikátor, $y=f*(x)$ mapuje vstup $x$ do kategorie $y$. Převzato z \cite{Goodfellow2016}.}
Dopředná umělá neuronová síť definuje mapovací funkci $\textbf{\emph{y}} = \emph{f}(\textbf{\emph{x}};\boldsymbol{\theta})$ a učí se hodnotu parametrů $\boldsymbol{\theta}$, jejichž výsledky je nejlepší aproximace cílové funkce. \cite{Goodfellow2016}

\textbf{Dopředné} umělé neuronové \textbf{sítě} nazýváme:
\begin{itemize}
    \item \emph{Dopředné}, jelikož v této architektuře neexistují žádné zpětnovazební propojení, které by sloužily jako vstup zpět pro sebe sama
    \footnote{Je-li dopředná umělé neuronové sítě obohacena o zpětnovazební propojení se sebou sama, nazýváme ji \textbf{Rekurentní neuronová síť}.}.
    \item \emph{Sítě}, jelikož jsou typicky reprezentovány kompozicí více různých funkcí.
\end{itemize}

Model takové sítě je asociován s orientovaným acyklickým grafem, který popisuje, jakým způsobem je tato kompozice tvořena.
Mějme tři funkce $f^{(1)}$, $f^{(2)}$, $f^{(3)}$, které jsou zřetězeny následovně: $f(\textbf{\emph{x}}) = f^{(3)}(f^{(2)}(f^{(1)}(\textbf{\emph{x}})))$.
Takto zřetězené struktury nazýváme \textbf{vrstvy} umělé neuronové sítě. \cite{Goodfellow2016}

Délku tohoto zřetězení nazýváme \textbf{hloubkou} modelu – odtud \emph{hluboké} neuronové sítě (umělé neuronové sítě s 2 a více skrytými vrstvami). \cite{Goodfellow2016}
\footnote{V tomto případě je $f^{(1)}$ \textbf{první vrstva}, $f^{(2)}$ je \textbf{druhá vrstva} a $f^{(3)}$ \textbf{třetí vrstva}.}.

Variační autoenkodér využívá dopředného průchodu modelu své umělé neuronové sítě při generování výstupů. \cite{Kingma2014}

\subsection{Perceptron}
\label{sec:perceptron}
Jedná se o jednu z nejjednodušších architektur umělé neuronové sítě (a \emph{model umělého neuronu}) představenou v \cite{Rosenblatt1957} inspirovanou principy Hebbovského učení \cite{Hebb1949}.
Perceptron přichází s důležitým principem \textbf{numerických hodnot vstupů, výstupů} (oproti pouhým binárním hodnotám v \textcite{McCulloch1943}) – tzv. \emph{linear treshold unit, LTU} a \textbf{vah} propojení mezi jednotlivými neurony. 
LTU budou nadále v této práci označovány jako \textbf{neuron}.

Perceptron byl později kritizován \cite{Minsky1969} za jeho neschopnost řešit triviální problémy (např. \emph{XOR} klasifikace), což eventuálně vedlo ke krátkodobé stagnaci konekcionismu\footnote{Konekcionismus je směr v oblasti umělé inteligence, který se pokouší o interpretaci biologického, lidského, mozku a jeho intelektuálních schopností použitím umělých neuronů a neuronových sítí. \cite{Rumelhart1986}}. \cite{Goodfellow2016}

Jak se ale ukázalo, některé z těchto limitací lze vyřešit uspořádáním více perceptronů za sebe do vrstev. \cite{Rumelhart1987}

Takto uspořádaná umělá neuronová síť se nazývá vícevrstvý perceptron.


\subsection{Vícevrstvý perceptron a algoritmus zpětné propagace}
\label{sec:multilayer_perceptron}
Vícevrstvý Perceptron je tvořen umělou neuronovou sítí, která se skládá z jedné vstupní vrstvy, \textbf{jedné nebo více skrytých vrstev} neuronů, a jedné výstupní vrstvy rovněž složené z neuronů.
Součástí každé vrstvy (vyjma výstupní) je tzv. \textbf{bias} neuron. Jednotlivé neurony jsou \textbf{plně propojený} s neurony v dalších vrstvách sítě. \cite{Geron2019}

Článek \cite{Rumelhart1987} ale představil i další revoluční myšlenku – algoritmus \textbf{zpětné propagace}
\footnote{Algoritmus zpětné propagace byl ve skutečnosti nezávisle objeven více výzkumníky, počínaje \cite{Bryson1969}.} (\emph{backpropagation}).

Průchod algoritmu sítí lze intuitivně popsat následovně: Pro každou trénovací instanci algoritmus zpětné propagace nejprve učiní predikci cílové hodnoty – tzv. \emph{forward pass}.
Poté změří chybu této predikce. Následně zpětně projde každou vrstvu sítě za účelem změření míry přispění k této chybě každým propojením (a jeho vahou) – tzv. \emph{reverse pass}.
Závěrem algoritmus \emph{lehce} upraví váhy jednotlivých propojení za účelem snížení chyby – tzv. \emph{Gradient Descent step}. \cite{Geron2019}

Pro správný chod algoritmu autoři \cite{Rumelhart1987} do architektury vícevrstvého Perceptronu zanesli další zásadní změnu – jako tzv. \textbf{aktivační funkci} využili logistickou regresi (zcela běžné je využití i dalších aktivačních funkcí, například ReLU).

Algoritmus zpětné propagace je \textbf{podstatnou součástí trénovací fáze modelů variačního autoenkodéru} a je využit při tzv. \textbf{reparametrizačním triku}, který popisuje \autoref{sec:reparametrization_trick}.

\subsection{Univerzální aproximační teorém}
\label{sec:universal_approximation_theorem}
Univerzální aproximační teorém\footnote{Universal approximation theorem} \cite{Hornik1989, Cybenko1989} tvrdí, že dopředná umělá neuronová síť (s alespoň jednou skrytou vrstvou a \emph{potlačující} aktivační funkcí – např. logistická funkce) je schopna aproximovat libovolnou (borelovsky měřitelnou) funkci s \emph{libovolnou mírou přesnosti}\footnote{Derivacemi dopředné umělé neuronové sítě lze stejně tak aproximovat derivace cílové funkce \cite{Hornik1990}.}.

Důsledkem Univerzálního aproximačního teorému je, že dostatečně velký \textbf{vícevrstvý perceptron zvládne \emph{reprezentovat} libovolnou funkci}.
Ale neexistuje zde jistota, že se algoritmus strojového učení bude schopen takovou funkci \emph{vždy} naučit \cite{Goodfellow2016}:

\begin{itemize}
    \item Zvolený optimizér algoritmu strojového učení použitý pro trénování nemusí být schopen najít parametry umělé neuronové sítě, které korespondují s cílovou funkcí.
    \item Algoritmus nemusí vybrat správnou funkci v důsledku přeučení.
\end{itemize}

Dopředné umělé neuronové sítě tak nabízejí univerzální systém pro \textbf{reprezentaci} libovolné funkce. \textbf{Pro libovolnou funkci existuje dopředná umělá neuronová síť, která ji aproximuje}
\footnote{S ohledem na \autoref{sec:no_free_lunch} neexistuje žádný univerzální způsob pro prozkoumání trénovací množiny dat a zvolení funkce, která bude libovolně přesně generalizovat na instance dat, které nejsou součástí této trénovací množiny.}.