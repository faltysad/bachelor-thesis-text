\section{Historický pohled}
Vícevrstvý Perceptron (viz \autoref{sec:multilayer_perceptron})
je univerzálním aproximátorem – tedy historicky nalézá uplatnění zejména v klasifikačních úlohách učení s učitelem.
Sofistikovaný algoritmus se schopností trénování Vícevrstvého Perceptronu s větším počtem skrytých vrstev stále schází, a to zejména v důsledku problému mizejícího gradientu\footnote{Vanishing gradient problem} \cite{Hochreiter1998}.
Až příchod algoritmu gradientního sestupu, který adresuje problém mizejícího gradientu v aplikacích konvolučních sítí \cite{LeCun1989} (a později i úloh učení bez učitele) značí počátek moderních metod hlubokého učení.
V oblasti hlubokého učení tak přirozeně dochází k emergenci a vývoji řady technik pro řešení úloh učení bez učitele.
V této kapitole je popsána pouze jedna z nich – architektura umělé neuronové sítě založené na \emph{enkodér-dekodér} modulech: \textbf{autoenkodér}.
Autoenkodéry byly poprvé představeny jako způsob pro předtrénování umělých neuronových sítí formou automatizované extrakce vlastností (viz \autoref{sec:feature_extraction}). 
Později Autoenkodéry nalézají uplatnění zejména v úlohách redukce dimenzionality (viz \autoref{sec:dimensionality_reduction}) či fůzi vlastností (\emph{feature fusion}). \cite{Charte2018}


Nedávné teoretické propojení autoenkodéru a modelů využívajících latentní proměnných (viz \autoref{sec:latent_variable_models}) však vedlo ke vzniku zcela nové architektury neuronové sítě kombinující charakter redukce dimenzionality autoenkodéru a pravděpodobnostní modely (viz \autoref{sec:probabilstic_models}) – \textbf{variačního autoenkodéru}.
To vyneslo autoenkodéry na popředí v oblasti generativního modelování. Představení variačního autoenkodéru je věnována \autoref{chap:vae} a možnostem jeho využití celý zbytek práce.

Byť autoenkodéry vznikly v kontextu hlubokého učení, není pravidlem že všechny modely autoenkodéru obsahují vícero skrytých vrstev. Následuje rozdělení autoenkodérů, mimo jiné, dle struktury jejich umělé neuronové sítě.
