\section{Autoenkodér s neúplnou skrytou vrstvou}
\label{sec:undercomplete_autoencoder}
Autoenkodér s neúplnou skrytou vrstvou (\emph{undercomplete autoencoder}) je autoenkodér, jehož dimenze kódu (\emph{h}) je menší, než dimenze vstupu.
Tuto skrytou vrstvu \emph{h} nazýváme \textbf{bottleneck}. Bottleneck je způsob, kterým se autoenkodér s neúplnou skrytou vrstvou učí kompresované reprezentaci znalostí. 
V důsledku bottleneck vrstvy je autoenkodér nucen zachytit pouze salientní vlastnosti trénovacích dat, které následně budou použity pro rekonstrukci. \cite{Goodfellow2016}

Trénovací proces Neuplného autoenkodéru je popsán jako minimalizace ztrátové funkce \cite{Charte2018}:

\begin{equation}
    \mathcal{L}(\mathbf{x}, g(f(\mathbf{x}))),
\end{equation}

kde $\mathcal{L}$ je ztrátová funkce, penalizující $g(f(\mathbf{x}))$ za \emph{rozdílnost} vůči $\mathbf{x}$ (např. \emph{střední kvadratická chyba}).


\begin{figure}[H]
    \centering
    \begin{neuralnetwork}[height=4]
        \tikzstyle{input neuron}=[neuron, circle, draw=black, fill=white];
        \tikzstyle{hidden neuron}=[neuron, circle, draw=black, fill=white];
        \tikzstyle{output neuron}=[neuron, circle, draw=black, fill=white];
      
        \inputlayer[count=4, bias=false, title=Enkodér, text=\xin]
      
      
        \hiddenlayer[count=2, bias=false, title=Kód $\emph{h}$]
        \linklayers
      
        \outputlayer[count=4, title=Dekodér, text=\xout]
        \linklayers
      
      \end{neuralnetwork}
    \caption{Jednoduchá architektura umělé neuronové sítě Autoenkodéru s neúplnou skrytou vrstvou. Skrytá vrstva představuje \emph{bottleneck}.}
    \label{fig:autoencoder_bottleneck}
\end{figure}
\subsubsection{Od Analýzy hlavních komponent po Autoenkodér}
Máme-li linéární dekodér (Autoenkodér používá pouze linéárni aktivační funkce) a jako ztrátová funkce $\mathcal{L}$ je použita \emph{střední kvadratická chyba},
pak se neuplný autoenkodér naučí stejný \emph{vektorový prostor}, který by byl výsledkem Analýzy hlavních komponent (viz \autoref{sec:pca}).
V tomto speciálním případě lze ukázat, že autoenkodér trénovaný na úloze kompresované reprezentace znalostí jako vedlejší efekt provedl Analýzu hlavních komponent. \cite{Baldi1989}, \cite{Kamyshanska2013}

Důležitým důsledkem tohoto jevu je, že \textbf{Autoenkodéry} s nelineární kódovací funkcí $f$
a nelineární dekódovací funkcní $g$ \textbf{jsou schopny učit se obecnější generalizaci}
než u Analýzy hlavních komponent. \cite{Goodfellow2016}

Na druhou stranu, má-li autoenkodér k dispozici příliš kapacity,
může se naučit pouze kopírovat vstup na výstupní vrstvě bez extrakce salientních vlastností (tedy identické zobrazení).

\subsubsection{Problém s naučením pouhého identického zobrazení}
\label{sec:identity}
Extrémním případem je teoretický scénář, ve kterém je Autoenkodér složen z kódu (\emph{h}) o jedné vrstvě a velmi výkonného enkodéru.
Takový Autoenkodér by se mohl naučit reprezentovat každý vstup $x_i$ kódem \emph{i}.
Dekodér by se pak tyto indexy mohl naučit mapovat zpátky na hodnoty konkrétních trénovacích vzorků dat.
Tento příklad se v praxi běžně nenaskytne, nicméne jasně ilustruje,
jak může Autoenkodér při úloze kopírování vstupu na výstupní vrstvu selhat naučit se užitečné vlastnosti o vstupních datech, jsou-li restrikce při učení příliš nízké. \cite{Goodfellow2016}

Proto je nutné \textbf{autoenkodéry regularizovat} (viz \autoref{sec:regularization}).

V dalších sekcích (\autoref{sec:sparse_autoencoder} - \autoref{sec:contractive_autoencoder}) jsou tedy představeny přístupy k architekturám Autoenkodérů \textbf{s využitím regularizace}.