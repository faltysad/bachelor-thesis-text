\section{Reparametrizační trik}
\label{sec:reparametrization_trick}
Reparametrizační trik
\footnote{Reparametrizační trik \emph{funguje} pouze pakliže lze vzorkovat z $Q(z\mid X)$ vyhodnocením funkce $h(\eta, X)$, kde $\eta$ je šum (z distribuce jejíž parametry nejsou učeny z dat). $h$ také musí být spojitá na $X$, abychom skrze něj mohli provádět zpětnou propagaci.
To, mimo jiné, znamená, že $Q(z\mid X)$ a tím padem i $P(z)$ \textbf{nemohou být diskrétní rozdělení}. V opačném případě by došlo k nespojitosti prostoru vzorků $Q$ – a tedy neschopnosti generativního modelu generovat vzorky v celém rozsahu, včetně vzorků které nebyl součástí dat (běžný problém autoenkodérů, které uvedla \autoref{chap:autoencoder}.)}
spočívá v přesunutí procesu vzorkování do vstupní vrstvy.

Mějme $\mu(X)$ a $\Sigma(X)$ – \textbf{střední hodnotu} a \textbf{kovarianci} $Q(z\mid X)$.
Pak je možné vzorkovat z $\mathcal{N}(\mu(X), \Sigma(X))$ – nejprve provedeme vzorkování z $\epsilon \sim \mathcal{N}(0, I)$ a následně spočteme $z = \mu(X) + \Sigma^{\frac{1}{2}}(X) * \epsilon$.

Tedy finální rovnice, jejíž gradient chceme spočítat má následující tvar:

\begin{equation} \label{eq:vae_reparam_trick}
    \mathds{E}_{X \sim D} \left[ \mathds{E}_{\epsilon \sim \mathcal{N}(0, 1)} \left[ \log P(X\mid z = \mu(X) + \Sigma^{\frac{1}{2}} (X) * \epsilon) \right] - \mathcal{D}_{KL} \left[ Q (z \mid X) \parallel P(z) \right] \right].
\end{equation}

Kýžená vlastnost \autoref{eq:vae_reparam_trick} je, že v modelu její \textbf{umělé neuronové sítě lze provést zpětnou propagaci}. 
Toto je znázorněno v umělé neuronové síti TODO FIGURE NO-BACKPROP 

\footnote{Žádné střední hodnoty ($E$) \autoref{eq:vae_reparam_trick} \textbf{nezávisí} na parametrech modelu. Tedy do těchto středních hodnot můžeme bezpečně převést symbol gradientu a zároveň dodržet jejich rovnost. 
Tedy, mějme neměnné $X$ a $\epsilon$, pak je tato funkce \textbf{spojitá a deterministická} v parametrech $P$ a $Q$, což v důsledku znamená možnost zpětné propagace vypočíst gradient algoritmem stochastického gradientního sestupu. }. 

\begin{algorithm}[H]
    \caption{Stochastic optimization of the ELBO.}\label{alg:reparam_trick}
        \KwData{}
                \hspace{6mm}$\mathcal{D}$: Dataset\\
                \hspace{6mm}$q_\phi(\textbf{z}\mid\textbf{x})$: Inferenční model (enkodér)\\
                \hspace{6mm}$p_\theta(\textbf{x}, \textbf{z})$: Generativní model (dekodér)\\
        \KwResult{}
        \hspace{6mm}$\boldsymbol{\theta}, \boldsymbol{\phi}$: Naučené parametry\\

        $\boldsymbol{\theta}, \boldsymbol{\phi} \gets \text{Inicializace parametrů}$

        \While{SGD not converged}{
            $\mathcal{M} \sim \mathcal{D}$ (Random minibatch of data)\\
            $\boldsymbol{\epsilon} \sim p(\boldsymbol{\epsilon})$ (Random noise for every datapoint in $\mathcal{M}$)\\
            Compute $\tilde{\mathcal{L}}$ and its gradients $\nabla_{\theta,\phi}(\mathcal{M}, \boldsymbol{\epsilon})$\\
            Update $\boldsymbol{\theta}$ and $\boldsymbol{\phi}$ using SGD optimizer
            }
\end{algorithm}