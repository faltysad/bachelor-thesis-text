\section{Interpretace}
Jak bylo ukázáno, proces učení variačních autoenkodérů je efektivně řešitelný problém (\emph{tractable}).

Při učení je optimalizován $\log P(X)$ skrze celou množinu vstupních dat $D$. Nicméně, není optimalizován \emph{přesně} $\log P(X)$, ale pouze jeho odhad.

Tato sekce slouží pro odkrytí mechanismů, které se skutečně na pozadí účelové funkce variačního autoenkodéru dějí.
Je zaměřena na tři otázky spojené s procesem trénování variačního autoenkodéru:
\begin{enumerate}
    \item Jak moc velkou chybu způsobuje současná optimalizace $\mathcal{D}_{KL}\left[ Q(z\mid X) \parallel P(z\mid X) \right]$ dodatečně vedle optimalizace $\log P(X)$.
    \item Interpretace \autoref{eq:vae_objective} v kontextu informační teorie a propojení s dalšími přístupy založenými na Minimal Description Length
    \item Zda-li u variačních autoenkodérů existují regularizační prvky obdobné k autoenkodérům které uvedla \autoref{chap:autoencoder}.
\end{enumerate}

\subsection{Chyba z $\mathcal{D}_{KL}\left[ Q(z\mid X) \parallel P(z\mid X) \right]$}
\textbf{Efektivní řešitelnost} modelu variačního autoenkodéru závisí na předpokladu,
že $Q(z\mid X)$ \textbf{lze modelovat jako Gaussovu funkci se střední hodnotou $\mu(X)$ a rozptylem $\Sigma(X)$}.

Rozdělení $P(X)$ konverguje k původnímu rozdělení (které generuje vstupní data) pouze když se $\mathcal{D}_{KL}\left[ Q(z\mid X) \parallel P(z\mid X) \right]$ limitně blíží k nule.
Což ale nelze jednoduše zaručit. Ani za předpokladu, že $\mu(X)$ a $\Sigma(X)$ jsou vysoko-kapacitní umělé neuronové sítě
\footnote{Vysoko-kapacitní neuronové sítě, jsou sítě s vysokým počtem parametrů a vah, které jim umožňují učit se komplexním vztahům mezi daty. \cite[Kapitola 5]{Goodfellow2016}}
neplatí, že posteriorní rozdělení $P(z\mid X)$ musí být vždy Gaussovo (pro libovolnou funkci $f$ která je použita pro definování $P$).
Tedy, pro neměnné $P$ to znamená, že $\mathcal{D}_{KL}\left[ Q(z\mid X) \parallel P(z\mid X) \right]$ \textbf{nikdy neni rovno nule} (pokud by tato KL divergence byla rovna nule, viz \autoref{eq:elbo_kl}, znamenalo by to perfektní rekonstrukci původních dat).

Nicméně, máme-li dostatečně vysoko-kapacitní neuronové sítě, existuje mnoho funkcí $f$ (viz \autoref{sec:universal_approximation_theorem}), které zaručí, že naučený model generuje libovolné výstupní rozdělení pravděpodobnosti
\footnote{Tedy je schopen generovat vzorky \emph{dostatečně podobné} vzorkům množiny trénovacích dat, včetně vzorků, které nebyly při trénování k dispozici.}.
Libovolná z těchto funkcí maximalizuje $\log P(X)$ stejně dobře. Tedy stačí vybrat jednu z těchto funkcí, která maximalizuje $\log P(X)$ \textbf{a zároveň} zaručí, že $P (z\mid X)$ je Gaussovou funkcí pro všechna $X$.
Pokud toto platí, $\mathcal{D}_{KL}\left[ Q(z\mid X) \parallel P(z\mid X) \right]$, \emph{tlačí} model směrem k parametrizaci původní distribuce.

Zbývá zodpovědět, zda-li taková funkce existuje pro všechny libovolné distribuce, které bychom mohli chtít aproximovat.
Tento problém (v kontextu variačních autoenkodérů) zatím zůstává nezodpovězen. Nicméně existuje alespoň formální důkaz \textbf{nulové chyby aproximace} VAE alespoň v triviálním teoretickém scénáři na 1D problému \cite[Příloha A]{Doersch2021}.
Autoři rámce VAE se domnívají, že budoucí teoretický výzkum by měl být schopný na tomto důkazu stavět a rozšířit jej na více (složitějších) scénářů s praktickým využitím. 