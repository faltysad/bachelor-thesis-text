\section{Generování nových vzorků}

Pro vygenerování nového vzorku $\textbf{\emph{x}}$ z modelu VAE je nejprve nutné získat vzorek $\textbf{\emph{z}}$ z rozdělení pravděpodobnosti kódu $p_{model}(\textbf{\emph{z}})$.
Poté je tento vzorek vstupem pro síť generátoru $g(\textbf{\emph{z}})$. Na konec je $\textbf{\emph{x}}$ vzorkováno z rozdělení pravděpodobnosti $p_{model}(\textbf{\emph{x}};g(\textbf{\emph{z}})) = p_{model}(\textbf{\emph{x}}\mid\textbf{\emph{z}})$. \cite{Kingma2014}

Při trénování je však pro získání $\textbf{\emph{z}}$ použit enkodér (\emph{approximate inference network}) $q(\textbf{\emph{z}}\mid\textbf{\emph{x}})$ a na $p_{model}(\textbf{\emph{x}}\mid\textbf{\emph{z}})$ pak lze pohlížet jako na dekodér. \cite{Kingma2014}

Klíčovým principem VAE je, že může být trénován maximalizací variační dolní meze (\emph{variational lower bound}) $\mathcal{L}(q)$ asociovanou s datovým bodem $\textbf{\emph{x}}$ následovně \cite{Goodfellow2016}: 

\begin{align}
    \mathcal{L}(q) &= \overbrace{ \mathds{E}_{\textbf{\emph{z}} \sim q(\textbf{\emph{z}}\mid\textbf{\emph{x}})} \log p_{model}(\textbf{\emph{z}}, \textbf{\emph{x}}) }^\text{joint log-likelihood} + \overbrace{ \mathcal{H}(q(\textbf{z} \mid \textbf{\emph{x}})) }^\text{entropy} \label{eq:sampling_vae_1} \\
    &= \mathds{E}_{\textbf{\emph{z}} \sim q(\textbf{\emph{z}}\mid\textbf{\emph{x}})} \log p_{model}(\textbf{\emph{x}}\mid\textbf{\emph{z}}) + D_{KL}(q(\textbf{z} \mid \textbf{\emph{x}})\parallel p_{model}(\textbf{z}))) \label{eq:sampling_vae_2} \\
    &\leq \log p_{model}(\textbf{\emph{x}})
\end{align}

První člen \autoref{eq:sampling_vae_1} ukazuje \emph{joint log-likelihood} viditelných a skrytých proměnných pod \emph{approximate posterior} skrze latentní proměnné (stejně jako u EM, s rozdílem že používáme \emph{approximate} místo \emph{exact} posterior).

Druhý člen \autoref{eq:sampling_vae_1} ukazuje entropii \emph{approximate posterior}.
Je-li $q$ Gaussova distribuce, a je-li k predikované střední hodnotě přičten šum, maximalizace této entropie vede k zvyšující se směrodatné odchylce tohoto šumu.
Obecně, prvek této entropie zajistí tendenci \emph{variational posterior} přidělit vysokou hodnotu funkce pravdepodobnostni hodnotám $\textbf{\emph{z}}$ které by mohly generovat $\textbf{\emph{x}}$ (raději než konvergovat pouze k odhadu jedné hodnoty s \emph{největší pravděpodobností}).

První člen \autoref{eq:sampling_vae_2} ukazuje \emph{log-likelihood} rekonstrukce, který lze nalézt i v ostatních typech autoenkodérů (\autoref{chap:autoencoder}). 

Druhý člen \autoref{eq:sampling_vae_2} zajišťuje, aby se \emph{approximate posterior distribution} $q(\textbf{z}\mid\textbf{\emph{x}})$ a model prior $p_{model}(\textbf{\emph{z}})$ k sobě blížily. \cite{Goodfellow2016}

Tradiční přístupy k variačnímu odvozování a učení odvozují $q$ pomocí optimalizačního algoritmu (typicky iterují skrze soustavy \emph{fixed-point} rovnic).
Takové přístupy jsou pomalé a výpočetně neefektivní, jelikož často vyžadují schopnost vypočítat $\mathds{E}_{\textbf{\emph{z}} \sim q}$ v uzavřeném tvaru. 

\textbf{Hlavní myšlenkou variačních autoenkodérů je natrénování parametrického autoenkodéru} (také nazývaného jako \emph{odvozovací síť} či \emph{rozpoznávací model}) \textbf{který produkuje parametry $\textbf{q}$}.
Je-li $\textbf{\emph{z}}$ spojitá proměnná, lze vždy provést algoritmus zpětné propagace skrze vzorky $\textbf{\emph{z}}$ získané z $q(\textbf{\emph{z}}\mid\textbf{\emph{x}}) = q(\textbf{\emph{z}};f(\textbf{\emph{x}}; \bm{\emph{$\theta$}}))$ za účelem spočtení gradientu s respektem k $\bm{\emph{$\theta$}}$. \cite{Goodfellow2016}

\textbf{Učení VAE pak spočívá čistě v maximalizaci $\mathcal{L}$ s ohledem na parametry enkodéru a dekodéru}. \cite{Kingma2014}