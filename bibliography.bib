@TechReport{Tomczak2017,
  author   = {Tomczak, Jakub M. and Welling, Max},
  title    = {Improving {Variational} {Auto}-{Encoders} using {Householder} {Flow}},
  year     = {2017},
  month    = jan,
  note     = {arXiv:1611.09630 [cs, stat] type: article},
  abstract = {Variational auto-encoders (VAE) are scalable and powerful generative models. However, the choice of the variational posterior determines tractability and flexibility of the VAE. Commonly, latent variables are modeled using the normal distribution with a diagonal covariance matrix. This results in computational efficiency but typically it is not flexible enough to match the true posterior distribution. One fashion of enriching the variational posterior distribution is application of normalizing flows, i.e., a series of invertible transformations to latent variables with a simple posterior. In this paper, we follow this line of thinking and propose a volume-preserving flow that uses a series of Householder transformations. We show empirically on MNIST dataset and histopathology data that the proposed flow allows to obtain more flexible variational posterior and competitive results comparing to other normalizing flows.},
  annote   = {Comment: A corrected version of the paper submitted to Bayesian Deep Learning Workshop (NIPS 2016)},
  doi      = {10.48550/arXiv.1611.09630},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1611.09630.pdf:application/pdf},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1611.09630},
  urldate  = {2023-05-06},
}

@TechReport{Kulkarni2015,
  author   = {Kulkarni, Tejas D. and Whitney, Will and Kohli, Pushmeet and Tenenbaum, Joshua B.},
  title    = {Deep {Convolutional} {Inverse} {Graphics} {Network}},
  year     = {2015},
  month    = jun,
  note     = {arXiv:1503.03167 [cs] type: article},
  abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
  annote   = {Comment: First two authors contributed equally},
  doi      = {10.48550/arXiv.1503.03167},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1503.03167.pdf:application/pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1503.03167},
  urldate  = {2023-04-04},
}

@Book{Goodfellow2016,
  author    = {Goodfellow, Ian and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  title     = {Deep Learning},
  year      = {2016},
  note      = {\url{http://www.deeplearningbook.org}},
}

@Article{Charte2018,
  author     = {Charte, David and Charte, Francisco and García, Salvador and del Jesus, María J. and Herrera, Francisco},
  journal    = {Information Fusion},
  title      = {A practical tutorial on autoencoders for nonlinear feature fusion: {Taxonomy}, models, software and guidelines},
  year       = {2018},
  issn       = {15662535},
  month      = nov,
  note       = {arXiv:1801.01586 [cs]},
  pages      = {78--96},
  volume     = {44},
  abstract   = {Many of the existing machine learning algorithms, both supervised and unsupervised, depend on the quality of the input characteristics to generate a good model. The amount of these variables is also important, since performance tends to decline as the input dimensionality increases, hence the interest in using feature fusion techniques, able to produce feature sets that are more compact and higher level. A plethora of procedures to fuse original variables for producing new ones has been developed in the past decades. The most basic ones use linear combinations of the original variables, such as PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis), while others find manifold embeddings of lower dimensionality based on non-linear combinations, such as Isomap or LLE (Linear Locally Embedding) techniques. More recently, autoencoders (AEs) have emerged as an alternative to manifold learning for conducting nonlinear feature fusion. Dozens of AE models have been proposed lately, each with its own specific traits. Although many of them can be used to generate reduced feature sets through the fusion of the original ones, there also AEs designed with other applications in mind. The goal of this paper is to provide the reader with a broad view of what an AE is, how they are used for feature fusion, a taxonomy gathering a broad range of models, and how they relate to other classical techniques. In addition, a set of didactic guidelines on how to choose the proper AE for a given task is supplied, together with a discussion of the software tools available. Finally, two case studies illustrate the usage of AEs with datasets of handwritten digits and breast cancer.},
  doi        = {10.1016/j.inffus.2017.12.007},
  file       = {:Charte2018 - A Practical Tutorial on Autoencoders for Nonlinear Feature Fusion_ Taxonomy, Models, Software and Guidelines.pdf:PDF},
  keywords   = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  shorttitle = {A practical tutorial on autoencoders for nonlinear feature fusion},
  url        = {http://arxiv.org/abs/1801.01586},
  urldate    = {2023-03-04},
}

@Book{Phillips2021,
  author    = {Phillips, Jeff M.},
  publisher = {Springer International Publishing},
  title     = {Mathematical {Foundations} for {Data} {Analysis}},
  year      = {2021},
  isbn      = {9783030623401},
  month     = mar,
  note      = {Google-Books-ID: AUDYzQEACAAJ},
  abstract  = {This textbook, suitable for an early undergraduate up to a graduate course, provides an overview of many basic principles and techniques needed for modern data analysis. In particular, this book was designed and written as preparation for students planning to take rigorous Machine Learning and Data Mining courses. It introduces key conceptual tools necessary for data analysis, including concentration of measure and PAC bounds, cross validation, gradient descent, and principal component analysis. It also surveys basic techniques in supervised (regression and classification) and unsupervised learning (dimensionality reduction and clustering) through an accessible, simplified presentation. Students are recommended to have some background in calculus, probability, and linear algebra. Some familiarity with programming and algorithms is useful to understand advanced topics on computational techniques.},
  file      = {:Phillips2021 - Mathematical Foundations for Data Analysis.html:URL},
  keywords  = {Mathematics / Counting \& Numeration, Mathematics / Graphic Methods, Mathematics / Numerical Analysis, Mathematics / Probability \& Statistics / Stochastic Processes, Mathematics / Combinatorics},
  language  = {en},
}

@Book{2010,
  title    = {Dynamic {Programming}},
  year     = {2010},
  isbn     = {9780691146683},
  month    = jul,
  language = {en},
  url      = {https://press.princeton.edu/books/paperback/9780691146683/dynamic-programming},
  urldate  = {2023-03-05},
}

@Book{Bellman1957,
  author    = {Bellman, Richard},
  publisher = {Princeton University Press},
  title     = {Dynamic {Programming}},
  year      = {1957},
  isbn      = {9780691079516},
  note      = {Google-Books-ID: wdtoPwAACAAJ},
  abstract  = {A multi-stage allocation process; A stochastic multi-stage decision process; The structure of dynamic programming processes; Existence and uniqueness theorems; The optimal inventory equation; Bottleneck problems in multi-stage production processes; Bottleneck problems; A continuous stochastic decision process; A new formalism in the calculus of variations; Multi-stages games; Markovian decision processes.},
  file      = {Google Books Link:https\://books.google.cz/books?id=wdtoPwAACAAJ:text/html},
  language  = {en},
}

@InProceedings{Liu2006,
  author    = {Weifeng Liu and Pokharel, P.P. and Principe, J.C.},
  booktitle = {The 2006 IEEE International Joint Conference on Neural Network Proceedings},
  title     = {Correntropy: A Localized Similarity Measure},
  year      = {2006},
  pages     = {4919-4924},
  doi       = {10.1109/IJCNN.2006.247192},
}

@Book{Stanczyk2015,
  editor    = {Stańczyk, Urszula and Jain, Lakhmi C.},
  publisher = {Springer Berlin Heidelberg},
  title     = {Feature {Selection} for {Data} and {Pattern} {Recognition}},
  year      = {2015},
  address   = {Berlin, Heidelberg},
  isbn      = {9783662456194 9783662456200},
  series    = {Studies in {Computational} {Intelligence}},
  volume    = {584},
  doi       = {10.1007/978-3-662-45620-0},
  language  = {en},
  url       = {https://link.springer.com/10.1007/978-3-662-45620-0},
  urldate   = {2023-03-05},
}

@Book{Liu1998,
  editor    = {Liu, Huan and Motoda, Hiroshi},
  publisher = {Springer US},
  title     = {Feature {Extraction}, {Construction} and {Selection}},
  year      = {1998},
  address   = {Boston, MA},
  isbn      = {9781461376224 9781461557258},
  doi       = {10.1007/978-1-4615-5725-8},
  file      = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2F978-1-4615-5725-8.pdf:application/pdf},
  keywords  = {algorithms, data analysis, data mining, genetic algorithms, knowledge discovery, learning, machine learning, robot},
  url       = {http://link.springer.com/10.1007/978-1-4615-5725-8},
  urldate   = {2023-03-05},
}

@Book{Mitchell1997,
  author    = {Mitchell, Tom M.},
  publisher = {McGraw-Hill},
  title     = {Machine Learning},
  year      = {1997},
  address   = {New York},
  isbn      = {978-0-07-042807-2},
  abstract  = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. This exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning---including probability and statistics, artificial intelligence, and neural networks---unifying them all in a logical and coherent manner.},
  added-at  = {2017-05-08T14:37:30.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/23e79734ee1a6e49aee02ffd108224d1c/flint63},
  file      = {eBook:1900-99/Mitchell97.pdf:PDF;McGraw-Hill Product page:http\://www.mhprofessional.com/product.php?isbn=0070428077:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/0070428077/:URL},
  groups    = {public},
  interhash = {479a66c32badb3a455fbdcf8e6633a5d},
  intrahash = {3e79734ee1a6e49aee02ffd108224d1c},
  keywords  = {01624 105 book shelf ai learn algorithm},
  timestamp = {2017-07-13T17:10:10.000+0200},
  username  = {flint63},
}

@Book{Chollet2017,
  author    = {Chollet, François},
  publisher = {Manning},
  title     = {Deep Learning with Python},
  year      = {2017},
  isbn      = {9781617294433},
  month     = nov,
  added-at  = {2018-08-01T08:16:18.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/231f94815ebbd65d3a31e4a69e818573e/jaeschke},
  interhash = {cfbfd3f93853a469e5e6978f61a74a0a},
  intrahash = {31f94815ebbd65d3a31e4a69e818573e},
  keywords  = {ai deep deeplearning learning ml},
  timestamp = {2021-05-19T08:35:34.000+0200},
}

@Article{Erhan2010,
  author   = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
  journal  = {Journal of Machine Learning Research},
  title    = {Why {Does} {Unsupervised} {Pre}-training {Help} {Deep} {Learning}?},
  year     = {2010},
  issn     = {1533-7928},
  number   = {19},
  pages    = {625--660},
  volume   = {11},
  abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
  file     = {Full Text PDF:http\://jmlr.org/papers/volume11/erhan10a/erhan10a.pdf:application/pdf},
  url      = {http://jmlr.org/papers/v11/erhan10a.html},
  urldate  = {2023-03-06},
}

@InProceedings{Ranzato2007,
  author   = {Ranzato, M.A. and Huang, Fu and Boureau, Y-Lan and Lecun, Yann},
  title    = {Unsupervised {Learning} of {Invariant} {Feature} {Hierarchies} with {Applications} to {Object} {Recognition}},
  year     = {2007},
  month    = jul,
  pages    = {1--8},
  abstract = {We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64\% error on MNIST, and 54\% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.},
  doi      = {10.1109/CVPR.2007.383157},
  file     = {Full Text PDF:https\://www.researchgate.net/profile/Yann-Lecun/publication/224716259_Unsupervised_Learning_of_Invariant_Feature_Hierarchies_with_Applications_to_Object_Recognition/links/0912f50f9e762562f3000000/Unsupervised-Learning-of-Invariant-Feature-Hierarchies-with-Applications-to-Object-Recognition.pdf:application/pdf;ResearchGate Link:https\://www.researchgate.net/publication/224716259_Unsupervised_Learning_of_Invariant_Feature_Hierarchies_with_Applications_to_Object_Recognition:},
}

@InProceedings{Bengio2006,
  author    = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Greedy {Layer}-{Wise} {Training} of {Deep} {Networks}},
  year      = {2006},
  publisher = {MIT Press},
  volume    = {19},
  abstract  = {Recent analyses (Bengio, Delalleau, \& Le Roux, 2006; Bengio \& Le Cun, 2007) of modern nonparametric machine learning algorithms that are kernel machines, such as Support Vector Machines (SVMs), graph-based manifold and semi-supervised learning algorithms suggest fundamental limitations of some learning algorithms. The problem is clear in kernel-based approaches when the kernel is "local" (e.g., the Gaussian kernel), i.e., K (x, y ) converges to a constant when {\textbar}{\textbar}x - y {\textbar}{\textbar} increases. These analyses point to the difficulty of learning "highly-varying functions", i.e., functions that have a large number of "variations" in the domain of interest, e.g., they would require a large number of pieces to be well represented by a piecewise-linear approximation. Since the number of pieces can be made to grow exponentially with the number of factors of variations in the input, this is connected with the well-known curse of dimensionality for classical non-parametric learning algorithms (for regression, classification and density estimation). If the shapes of all these pieces are unrelated, one needs enough examples for each piece in order to generalize properly. However, if these shapes are related and can be predicted from each other, "non-local" learning algorithms have the potential to generalize to pieces not covered by the training set. Such ability would seem necessary for learning in complex domains such as Artificial Intelligence tasks (e.g., related to vision, language, speech, robotics). Kernel machines (not only those with a local kernel) have a shallow architecture, i.e., only two levels of data-dependent computational elements. This is also true of feedforward neural networks with a single hidden layer (which can become SVMs when the number of hidden units becomes large (Bengio, Le Roux, Vincent, Delalleau, \& Marcotte, 2006)). A serious problem with shallow architectures is that they can be very inefficient in terms of the number of computational units (e.g., bases, hidden units), and thus in terms of required examples (Bengio \& Le Cun, 2007). One way to represent a highly-varying function compactly (with few parameters) is through the composition of many non-linearities, i.e., with a deep architecture. For example, the parity function with d inputs requires O(2d ) examples and parameters to be represented by a Gaussian SVM (Bengio et al., 2006), O(d2 ) parameters for a one-hidden-layer neural network, O(d) parameters and units for a multi-layer network with O(log2 d) layers, and O(1) parameters with a recurrent neural network. More generally,},
  file      = {Full Text PDF:https\://proceedings.neurips.cc/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf:application/pdf},
  url       = {https://proceedings.neurips.cc/paper/2006/hash/5da713a690c067105aeb2fae32403405-Abstract.html},
  urldate   = {2023-03-06},
}

@Article{Valiant1984,
  author   = {Valiant, L. G.},
  journal  = {Communications of the ACM},
  title    = {A theory of the learnable},
  year     = {1984},
  issn     = {0001-0782},
  month    = nov,
  number   = {11},
  pages    = {1134--1142},
  volume   = {27},
  doi      = {10.1145/1968.1972},
  file     = {Full Text PDF:https\://dl.acm.org/doi/pdf/10.1145/1968.1972:application/pdf},
  keywords = {probabilistic models of learning, inductive inference, propositional expressions},
  url      = {https://doi.org/10.1145/1968.1972},
  urldate  = {2023-03-08},
}

@Article{Wolpert1996,
  author   = {Wolpert, David H.},
  journal  = {Neural Computation},
  title    = {{The Lack of A Priori Distinctions Between Learning Algorithms}},
  year     = {1996},
  issn     = {0899-7667},
  month    = {10},
  number   = {7},
  pages    = {1341-1390},
  volume   = {8},
  abstract = {{This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.}},
  doi      = {10.1162/neco.1996.8.7.1341},
  eprint   = {https://direct.mit.edu/neco/article-pdf/8/7/1341/813495/neco.1996.8.7.1341.pdf},
  url      = {https://doi.org/10.1162/neco.1996.8.7.1341},
}

@Book{Geron2019,
  author     = {Geron, Aurelien},
  publisher  = {O'Reilly Media, Inc.},
  title      = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: {Concepts}, {Tools}, and {Techniques} to {Build} {Intelligent} {Systems}},
  year       = {2019},
  edition    = {2nd},
  isbn       = {9781492032649},
  abstract   = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks-Scikit-Learn and TensorFlow-author Aurelien Geron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use Scikit-Learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the TensorFlow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets},
  shorttitle = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn}, {Keras}, and {TensorFlow}},
}

@TechReport{Rosenblatt1957,
  author                      = {Rosenblatt, F.},
  title                       = {The perceptron - A perceiving and recognizing automaton},
  year                        = {1957},
  address                     = {Ithaca, New York},
  month                       = {January},
  number                      = {85-460-1},
  school                      = {Cornell Aeronautical Laboratory},
  title_with_no_special_chars = {The Perceptron A perceiving and recognizing automaton},
}

@Book{Hebb1949,
  author               = {Hebb, Donald O.},
  publisher            = {Wiley},
  title                = {The organization of behavior: {A} neuropsychological theory},
  year                 = {1949},
  address              = {New York},
  isbn                 = {0-8058-4300-0},
  month                = jun,
  abstract             = {{Donald Hebb pioneered many current themes in
                 behavioural neuroscience. He saw psychology as a
                 biological science, but one in which the organization
                 of behaviour must remain the central concern. Through
                 penetrating theoretical concepts, including the "cell
                 assembly," "phase sequence," and "Hebb synapse," he
                 offered a way to bridge the gap between cells, circuits
                 and behaviour. He saw the brain as a dynamically
                 organized system of multiple distributed parts, with
                 roots that extend into foundations of development and
                 evolutionary heritage. He understood that behaviour, as
                 brain, can be sliced at various levels and that one of
                 our challenges is to bring these levels into both
                 conceptual and empirical register. He could move
                 between theory and fact with an ease that continues to
                 inspire both students and professional investigators.
                 Although facts continue to accumulate at an
                 accelerating rate in both psychology and neuroscience,
                 and although these facts continue to force revision in
                 the details of Hebb's earlier contributions, his
                 overall insistence that we look at behaviour and brain
                 together â within a dynamic, relational and
                 multilayered framework â remains. His work touches
                 upon current studies of population coding, contextual
                 factors in brain representations, synaptic plasticity,
                 developmental construction of brain/behaviour
                 relations, clinical syndromes, deterioration of
                 performance with age and disease, and the formal
                 construction of connectionist models. The collection of
                 papers in this volume represent these and related
                 themes that Hebb inspired. We also acknowledge our
                 appreciation for Don Hebb as teacher, colleague and
                 friend.}},
  added-at             = {2011-06-02T00:21:57.000+0200},
  biburl               = {https://www.bibsonomy.org/bibtex/26432ae617e6db0127c8b197bf760d99e/mhwombat},
  citeulike-article-id = {500649},
  citeulike-linkout-0  = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0805843000},
  citeulike-linkout-1  = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0805843000},
  citeulike-linkout-2  = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0805843000},
  citeulike-linkout-3  = {http://www.amazon.jp/exec/obidos/ASIN/0805843000},
  citeulike-linkout-4  = {http://www.amazon.co.uk/exec/obidos/ASIN/0805843000/citeulike00-21},
  citeulike-linkout-5  = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0805843000},
  citeulike-linkout-6  = {http://www.worldcat.org/isbn/0805843000},
  citeulike-linkout-7  = {http://books.google.com/books?vid=ISBN0805843000},
  citeulike-linkout-8  = {http://www.amazon.com/gp/search?keywords=0805843000\&index=books\&linkCode=qs},
  citeulike-linkout-9  = {http://www.librarything.com/isbn/0805843000},
  day                  = {15},
  file                 = {:neural_nets/Hebb 1949.pdf:PDF},
  groups               = {public},
  howpublished         = {Hardcover},
  interhash            = {ba8f8b92a0de2c83bdbcc9d742235a59},
  intrahash            = {6432ae617e6db0127c8b197bf760d99e},
  keywords             = {MSc checked network neural seminal},
  posted-at            = {2006-02-10 16:35:34},
  priority             = {2},
  timestamp            = {2016-07-12T19:25:30.000+0200},
  username             = {mhwombat},
}

@Book{Minsky1969,
  author    = {Minsky, M. and Papert, S.},
  publisher = {MIT Press},
  title     = {Perceptrons},
  year      = {1969},
  address   = {Cambridge, MA},
  added-at  = {2013-12-05T10:32:26.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/24587aec0472c41d00c38bf3e888304ba/prlz77},
  interhash = {9ad5b73f68093070d73e54312145eca2},
  intrahash = {4587aec0472c41d00c38bf3e888304ba},
  keywords  = {critic minsky papert perceptron},
  timestamp = {2013-12-05T10:32:26.000+0100},
}

@InBook{Rumelhart1987,
  author    = {Rumelhart, David E. and McClelland, James L.},
  pages     = {318-362},
  title     = {Learning Internal Representations by Error Propagation},
  year      = {1987},
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
}

@PhdThesis{Werbos1974,
  author               = {Werbos, P. J.},
  school               = {Harvard University},
  title                = {Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
  year                 = {1974},
  added-at             = {2008-02-26T11:58:58.000+0100},
  biburl               = {https://www.bibsonomy.org/bibtex/2b0644d7aa84be0df0f198d586d341843/schaul},
  citeulike-article-id = {2381655},
  description          = {idsia},
  interhash            = {4165e2708a0468e89f8305f21ee2c711},
  intrahash            = {b0644d7aa84be0df0f198d586d341843},
  keywords             = {juergen},
  priority             = {2},
  timestamp            = {2008-02-26T11:59:46.000+0100},
}

@Article{Hornik1989,
  author  = {Kurt Hornik and Maxwell B. Stinchcombe and Halbert L. White},
  journal = {Neural Networks},
  title   = {Multilayer feedforward networks are universal approximators},
  year    = {1989},
  pages   = {359-366},
  volume  = {2},
}

@Article{Cybenko1989,
  author   = {Cybenko, G.},
  journal  = {Mathematics of Control, Signals and Systems},
  title    = {Approximation by superpositions of a sigmoidal function},
  year     = {1989},
  issn     = {1435-568X},
  month    = dec,
  number   = {4},
  pages    = {303--314},
  volume   = {2},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  doi      = {10.1007/BF02551274},
  file     = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2FBF02551274.pdf:application/pdf},
  language = {en},
  url      = {https://doi.org/10.1007/BF02551274},
  urldate  = {2023-03-08},
}

@Article{Hornik1990,
  author   = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal  = {Neural Networks},
  title    = {Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks},
  year     = {1990},
  issn     = {0893-6080},
  month    = jan,
  number   = {5},
  pages    = {551--560},
  volume   = {3},
  abstract = {We give conditions ensuring that multilayer feedforward networks with as few as a single hidden layer and an appropriately smooth hidden layer activation function are capable of arbitrarily accurate approximation to an arbitrary function and its derivatives. In fact, these networks can approximate functions that are not differentiable in the classical sense, but possess only a generalized derivative, as is the case for certain piecewise differentiable functions. The conditions imposed on the hidden layer activation function are relatively mild; the conditions imposed on the domain of the function to be approximated have practical implications. Our approximation results provide a previously missing theoretical justification for the use of multilayer feedforward networks in applications requiring simultaneous approximation of a function and its derivatives.},
  doi      = {10.1016/0893-6080(90)90005-6},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/0893608090900056/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords = {Approximation, Derivatives, Sobolev space, Feedforward networks},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/0893608090900056},
  urldate  = {2023-03-08},
}

 
@Book{Murphy2022,
  author    = {Kevin P. Murphy},
  publisher = {MIT Press},
  title     = {Probabilistic Machine Learning: An introduction},
  year      = {2022},
  url       = {probml.ai},
}

@Article{Kingma2019,
  author   = {Kingma, Diederik P. and Welling, Max},
  journal  = {Foundations and Trends® in Machine Learning},
  title    = {An {Introduction} to {Variational} {Autoencoders}},
  year     = {2019},
  issn     = {1935-8237, 1935-8245},
  note     = {arXiv:1906.02691 [cs, stat]},
  number   = {4},
  pages    = {307--392},
  volume   = {12},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  doi      = {10.1561/2200000056},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1906.02691.pdf:application/pdf},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  url      = {http://arxiv.org/abs/1906.02691},
  urldate  = {2023-03-09},
}

 
@Book{Murphy2023,
  author    = {Kevin P. Murphy},
  publisher = {MIT Press},
  title     = {Probabilistic Machine Learning: Advanced Topics},
  year      = {2023},
  url       = {http://probml.github.io/book2},
}

@Article{Kullback1951,
  author    = {Kullback, S. and Leibler, R. A.},
  journal   = {The Annals of Mathematical Statistics},
  title     = {On {Information} and {Sufficiency}},
  year      = {1951},
  issn      = {0003-4851},
  number    = {1},
  pages     = {79--86},
  volume    = {22},
  file      = {JSTOR Full Text PDF:https\://www.jstor.org/stable/pdfplus/10.2307/2236703.pdf?acceptTC=true:application/pdf},
  publisher = {Institute of Mathematical Statistics},
  url       = {https://www.jstor.org/stable/2236703},
  urldate   = {2023-03-09},
}

@Article{Samuel1967,
  author  = {Arthur L. Samuel},
  journal = {IBM J. Res. Dev.},
  title   = {Some Studies in Machine Learning Using the Game of Checkers},
  year    = {1967},
  pages   = {206-227},
  volume  = {44},
}

@Article{Hochreiter1998,
  author    = {Hochreiter, Sepp},
  journal   = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  title     = {The {Vanishing} {Gradient} {Problem} {During} {Learning} {Recurrent} {Neural} {Nets} and {Problem} {Solutions}},
  year      = {1998},
  issn      = {0218-4885},
  month     = apr,
  number    = {02},
  pages     = {107--116},
  volume    = {06},
  abstract  = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
  doi       = {10.1142/S0218488598000094},
  file      = {Full Text PDF:https\://www.worldscientific.com/doi/pdf/10.1142/S0218488598000094:application/pdf},
  keywords  = {Recurrent neural nets, vanishing gradient, long-term dependencies, long short-term memory},
  publisher = {World Scientific Publishing Co.},
  url       = {https://www.worldscientific.com/doi/abs/10.1142/S0218488598000094},
  urldate   = {2023-03-09},
}

@Article{LeCun1989,
  author   = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal  = {Neural Computation},
  title    = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
  year     = {1989},
  issn     = {0899-7667},
  month    = {12},
  number   = {4},
  pages    = {541-551},
  volume   = {1},
  abstract = {{The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.}},
  doi      = {10.1162/neco.1989.1.4.541},
  eprint   = {https://direct.mit.edu/neco/article-pdf/1/4/541/811941/neco.1989.1.4.541.pdf},
  url      = {https://doi.org/10.1162/neco.1989.1.4.541},
}

@Article{Baldi1989,
  author     = {Baldi, Pierre and Hornik, Kurt},
  journal    = {Neural Networks},
  title      = {Neural networks and principal component analysis: {Learning} from examples without local minima},
  year       = {1989},
  issn       = {0893-6080},
  month      = jan,
  number     = {1},
  pages      = {53--58},
  volume     = {2},
  abstract   = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.},
  doi        = {10.1016/0893-6080(89)90014-2},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/0893608089900142/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords   = {Neural networks, Principal component analysis, Learning, Back propagation},
  language   = {en},
  shorttitle = {Neural networks and principal component analysis},
  url        = {https://www.sciencedirect.com/science/article/pii/0893608089900142},
  urldate    = {2023-03-09},
}

@InProceedings{Kamyshanska2013,
  author  = {Kamyshanska, Hanna and Memisevic, Roland},
  title   = {On autoencoder scoring},
  year    = {2013},
  month   = {01},
  journal = {30th International Conference on Machine Learning, ICML 2013},
}

@Article{Olshausen1997,
  author     = {Olshausen, Bruno A. and Field, David J.},
  journal    = {Vision Research},
  title      = {Sparse coding with an overcomplete basis set: {A} strategy employed by {V1}?},
  year       = {1997},
  issn       = {0042-6989},
  month      = dec,
  number     = {23},
  pages      = {3311--3325},
  volume     = {37},
  abstract   = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
  doi        = {10.1016/S0042-6989(97)00169-7},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0042698997001697/pdf?md5=4da7cc24881b6697abdf982e3bf98d46&pid=1-s2.0-S0042698997001697-main.pdf&isDTMRedir=Y:application/pdf},
  keywords   = {Coding, V1, Gabor-wavelet, Natural images},
  language   = {en},
  shorttitle = {Sparse coding with an overcomplete basis set},
  url        = {https://www.sciencedirect.com/science/article/pii/S0042698997001697},
  urldate    = {2023-03-09},
}

@TechReport{Rifai2012,
  author   = {Rifai, Salah and Bengio, Yoshua and Dauphin, Yann and Vincent, Pascal},
  title    = {A {Generative} {Process} for {Sampling} {Contractive} {Auto}-{Encoders}},
  year     = {2012},
  month    = jun,
  note     = {arXiv:1206.6434 [cs, stat] type: article},
  abstract = {The contractive auto-encoder learns a representation of the input data that captures the local manifold structure around each data point, through the leading singular vectors of the Jacobian of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a high-density region of the input space. This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto-encoder. The associated stochastic process defines a distribution from which one can sample, and which experimentally appears to converge quickly and mix well between modes, compared to Restricted Boltzmann Machines and Deep Belief Networks. The intuitions behind this procedure can also be used to train the second layer of contraction that pools lower-level features and learns to be invariant to the local directions of variation discovered in the first layer. We show that this can help learn and represent invariances present in the data and improve classification error.},
  annote   = {Comment: Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)},
  doi      = {10.48550/arXiv.1206.6434},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1206.6434.pdf:application/pdf},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1206.6434},
  urldate  = {2023-03-09},
}

@TechReport{Goodfellow2014,
  author   = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  title    = {Generative {Adversarial} {Networks}},
  year     = {2014},
  month    = jun,
  note     = {arXiv:1406.2661 [cs, stat] type: article},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  doi      = {10.48550/arXiv.1406.2661},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1406.2661.pdf:application/pdf},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1406.2661},
  urldate  = {2023-03-09},
}

@Article{Kingma2014,
  author    = {Kingma, D. P. and Welling, M.},
  title     = {Auto-{Encoding} {Variational} {Bayes}},
  year      = {2014},
  file      = {Full Text PDF:https\://pure.uva.nl/ws/files/2511146/162970_1312.6114v10.pd.pdf:application/pdf},
  language  = {en},
  publisher = {Ithaca, NYarXiv.org},
  url       = {https://dare.uva.nl/search?identifier=cf65ba0f-d88f-4a49-8ebd-3a7fce86edd7},
  urldate   = {2023-03-24},
}

@TechReport{Rezende2014,
  author   = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  title    = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},
  year     = {2014},
  month    = may,
  note     = {arXiv:1401.4082 [cs, stat] type: article},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  annote   = {Comment: Appears In Proceedings of the 31st International Conference on Machine Learning (ICML), JMLR: W{\textbackslash}\&CP volume 32, 2014},
  doi      = {10.48550/arXiv.1401.4082},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1401.4082.pdf:application/pdf},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Methodology},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1401.4082},
  urldate  = {2023-03-24},
}

@InCollection{Banerjee2007,
  author     = {Banerjee, Arindam},
  publisher  = {Society for Industrial and Applied Mathematics},
  title      = {An {Analysis} of {Logistic} {Models}: {Exponential} {Family} {Connections} and {Online} {Performance}},
  year       = {2007},
  isbn       = {9780898716306},
  month      = apr,
  pages      = {204--215},
  series     = {Proceedings},
  abstract   = {Logistic models are arguably one of the most widely used data analysis techniques. In this paper, we present analyses focussing on two important aspects of logistic models—its relationship with exponential family based generative models, and its performance in online and potentially adversarial settings. In particular, we present two new theoretical results on logistic models focusing on the above two aspects. First, we establish an exact connection between logistic models and exponential family based generative models, resolving a long-standing ambiguity over their relationship. Second, we show that online Bayesian logistic models are competitive to the best batch models, even in potentially adversarial settings. Further, we discuss relevant connections of our analysis to the literature on integral transforms, and also present a new optimality result for Bayesian models. The analysis makes a strong case for using logistic models and partly explains the success of such models for a wide range of practical problems.},
  doi        = {10.1137/1.9781611972771.19},
  file       = {Full Text PDF:https\://epubs.siam.org/doi/pdf/10.1137/1.9781611972771.19:application/pdf},
  shorttitle = {An {Analysis} of {Logistic} {Models}},
  url        = {https://epubs.siam.org/doi/abs/10.1137/1.9781611972771.19},
  urldate    = {2023-03-25},
}

@Article{Soenderby2016,
  author = {Sønderby, Casper and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren and Winther, Ole},
  title  = {How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks},
  year   = {2016},
  month  = {02},
}

@Article{Dayan1995,
  author   = {Dayan, P. and Hinton, G. E. and Neal, R. M. and Zemel, R. S.},
  journal  = {Neural Computation},
  title    = {The {Helmholtz} machine},
  year     = {1995},
  issn     = {0899-7667},
  month    = sep,
  number   = {5},
  pages    = {889--904},
  volume   = {7},
  abstract = {Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.},
  doi      = {10.1162/neco.1995.7.5.889},
  file     = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/7584891:text/html},
  keywords = {Algorithms, Feedback, Humans, Models, Psychological, Pattern Recognition, Automated, Visual, Perception, Stochastic Processes},
  language = {eng},
  pmid     = {7584891},
}

@Misc{LeCun2022,
  author   = {Yann LeCun},
  month    = jun,
  title    = {A {Path} {Towards} {Autonomous} {Machine} {Intelligence}},
  year     = {2022},
  abstract = {How could machines learn as efficiently as humans and animals?  How could machines learn to reason and plan?  How could machines learn representations of percepts and action plans at multiple...},
  journal  = {OpenReview},
  language = {en},
  url      = {https://openreview.net/forum?id=BZ5a1r-kVsf},
  urldate  = {2023-03-25},
}

@TechReport{Bengio2014,
  author     = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  title      = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
  year       = {2014},
  month      = apr,
  note       = {arXiv:1206.5538 [cs] type: article},
  abstract   = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  doi        = {10.48550/arXiv.1206.5538},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1206.5538.pdf:application/pdf},
  keywords   = {Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {Representation {Learning}},
  url        = {http://arxiv.org/abs/1206.5538},
  urldate    = {2023-03-25},
}

@Article{Gershman2014,
  author  = {Samuel J. Gershman and Noah D. Goodman},
  journal = {Cognitive Science},
  title   = {Amortized Inference in Probabilistic Reasoning},
  year    = {2014},
  volume  = {36},
}

@Article{Topsoee1974,
  author    = {Topsøe, Flemming},
  journal   = {Mathematica Scandinavica},
  title     = {Compactness and {Tightness} in a {Space} of {Measures} with the {Topology} of {Weak} {Convergence}},
  year      = {1974},
  issn      = {0025-5521},
  number    = {2},
  pages     = {187--210},
  volume    = {34},
  file      = {JSTOR Full Text PDF:https\://www.jstor.org/stable/pdfplus/10.2307/24490646.pdf?acceptTC=true:application/pdf},
  publisher = {Mathematica Scandinavica},
  url       = {https://www.jstor.org/stable/24490646},
  urldate   = {2023-03-26},
}

@Book{Wasserman2013,
  author    = {Wasserman, L.},
  publisher = {Springer New York},
  title     = {All of Statistics: A Concise Course in Statistical Inference},
  year      = {2013},
  isbn      = {9780387217369},
  series    = {Springer Texts in Statistics},
  lccn      = {2003062209},
  url       = {https://books.google.cz/books?id=qrcuBAAAQBAJ},
}

@TechReport{Doersch2021,
  author   = {Doersch, Carl},
  title    = {Tutorial on {Variational} {Autoencoders}},
  year     = {2021},
  month    = jan,
  note     = {arXiv:1606.05908 [cs, stat] type: article},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  doi      = {10.48550/arXiv.1606.05908},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1606.05908.pdf:application/pdf},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1606.05908},
  urldate  = {2023-03-31},
}

@InProceedings{Devroye1986,
  author    = {Devroye, Luc},
  booktitle = {Proceedings of the 18th Conference on Winter Simulation},
  title     = {Sample-Based Non-Uniform Random Variate Generation},
  year      = {1986},
  address   = {New York, NY, USA},
  pages     = {260–265},
  publisher = {Association for Computing Machinery},
  series    = {WSC '86},
  abstract  = {A sample of n lid random variables with a given unknown density is given. We discuss several issues related to the problem or generating a new sample of lid random variables with almost the same density. In particular, we look at sample independence, consistency, sample indistinguishability, moment matching and generator efficiency. We also introduce the notion of a replacement number, the minimum number of observations in a given sample that have to be replaced to obtain a sample with a given density.},
  doi       = {10.1145/318242.318443},
  isbn      = {0911801111},
  location  = {Washington, D.C., USA},
  numpages  = {6},
  url       = {https://doi.org/10.1145/318242.318443},
}

@Article{Vincent2010,
  author     = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  journal    = {J. Mach. Learn. Res.},
  title      = {Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},
  year       = {2010},
  issn       = {1532-4435},
  month      = {dec},
  pages      = {3371–3408},
  volume     = {11},
  abstract   = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
  issue_date = {3/1/2010},
  numpages   = {38},
  publisher  = {JMLR.org},
}

@TechReport{Bengio2014a,
  author   = {Bengio, Yoshua and Thibodeau-Laufer, Éric and Alain, Guillaume and Yosinski, Jason},
  title    = {Deep {Generative} {Stochastic} {Networks} {Trainable} by {Backprop}},
  year     = {2014},
  month    = may,
  note     = {arXiv:1306.1091 [cs] type: article},
  abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining.},
  annote   = {Comment: arXiv admin note: text overlap with arXiv:1305.0445, Also published in ICML'2014},
  doi      = {10.48550/arXiv.1306.1091},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1306.1091.pdf:application/pdf},
  keywords = {Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1306.1091},
  urldate  = {2023-04-05},
}

@InProceedings{Salakhutdinov2010,
  author    = {Salakhutdinov, Ruslan and Larochelle, Hugo},
  booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  title     = {Efficient Learning of Deep Boltzmann Machines},
  year      = {2010},
  address   = {Chia Laguna Resort, Sardinia, Italy},
  editor    = {Teh, Yee Whye and Titterington, Mike},
  month     = {13--15 May},
  pages     = {693--700},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {9},
  abstract  = {We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM’s), a generative model with many layers of hidden variables. The algorithm learns a separate “recognition” model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM’s practical. Finally, we demonstrate that the DBM’s trained using the proposed approximate inference algorithm perform well compared to DBN’s and SVM’s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks.},
  pdf       = {http://proceedings.mlr.press/v9/salakhutdinov10a/salakhutdinov10a.pdf},
  url       = {https://proceedings.mlr.press/v9/salakhutdinov10a.html},
}

@InProceedings{Gregor2014,
  author    = {Gregor, Karol and Danihelka, Ivo and Mnih, Andriy and Blundell, Charles and Wierstra, Daan},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  title     = {Deep AutoRegressive Networks},
  year      = {2014},
  address   = {Bejing, China},
  editor    = {Xing, Eric P. and Jebara, Tony},
  month     = {22--24 Jun},
  number    = {2},
  pages     = {1242--1250},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {32},
  abstract  = {We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data.  Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling.  We derive an efficient approximate parameter estimation method based on the minimum  description length (MDL) principle,  which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference.   We demonstrate state-of-the-art generative performance on a number of classic data sets: several UCI data sets, MNIST and Atari 2600 games.},
  pdf       = {http://proceedings.mlr.press/v32/gregor14.pdf},
  url       = {https://proceedings.mlr.press/v32/gregor14.html},
}

@Article{Hinton1995,
  author   = {Hinton, G. E. and Dayan, P. and Frey, B. J. and Neal, R. M.},
  journal  = {Science (New York, N.Y.)},
  title    = {The "wake-sleep" algorithm for unsupervised neural networks},
  year     = {1995},
  issn     = {0036-8075},
  month    = may,
  number   = {5214},
  pages    = {1158--1161},
  volume   = {268},
  abstract = {An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up "recognition" connections convert the input into representations in successive hidden layers, and top-down "generative" connections reconstruct the representation in one layer from the representation in the layer above. In the "wake" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the "sleep" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.},
  doi      = {10.1126/science.7761831},
  file     = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/7761831:text/html},
  keywords = {Algorithms, Neural Networks, Computer, Probability, Stochastic Processes},
  language = {eng},
  pmid     = {7761831},
}

@InProceedings{Sohn2015,
  author    = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learning Structured Output Representation using Deep Conditional Generative Models},
  year      = {2015},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {28},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2015/file/8d55a249e6baa5c06772297520da2051-Paper.pdf},
}

@Article{Kirkpatrick1983,
  author    = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
  journal   = {Science},
  title     = {Optimization by {Simulated} {Annealing}},
  year      = {1983},
  month     = may,
  number    = {4598},
  pages     = {671--680},
  volume    = {220},
  abstract  = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
  doi       = {10.1126/science.220.4598.671},
  file      = {Full Text PDF:https\://www.science.org/doi/pdf/10.1126/science.220.4598.671:application/pdf},
  publisher = {American Association for the Advancement of Science},
  url       = {https://www.science.org/doi/10.1126/science.220.4598.671},
  urldate   = {2023-04-05},
}

@TechReport{Bowman2016,
  author   = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
  title    = {Generating {Sentences} from a {Continuous} {Space}},
  year     = {2016},
  month    = may,
  note     = {arXiv:1511.06349 [cs] type: article},
  abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
  annote   = {Comment: First two authors contributed equally. Work was done when all authors were at Google, Inc},
  doi      = {10.48550/arXiv.1511.06349},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1511.06349.pdf:application/pdf},
  keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1511.06349},
  urldate  = {2023-04-05},
}

@InProceedings{Kingma2016,
  author    = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Improved Variational Inference with Inverse Autoregressive Flow},
  year      = {2016},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {29},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf},
}

@TechReport{Gregor2015,
  author     = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  title      = {{DRAW}: {A} {Recurrent} {Neural} {Network} {For} {Image} {Generation}},
  year       = {2015},
  month      = may,
  note       = {arXiv:1502.04623 [cs] type: article},
  abstract   = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
  doi        = {10.48550/arXiv.1502.04623},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1502.04623.pdf:application/pdf},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  school     = {arXiv},
  shorttitle = {{DRAW}},
  url        = {http://arxiv.org/abs/1502.04623},
  urldate    = {2023-04-06},
}

@InProceedings{Hinton1993,
  author    = {Hinton, Geoffrey E. and van Camp, Drew},
  booktitle = {Proceedings of the Sixth Annual Conference on Computational Learning Theory},
  title     = {Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights},
  year      = {1993},
  address   = {New York, NY, USA},
  pages     = {5–13},
  publisher = {Association for Computing Machinery},
  series    = {COLT '93},
  doi       = {10.1145/168304.168306},
  isbn      = {0897916115},
  location  = {Santa Cruz, California, USA},
  numpages  = {9},
  url       = {https://doi.org/10.1145/168304.168306},
}

@TechReport{White2016,
  author   = {White, Tom},
  title    = {Sampling {Generative} {Networks}},
  year     = {2016},
  month    = dec,
  note     = {arXiv:1609.04468 [cs, stat] type: article},
  abstract = {We introduce several techniques for sampling and visualizing the latent spaces of generative models. Replacing linear interpolation with spherical linear interpolation prevents diverging from a model's prior distribution and produces sharper samples. J-Diagrams and MINE grids are introduced as visualizations of manifolds created by analogies and nearest neighbors. We demonstrate two new techniques for deriving attribute vectors: bias-corrected vectors with data replication and synthetic vectors with data augmentation. Binary classification using attribute vectors is presented as a technique supporting quantitative analysis of the latent space. Most techniques are intended to be independent of model type and examples are shown on both Variational Autoencoders and Generative Adversarial Networks.},
  annote   = {Comment: 11 pages, 11 figures},
  doi      = {10.48550/arXiv.1609.04468},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1609.04468.pdf:application/pdf},
  keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1609.04468},
  urldate  = {2023-04-09},
}

@Article{GomezBombarelli2018,
  author  = {Gómez-Bombarelli, Rafael and Wei, Jennifer N. and Duvenaud, David and Hernández-Lobato, José Miguel and Sánchez-Lengeling, Benjamín and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and Aspuru-Guzik, Alán},
  journal = {ACS Central Science},
  title   = {Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules},
  year    = {2018},
  note    = {PMID: 29532027},
  number  = {2},
  pages   = {268-276},
  volume  = {4},
  doi     = {10.1021/acscentsci.7b00572},
  eprint  = {https://doi.org/10.1021/acscentsci.7b00572},
  url     = {https://doi.org/10.1021/acscentsci.7b00572},
}

@TechReport{Ravanbakhsh2016,
  author   = {Ravanbakhsh, Siamak and Lanusse, Francois and Mandelbaum, Rachel and Schneider, Jeff and Poczos, Barnabas},
  title    = {Enabling {Dark} {Energy} {Science} with {Deep} {Generative} {Models} of {Galaxy} {Images}},
  year     = {2016},
  month    = nov,
  note     = {arXiv:1609.05796 [astro-ph, stat] type: article},
  abstract = {Understanding the nature of dark energy, the mysterious force driving the accelerated expansion of the Universe, is a major challenge of modern cosmology. The next generation of cosmological surveys, specifically designed to address this issue, rely on accurate measurements of the apparent shapes of distant galaxies. However, shape measurement methods suffer from various unavoidable biases and therefore will rely on a precise calibration to meet the accuracy requirements of the science analysis. This calibration process remains an open challenge as it requires large sets of high quality galaxy images. To this end, we study the application of deep conditional generative models in generating realistic galaxy images. In particular we consider variations on conditional variational autoencoder and introduce a new adversarial objective for training of conditional generative networks. Our results suggest a reliable alternative to the acquisition of expensive high quality observations for generating the calibration data needed by the next generation of cosmological surveys.},
  doi      = {10.48550/arXiv.1609.05796},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1609.05796.pdf:application/pdf},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Cosmology and Nongalactic Astrophysics, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1609.05796},
  urldate  = {2023-04-09},
}

@TechReport{Deshpande2017,
  author   = {Deshpande, Aditya and Lu, Jiajun and Yeh, Mao-Chuang and Chong, Min Jin and Forsyth, David},
  title    = {Learning {Diverse} {Image} {Colorization}},
  year     = {2017},
  month    = apr,
  note     = {arXiv:1612.01958 [cs] type: article},
  abstract = {Colorization is an ambiguous problem, with multiple viable colorizations for a single grey-level image. However, previous methods only produce the single most probable colorization. Our goal is to model the diversity intrinsic to the problem of colorization and produce multiple colorizations that display long-scale spatial co-ordination. We learn a low dimensional embedding of color fields using a variational autoencoder (VAE). We construct loss terms for the VAE decoder that avoid blurry outputs and take into account the uneven distribution of pixel colors. Finally, we build a conditional model for the multi-modal distribution between grey-level image and the color field embeddings. Samples from this conditional model result in diverse colorization. We demonstrate that our method obtains better diverse colorizations than a standard conditional variational autoencoder (CVAE) model, as well as a recently proposed conditional generative adversarial network (cGAN).},
  annote   = {Comment: This revision to appear in CVPR17},
  doi      = {10.48550/arXiv.1612.01958},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1612.01958.pdf:application/pdf},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1612.01958},
  urldate  = {2023-04-09},
}

@TechReport{Gulrajani2016,
  author     = {Gulrajani, Ishaan and Kumar, Kundan and Ahmed, Faruk and Taiga, Adrien Ali and Visin, Francesco and Vazquez, David and Courville, Aaron},
  title      = {{PixelVAE}: {A} {Latent} {Variable} {Model} for {Natural} {Images}},
  year       = {2016},
  month      = nov,
  note       = {arXiv:1611.05013 [cs] type: article},
  abstract   = {Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64x64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.},
  doi        = {10.48550/arXiv.1611.05013},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1611.05013.pdf:application/pdf},
  keywords   = {Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {{PixelVAE}},
  url        = {http://arxiv.org/abs/1611.05013},
  urldate    = {2023-04-09},
}

@TechReport{Gregor2016,
  author   = {Gregor, Karol and Besse, Frederic and Rezende, Danilo Jimenez and Danihelka, Ivo and Wierstra, Daan},
  title    = {Towards {Conceptual} {Compression}},
  year     = {2016},
  month    = apr,
  note     = {arXiv:1604.08772 [cs, stat] type: article},
  abstract = {We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.},
  annote   = {Comment: 14 pages, 13 figures},
  doi      = {10.48550/arXiv.1604.08772},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1604.08772.pdf:application/pdf},
  keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1604.08772},
  urldate  = {2023-04-10},
}

@TechReport{Yan2016,
  author     = {Yan, Xinchen and Yang, Jimei and Sohn, Kihyuk and Lee, Honglak},
  title      = {{Attribute2Image}: {Conditional} {Image} {Generation} from {Visual} {Attributes}},
  year       = {2016},
  month      = oct,
  note       = {arXiv:1512.00570 [cs] type: article},
  abstract   = {This paper investigates a novel problem of generating images from visual attributes. We model the image as a composite of foreground and background and develop a layered generative model with disentangled latent variables that can be learned end-to-end using a variational auto-encoder. We experiment with natural images of faces and birds and demonstrate that the proposed models are capable of generating realistic and diverse samples with disentangled latent representations. We use a general energy minimization algorithm for posterior inference of latent variables given novel images. Therefore, the learned generative models show excellent quantitative and visual results in the tasks of attribute-conditioned image reconstruction and completion.},
  annote     = {Comment: 19 pages, accepted by ECCV 2016, The 14th European Conference on Computer Vision (2016)},
  doi        = {10.48550/arXiv.1512.00570},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1512.00570.pdf:application/pdf},
  keywords   = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
  school     = {arXiv},
  shorttitle = {{Attribute2Image}},
  url        = {http://arxiv.org/abs/1512.00570},
  urldate    = {2023-04-10},
}

@InProceedings{Huang2012,
  author    = {Gary B. Huang and Marwan Mattar and Honglak Lee and Erik Learned-Miller},
  booktitle = {NIPS},
  title     = {Learning to Align from Scratch},
  year      = {2012},
}

@Misc{Wah2011,
  author    = {Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
  month     = jul,
  title     = {The {Caltech}-{UCSD} {Birds}-200-2011 {Dataset}},
  year      = {2011},
  abstract  = {CUB-200-2011 is an extended version of CUB-200 [7], a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and at- tribute labels. Images and annotations were filtered by mul- tiple users of Mechanical Turk. We introduce benchmarks and baseline experiments for multi-class categorization and part localization.},
  address   = {Pasadena, CA},
  copyright = {other},
  file      = {Full Text PDF:https\://authors.library.caltech.edu/27452/1/CUB_200_2011.pdf:application/pdf},
  language  = {en},
  number    = {2010-001},
  publisher = {California Institute of Technology},
  url       = {https://resolver.caltech.edu/CaltechAUTHORS:20111026-120541847},
  urldate   = {2023-04-10},
}

@Book{Foster2023,
  author     = {Foster, David},
  publisher  = {O'Reilly Media},
  title      = {Generative {Deep} {Learning}: {Teaching} {Machines} {To} {Paint}, {Write}, {Compose}, and {Play}},
  year       = {2023},
  edition    = {2nd edition},
  isbn       = {9781098134181},
  month      = jun,
  abstract   = {Generative AI is the hottest topic in tech. This practical book teaches machine learning engineers and data scientists how to create impressive generative deep learning models from scratch using Tensorflow and Keras, including variational autoencoders (VAEs), generative adversarial networks (GANs), Transformers, normalizing flows, energy-based models, and denoising diffusion models. The book starts with the basics of deep learning and progresses to cutting-edge architectures. Through tips and tricks, readers can make their models learn more efficiently and become more creative. Discover how VAEs can change facial expressions in photosTrain GANs to generate images based on your own datasetBuild diffusion models to produce new varieties of flowersTrain your own GPT for text generationLearn how large language models like ChatGPT are trainedExplore state-of-the-art architectures such as StyleGAN 2 and Vision Transformer VQ-GANCompose polyphonic music using Transformers and MuseGANUnderstand how generative world models can solve reinforcement learning tasksDive into multimodal models such as DALL.E 2, Imagen and Stable Diffusion for text-to-image generationThe book also explores the future of generative AI and how individuals and companies can proactively begin to leverage this remarkable new technology to create competitive advantage.},
  file       = {Amazon.com Link:https\://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1098134184:text/html},
  language   = {English},
  shorttitle = {Generative {Deep} {Learning}},
}

@Article{LeCun2010,
  author       = {LeCun, Yann and Cortes, Corinna},
  title        = {{MNIST} handwritten digit database},
  year         = {2010},
  added-at     = {2010-06-28T21:16:30.000+0200},
  biburl       = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups       = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash    = {21b9d0558bd66279df9452562df6e6f3},
  intrahash    = {935bad99fa1f65e03c25b315aa3c1032},
  keywords     = {MSc _checked character_recognition mnist network neural},
  lastchecked  = {2016-01-14 14:24:11},
  timestamp    = {2016-07-12T19:25:30.000+0200},
  url          = {http://yann.lecun.com/exdb/mnist/},
  username     = {mhwombat},
}

@InProceedings{Hinton2002,
  author    = {Hinton, G. E. and Roweis, S.},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Stochastic Neighbor Embedding},
  year      = {2002},
  editor    = {S. Becker and S. Thrun and K. Obermayer},
  publisher = {MIT Press},
  volume    = {15},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf},
}

@InProceedings{Higgins2022,
  author     = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  title      = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
  year       = {2022},
  month      = jul,
  abstract   = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta {\textgreater} 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
  file       = {Full Text PDF:https\://openreview.net/pdf?id=Sy2fzU9gl:application/pdf},
  language   = {en},
  shorttitle = {beta-{VAE}},
  url        = {https://openreview.net/forum?id=Sy2fzU9gl},
  urldate    = {2023-05-06},
}

@TechReport{Sankarapandian2021,
  author   = {Sankarapandian, Sivaramakrishnan and Kulis, Brian},
  title    = {\${\textbackslash}beta\$-{Annealed} {Variational} {Autoencoder} for glitches},
  year     = {2021},
  month    = jul,
  note     = {arXiv:2107.10667 [gr-qc] type: article},
  abstract = {Gravitational wave detectors such as LIGO and Virgo are susceptible to various types of instrumental and environmental disturbances known as glitches which can mask and mimic gravitational waves. While there are 22 classes of non-Gaussian noise gradients currently identified, the number of classes is likely to increase as these detectors go through commissioning between observation runs. Since identification and labelling new noise gradients can be arduous and time-consuming, we propose \${\textbackslash}beta\$-Annelead VAEs to learn representations from spectograms in an unsupervised way. Using the same formulation as {\textbackslash}cite\{alemi2017fixing\}, we view Bottleneck-VAEs{\textasciitilde}cite\{burgess2018understanding\} through the lens of information theory and connect them to \${\textbackslash}beta\$-VAEs{\textasciitilde}cite\{higgins2017beta\}. Motivated by this connection, we propose an annealing schedule for the hyperparameter \${\textbackslash}beta\$ in \${\textbackslash}beta\$-VAEs which has advantages of: 1) One fewer hyperparameter to tune, 2) Better reconstruction quality, while producing similar levels of disentanglement.},
  doi      = {10.48550/arXiv.2107.10667},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2107.10667.pdf:application/pdf},
  keywords = {Computer Science - Machine Learning, General Relativity and Quantum Cosmology},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2107.10667},
  urldate  = {2023-05-06},
}

@TechReport{Kingma2017,
  author     = {Kingma and Ba, Jimmy},
  title      = {Adam: {A} {Method} for {Stochastic} {Optimization}},
  year       = {2017},
  month      = jan,
  note       = {arXiv:1412.6980 [cs] type: article},
  abstract   = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  annote     = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
  doi        = {10.48550/arXiv.1412.6980},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1412.6980.pdf:application/pdf},
  keywords   = {Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {Adam},
  url        = {http://arxiv.org/abs/1412.6980},
  urldate    = {2023-05-07},
}

@Article{Mishkin2017,
  author   = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
  journal  = {Computer Vision and Image Understanding},
  title    = {Systematic evaluation of {CNN} advances on the {ImageNet}},
  year     = {2017},
  issn     = {10773142},
  month    = aug,
  note     = {arXiv:1606.02228 [cs]},
  pages    = {11--19},
  volume   = {161},
  abstract = {The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the "deficit" is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.},
  annote   = {Comment: Submitted to CVIU Special Issue on Deep Learning. Updated dataset quality experiment},
  doi      = {10.1016/j.cviu.2017.05.007},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1606.02228.pdf:application/pdf},
  keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
  url      = {http://arxiv.org/abs/1606.02228},
  urldate  = {2023-05-07},
}

@InProceedings{Bartler2019,
  author    = {Bartler, Alexander and Wiewel, Felix and Mauch, Lukas and Yang, Bin},
  booktitle = {2019 27th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
  title     = {Training {Variational} {Autoencoders} with {Discrete} {Latent} {Variables} {Using} {Importance} {Sampling}},
  year      = {2019},
  month     = sep,
  note      = {ISSN: 2076-1465},
  pages     = {1--5},
  abstract  = {The Variational Autoencoder (VAE) is a popular generative latent variable model that is often used for representation learning. Standard VAEs assume continuous-valued latent variables and are trained by maximization of the evidence lower bound (ELBO). Conventional methods obtain a differentiable estimate of the ELBO with reparametrized sampling and optimize it with Stochastic Gradient Descend (SGD). However, this is not possible if we want to train VAEs with discrete-valued latent variables, since reparametrized sampling is not possible. In this paper, we propose an easy method to train VAEs with binary or categorically valued latent representations. Therefore, we use a differentiable estimator for the ELBO which is based on importance sampling. In experiments, we verify the approach and train two different VAEs architectures with Bernoulli and categorically distributed latent representations on two different benchmark datasets.},
  doi       = {10.23919/EUSIPCO.2019.8902811},
  issn      = {2076-1465},
  keywords  = {Decoding, Training, Monte Carlo methods, Signal processing, Europe, Standards, Stochastic processes, variational autoencoder, discrete latent variables, importance sampling},
}

@TechReport{Asperti2020,
  author   = {Asperti, Andrea},
  title    = {Variance {Loss} in {Variational} {Autoencoders}},
  year     = {2020},
  month    = may,
  note     = {arXiv:2002.09860 [cs] type: article},
  abstract = {In this article, we highlight what appears to be major issue of Variational Autoencoders, evinced from an extensive experimentation with different network architectures and datasets: the variance of generated data is significantly lower than that of training data. Since generative models are usually evaluated with metrics such as the Frechet Inception Distance (FID) that compare the distributions of (features of) real versus generated images, the variance loss typically results in degraded scores. This problem is particularly relevant in a two stage setting, where we use a second VAE to sample in the latent space of the first VAE. The minor variance creates a mismatch between the actual distribution of latent variables and those generated by the second VAE, that hinders the beneficial effects of the second stage. Renormalizing the output of the second VAE towards the expected normal spherical distribution, we obtain a sudden burst in the quality of generated samples, as also testified in terms of FID.},
  annote   = {Comment: Article accepted at the Sixth International Conference on Machine Learning, Optimization, and Data Science. July 19-23, 2020 - Certosa di Pontignano, Siena, Italy},
  doi      = {10.48550/arXiv.2002.09860},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2002.09860.pdf:application/pdf},
  keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2002.09860},
  urldate  = {2023-05-07},
}

@Book{Vapnik2000,
  author    = {Vladimir Vapnik},
  publisher = {Springer: New York},
  title     = {The Nature of Statistical Learning Theory},
  year      = {2000},
  isbn      = {1441931600},
}

@Book{Hume1978,
  author    = {Hume, David},
  editor    = {Selby-Bigge, L.A.},
  publisher = {Oxford University Press},
  title     = {A Treatise of Human Nature},
  year      = {1978},
  address   = {Oxford},
  note      = {revised P.H. Nidditch},
  added-at  = {2013-03-17T19:39:45.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/27fd4942eed6f4d04de897abab18161ba/david_rowthorn},
  interhash = {500095e93ae5a49a1c6c9f4026f13f9b},
  intrahash = {7fd4942eed6f4d04de897abab18161ba},
  keywords  = {hume identity memory personal},
  origdate  = {1739},
  timestamp = {2013-10-14T10:16:38.000+0200},
}

@Article{Hibbard2009,
  author  = {Hibbard, Bill},
  journal = {Journal of Artificial General Intelligence},
  title   = {Bias and No Free Lunch in Formal Measures of Intelligence},
  year    = {2009},
  month   = {12},
  pages   = {2009-9},
  volume  = {1},
  doi     = {10.2478/v10229-011-0004-6},
}

@Book{Taleb2008,
  author    = {Taleb, Nassim Nicholas},
  publisher = {Random House},
  title     = {The Black Swan: The Impact of the Highly Improbable},
  year      = {2008},
  address   = {London},
  edition   = {1},
  isbn      = {1400063515},
  added-at  = {2010-02-22T16:27:58.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/22d6bb55b10da553cc7822ae5945a2693/vatchoum},
  interhash = {f2b3a16f6d0bf3430d44a7508923ac2b},
  intrahash = {2d6bb55b10da553cc7822ae5945a2693},
  keywords  = {AleatoireBD AleatoireC {ALIRE,} {HasardBD}},
  timestamp = {2010-02-22T16:44:09.000+0100},
}

@Article{McCulloch1943,
  author   = {McCulloch, Warren S. and Pitts, Walter},
  journal  = {The bulletin of mathematical biophysics},
  title    = {A logical calculus of the ideas immanent in nervous activity},
  year     = {1943},
  issn     = {1522-9602},
  month    = dec,
  number   = {4},
  pages    = {115--133},
  volume   = {5},
  abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  doi      = {10.1007/BF02478259},
  file     = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2Fbf02478259.pdf:application/pdf},
  keywords = {Nervous Activity, Excitatory Synapse, Inhibitory Synapse, Temporal Summation, Spatial Summation},
  language = {en},
  url      = {https://doi.org/10.1007/BF02478259},
  urldate  = {2023-05-07},
}

@InProceedings{Rumelhart1986,
  author = {David E. Rumelhart and James L. McClelland},
  title  = {Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations},
  year   = {1986},
}

@Book{Bryson1969,
  author      = {Bryson, A. E. and Ho, Y. C.},
  publisher   = {Blaisdell},
  title       = {Applied Optimal Control},
  year        = {1969},
  address     = {New York},
  added-at    = {2014-11-23T18:37:23.000+0100},
  biburl      = {https://www.bibsonomy.org/bibtex/27d13cce517ce39632aeabf713bc6fbeb/prlz77},
  booktitle   = {Applied Optimal Control},
  description = {CCNLab BibTeX},
  interhash   = {3f3142a9345a8acc8c96f47a01da5372},
  intrahash   = {7d13cce517ce39632aeabf713bc6fbeb},
  keywords    = {backpropagation},
  timestamp   = {2014-11-23T18:37:23.000+0100},
}

@Comment{jabref-meta: databaseType:bibtex;}
