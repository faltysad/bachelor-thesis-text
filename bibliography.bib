@TechReport{Kingma2013,
  author   = {Kingma, Diederik P. and Welling, Max},
  title    = {Auto-{Encoding} {Variational} {Bayes}},
  year     = {2013},
  month    = dec,
  note     = {arXiv:1312.6114 [cs, stat] version: 1 type: article},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  annote   = {Comment: Fixes a typo in the abstract, no other changes},
  doi      = {10.48550/arXiv.1312.6114},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1312.6114v1.pdf:application/pdf},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1312.6114},
  urldate  = {2023-03-04},
}

@Book{Goodfellow2016,
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  title     = {Deep Learning},
  year      = {2016},
  note      = {\url{http://www.deeplearningbook.org}},
}

@Article{Charte2018,
  author     = {Charte, David and Charte, Francisco and García, Salvador and del Jesus, María J. and Herrera, Francisco},
  journal    = {Information Fusion},
  title      = {A practical tutorial on autoencoders for nonlinear feature fusion: {Taxonomy}, models, software and guidelines},
  year       = {2018},
  issn       = {15662535},
  month      = nov,
  note       = {arXiv:1801.01586 [cs]},
  pages      = {78--96},
  volume     = {44},
  abstract   = {Many of the existing machine learning algorithms, both supervised and unsupervised, depend on the quality of the input characteristics to generate a good model. The amount of these variables is also important, since performance tends to decline as the input dimensionality increases, hence the interest in using feature fusion techniques, able to produce feature sets that are more compact and higher level. A plethora of procedures to fuse original variables for producing new ones has been developed in the past decades. The most basic ones use linear combinations of the original variables, such as PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis), while others find manifold embeddings of lower dimensionality based on non-linear combinations, such as Isomap or LLE (Linear Locally Embedding) techniques. More recently, autoencoders (AEs) have emerged as an alternative to manifold learning for conducting nonlinear feature fusion. Dozens of AE models have been proposed lately, each with its own specific traits. Although many of them can be used to generate reduced feature sets through the fusion of the original ones, there also AEs designed with other applications in mind. The goal of this paper is to provide the reader with a broad view of what an AE is, how they are used for feature fusion, a taxonomy gathering a broad range of models, and how they relate to other classical techniques. In addition, a set of didactic guidelines on how to choose the proper AE for a given task is supplied, together with a discussion of the software tools available. Finally, two case studies illustrate the usage of AEs with datasets of handwritten digits and breast cancer.},
  doi        = {10.1016/j.inffus.2017.12.007},
  file       = {:Charte2018 - A Practical Tutorial on Autoencoders for Nonlinear Feature Fusion_ Taxonomy, Models, Software and Guidelines.pdf:PDF},
  keywords   = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  shorttitle = {A practical tutorial on autoencoders for nonlinear feature fusion},
  url        = {http://arxiv.org/abs/1801.01586},
  urldate    = {2023-03-04},
}

@Book{Phillips2021,
  author    = {Phillips, Jeff M.},
  publisher = {Springer International Publishing},
  title     = {Mathematical {Foundations} for {Data} {Analysis}},
  year      = {2021},
  isbn      = {9783030623401},
  month     = mar,
  note      = {Google-Books-ID: AUDYzQEACAAJ},
  abstract  = {This textbook, suitable for an early undergraduate up to a graduate course, provides an overview of many basic principles and techniques needed for modern data analysis. In particular, this book was designed and written as preparation for students planning to take rigorous Machine Learning and Data Mining courses. It introduces key conceptual tools necessary for data analysis, including concentration of measure and PAC bounds, cross validation, gradient descent, and principal component analysis. It also surveys basic techniques in supervised (regression and classification) and unsupervised learning (dimensionality reduction and clustering) through an accessible, simplified presentation. Students are recommended to have some background in calculus, probability, and linear algebra. Some familiarity with programming and algorithms is useful to understand advanced topics on computational techniques.},
  file      = {:Phillips2021 - Mathematical Foundations for Data Analysis.html:URL},
  keywords  = {Mathematics / Counting \& Numeration, Mathematics / Graphic Methods, Mathematics / Numerical Analysis, Mathematics / Probability \& Statistics / Stochastic Processes, Mathematics / Combinatorics},
  language  = {en},
}

@Book{2010,
  title    = {Dynamic {Programming}},
  year     = {2010},
  isbn     = {9780691146683},
  month    = jul,
  language = {en},
  url      = {https://press.princeton.edu/books/paperback/9780691146683/dynamic-programming},
  urldate  = {2023-03-05},
}

@Book{Bellman1957,
  author    = {Bellman, Richard},
  publisher = {Princeton University Press},
  title     = {Dynamic {Programming}},
  year      = {1957},
  isbn      = {9780691079516},
  note      = {Google-Books-ID: wdtoPwAACAAJ},
  abstract  = {A multi-stage allocation process; A stochastic multi-stage decision process; The structure of dynamic programming processes; Existence and uniqueness theorems; The optimal inventory equation; Bottleneck problems in multi-stage production processes; Bottleneck problems; A continuous stochastic decision process; A new formalism in the calculus of variations; Multi-stages games; Markovian decision processes.},
  file      = {Google Books Link:https\://books.google.cz/books?id=wdtoPwAACAAJ:text/html},
  language  = {en},
}

@InProceedings{Liu2006,
  author    = {Weifeng Liu and Pokharel, P.P. and Principe, J.C.},
  booktitle = {The 2006 IEEE International Joint Conference on Neural Network Proceedings},
  title     = {Correntropy: A Localized Similarity Measure},
  year      = {2006},
  pages     = {4919-4924},
  doi       = {10.1109/IJCNN.2006.247192},
}

@Book{Stanczyk2015,
  editor    = {Stańczyk, Urszula and Jain, Lakhmi C.},
  publisher = {Springer Berlin Heidelberg},
  title     = {Feature {Selection} for {Data} and {Pattern} {Recognition}},
  year      = {2015},
  address   = {Berlin, Heidelberg},
  isbn      = {9783662456194 9783662456200},
  series    = {Studies in {Computational} {Intelligence}},
  volume    = {584},
  doi       = {10.1007/978-3-662-45620-0},
  language  = {en},
  url       = {https://link.springer.com/10.1007/978-3-662-45620-0},
  urldate   = {2023-03-05},
}

@Book{Liu1998,
  editor    = {Liu, Huan and Motoda, Hiroshi},
  publisher = {Springer US},
  title     = {Feature {Extraction}, {Construction} and {Selection}},
  year      = {1998},
  address   = {Boston, MA},
  isbn      = {9781461376224 9781461557258},
  doi       = {10.1007/978-1-4615-5725-8},
  file      = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2F978-1-4615-5725-8.pdf:application/pdf},
  keywords  = {algorithms, data analysis, data mining, genetic algorithms, knowledge discovery, learning, machine learning, robot},
  url       = {http://link.springer.com/10.1007/978-1-4615-5725-8},
  urldate   = {2023-03-05},
}

@Book{Mitchell1997,
  author    = {Mitchell, Tom M.},
  publisher = {McGraw-Hill},
  title     = {Machine Learning},
  year      = {1997},
  address   = {New York},
  isbn      = {978-0-07-042807-2},
  abstract  = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. This exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning---including probability and statistics, artificial intelligence, and neural networks---unifying them all in a logical and coherent manner.},
  added-at  = {2017-05-08T14:37:30.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/23e79734ee1a6e49aee02ffd108224d1c/flint63},
  file      = {eBook:1900-99/Mitchell97.pdf:PDF;McGraw-Hill Product page:http\://www.mhprofessional.com/product.php?isbn=0070428077:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/0070428077/:URL},
  groups    = {public},
  interhash = {479a66c32badb3a455fbdcf8e6633a5d},
  intrahash = {3e79734ee1a6e49aee02ffd108224d1c},
  keywords  = {01624 105 book shelf ai learn algorithm},
  timestamp = {2017-07-13T17:10:10.000+0200},
  username  = {flint63},
}

@Book{Chollet2017,
  author    = {Chollet, François},
  publisher = {Manning},
  title     = {Deep Learning with Python},
  year      = {2017},
  isbn      = {9781617294433},
  month     = nov,
  added-at  = {2018-08-01T08:16:18.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/231f94815ebbd65d3a31e4a69e818573e/jaeschke},
  interhash = {cfbfd3f93853a469e5e6978f61a74a0a},
  intrahash = {31f94815ebbd65d3a31e4a69e818573e},
  keywords  = {ai deep deeplearning learning ml},
  timestamp = {2021-05-19T08:35:34.000+0200},
}

@Article{Erhan2010,
  author   = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
  journal  = {Journal of Machine Learning Research},
  title    = {Why {Does} {Unsupervised} {Pre}-training {Help} {Deep} {Learning}?},
  year     = {2010},
  issn     = {1533-7928},
  number   = {19},
  pages    = {625--660},
  volume   = {11},
  abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
  file     = {Full Text PDF:http\://jmlr.org/papers/volume11/erhan10a/erhan10a.pdf:application/pdf},
  url      = {http://jmlr.org/papers/v11/erhan10a.html},
  urldate  = {2023-03-06},
}

@InProceedings{Ranzato2007,
  author   = {Ranzato, M.A. and Huang, Fu and Boureau, Y-Lan and Lecun, Yann},
  title    = {Unsupervised {Learning} of {Invariant} {Feature} {Hierarchies} with {Applications} to {Object} {Recognition}},
  year     = {2007},
  month    = jul,
  pages    = {1--8},
  abstract = {We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64\% error on MNIST, and 54\% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.},
  doi      = {10.1109/CVPR.2007.383157},
  file     = {Full Text PDF:https\://www.researchgate.net/profile/Yann-Lecun/publication/224716259_Unsupervised_Learning_of_Invariant_Feature_Hierarchies_with_Applications_to_Object_Recognition/links/0912f50f9e762562f3000000/Unsupervised-Learning-of-Invariant-Feature-Hierarchies-with-Applications-to-Object-Recognition.pdf:application/pdf;ResearchGate Link:https\://www.researchgate.net/publication/224716259_Unsupervised_Learning_of_Invariant_Feature_Hierarchies_with_Applications_to_Object_Recognition:},
}

@InProceedings{Bengio2006,
  author    = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Greedy {Layer}-{Wise} {Training} of {Deep} {Networks}},
  year      = {2006},
  publisher = {MIT Press},
  volume    = {19},
  abstract  = {Recent analyses (Bengio, Delalleau, \& Le Roux, 2006; Bengio \& Le Cun, 2007) of modern nonparametric machine learning algorithms that are kernel machines, such as Support Vector Machines (SVMs), graph-based manifold and semi-supervised learning algorithms suggest fundamental limitations of some learning algorithms. The problem is clear in kernel-based approaches when the kernel is "local" (e.g., the Gaussian kernel), i.e., K (x, y ) converges to a constant when {\textbar}{\textbar}x - y {\textbar}{\textbar} increases. These analyses point to the difficulty of learning "highly-varying functions", i.e., functions that have a large number of "variations" in the domain of interest, e.g., they would require a large number of pieces to be well represented by a piecewise-linear approximation. Since the number of pieces can be made to grow exponentially with the number of factors of variations in the input, this is connected with the well-known curse of dimensionality for classical non-parametric learning algorithms (for regression, classification and density estimation). If the shapes of all these pieces are unrelated, one needs enough examples for each piece in order to generalize properly. However, if these shapes are related and can be predicted from each other, "non-local" learning algorithms have the potential to generalize to pieces not covered by the training set. Such ability would seem necessary for learning in complex domains such as Artificial Intelligence tasks (e.g., related to vision, language, speech, robotics). Kernel machines (not only those with a local kernel) have a shallow architecture, i.e., only two levels of data-dependent computational elements. This is also true of feedforward neural networks with a single hidden layer (which can become SVMs when the number of hidden units becomes large (Bengio, Le Roux, Vincent, Delalleau, \& Marcotte, 2006)). A serious problem with shallow architectures is that they can be very inefficient in terms of the number of computational units (e.g., bases, hidden units), and thus in terms of required examples (Bengio \& Le Cun, 2007). One way to represent a highly-varying function compactly (with few parameters) is through the composition of many non-linearities, i.e., with a deep architecture. For example, the parity function with d inputs requires O(2d ) examples and parameters to be represented by a Gaussian SVM (Bengio et al., 2006), O(d2 ) parameters for a one-hidden-layer neural network, O(d) parameters and units for a multi-layer network with O(log2 d) layers, and O(1) parameters with a recurrent neural network. More generally,},
  file      = {Full Text PDF:https\://proceedings.neurips.cc/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf:application/pdf},
  url       = {https://proceedings.neurips.cc/paper/2006/hash/5da713a690c067105aeb2fae32403405-Abstract.html},
  urldate   = {2023-03-06},
}

@Article{Valiant1984,
  author   = {Valiant, L. G.},
  journal  = {Communications of the ACM},
  title    = {A theory of the learnable},
  year     = {1984},
  issn     = {0001-0782},
  month    = nov,
  number   = {11},
  pages    = {1134--1142},
  volume   = {27},
  doi      = {10.1145/1968.1972},
  file     = {Full Text PDF:https\://dl.acm.org/doi/pdf/10.1145/1968.1972:application/pdf},
  keywords = {probabilistic models of learning, inductive inference, propositional expressions},
  url      = {https://doi.org/10.1145/1968.1972},
  urldate  = {2023-03-08},
}

@Article{Wolpert1996,
  author   = {Wolpert, David H.},
  journal  = {Neural Computation},
  title    = {{The Lack of A Priori Distinctions Between Learning Algorithms}},
  year     = {1996},
  issn     = {0899-7667},
  month    = {10},
  number   = {7},
  pages    = {1341-1390},
  volume   = {8},
  abstract = {{This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.}},
  doi      = {10.1162/neco.1996.8.7.1341},
  eprint   = {https://direct.mit.edu/neco/article-pdf/8/7/1341/813495/neco.1996.8.7.1341.pdf},
  url      = {https://doi.org/10.1162/neco.1996.8.7.1341},
}

@Book{Geron2019,
  author     = {Geron, Aurelien},
  publisher  = {O'Reilly Media, Inc.},
  title      = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: {Concepts}, {Tools}, and {Techniques} to {Build} {Intelligent} {Systems}},
  year       = {2019},
  edition    = {2nd},
  isbn       = {9781492032649},
  abstract   = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks-Scikit-Learn and TensorFlow-author Aurelien Geron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use Scikit-Learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the TensorFlow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets},
  shorttitle = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn}, {Keras}, and {TensorFlow}},
}

@TechReport{Rosenblatt1957,
  author                      = {Rosenblatt, F.},
  title                       = {The perceptron - A perceiving and recognizing automaton},
  year                        = {1957},
  address                     = {Ithaca, New York},
  month                       = {January},
  number                      = {85-460-1},
  school                      = {Cornell Aeronautical Laboratory},
  title_with_no_special_chars = {The Perceptron A perceiving and recognizing automaton},
}

@Book{Hebb1949,
  author               = {Hebb, Donald O.},
  publisher            = {Wiley},
  title                = {The organization of behavior: {A} neuropsychological theory},
  year                 = {1949},
  address              = {New York},
  isbn                 = {0-8058-4300-0},
  month                = jun,
  abstract             = {{Donald Hebb pioneered many current themes in
                 behavioural neuroscience. He saw psychology as a
                 biological science, but one in which the organization
                 of behaviour must remain the central concern. Through
                 penetrating theoretical concepts, including the "cell
                 assembly," "phase sequence," and "Hebb synapse," he
                 offered a way to bridge the gap between cells, circuits
                 and behaviour. He saw the brain as a dynamically
                 organized system of multiple distributed parts, with
                 roots that extend into foundations of development and
                 evolutionary heritage. He understood that behaviour, as
                 brain, can be sliced at various levels and that one of
                 our challenges is to bring these levels into both
                 conceptual and empirical register. He could move
                 between theory and fact with an ease that continues to
                 inspire both students and professional investigators.
                 Although facts continue to accumulate at an
                 accelerating rate in both psychology and neuroscience,
                 and although these facts continue to force revision in
                 the details of Hebb's earlier contributions, his
                 overall insistence that we look at behaviour and brain
                 together â within a dynamic, relational and
                 multilayered framework â remains. His work touches
                 upon current studies of population coding, contextual
                 factors in brain representations, synaptic plasticity,
                 developmental construction of brain/behaviour
                 relations, clinical syndromes, deterioration of
                 performance with age and disease, and the formal
                 construction of connectionist models. The collection of
                 papers in this volume represent these and related
                 themes that Hebb inspired. We also acknowledge our
                 appreciation for Don Hebb as teacher, colleague and
                 friend.}},
  added-at             = {2011-06-02T00:21:57.000+0200},
  biburl               = {https://www.bibsonomy.org/bibtex/26432ae617e6db0127c8b197bf760d99e/mhwombat},
  citeulike-article-id = {500649},
  citeulike-linkout-0  = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0805843000},
  citeulike-linkout-1  = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0805843000},
  citeulike-linkout-2  = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0805843000},
  citeulike-linkout-3  = {http://www.amazon.jp/exec/obidos/ASIN/0805843000},
  citeulike-linkout-4  = {http://www.amazon.co.uk/exec/obidos/ASIN/0805843000/citeulike00-21},
  citeulike-linkout-5  = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0805843000},
  citeulike-linkout-6  = {http://www.worldcat.org/isbn/0805843000},
  citeulike-linkout-7  = {http://books.google.com/books?vid=ISBN0805843000},
  citeulike-linkout-8  = {http://www.amazon.com/gp/search?keywords=0805843000\&index=books\&linkCode=qs},
  citeulike-linkout-9  = {http://www.librarything.com/isbn/0805843000},
  day                  = {15},
  file                 = {:neural_nets/Hebb 1949.pdf:PDF},
  groups               = {public},
  howpublished         = {Hardcover},
  interhash            = {ba8f8b92a0de2c83bdbcc9d742235a59},
  intrahash            = {6432ae617e6db0127c8b197bf760d99e},
  keywords             = {MSc checked network neural seminal},
  posted-at            = {2006-02-10 16:35:34},
  priority             = {2},
  timestamp            = {2016-07-12T19:25:30.000+0200},
  username             = {mhwombat},
}

@Book{Minsky1969,
  author    = {Minsky, M. and Papert, S.},
  publisher = {MIT Press},
  title     = {Perceptrons},
  year      = {1969},
  address   = {Cambridge, MA},
  added-at  = {2013-12-05T10:32:26.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/24587aec0472c41d00c38bf3e888304ba/prlz77},
  interhash = {9ad5b73f68093070d73e54312145eca2},
  intrahash = {4587aec0472c41d00c38bf3e888304ba},
  keywords  = {critic minsky papert perceptron},
  timestamp = {2013-12-05T10:32:26.000+0100},
}

@InBook{Rumelhart1987,
  author    = {Rumelhart, David E. and McClelland, James L.},
  pages     = {318-362},
  title     = {Learning Internal Representations by Error Propagation},
  year      = {1987},
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
}

@PhdThesis{Werbos1974,
  author               = {Werbos, P. J.},
  school               = {Harvard University},
  title                = {Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
  year                 = {1974},
  added-at             = {2008-02-26T11:58:58.000+0100},
  biburl               = {https://www.bibsonomy.org/bibtex/2b0644d7aa84be0df0f198d586d341843/schaul},
  citeulike-article-id = {2381655},
  description          = {idsia},
  interhash            = {4165e2708a0468e89f8305f21ee2c711},
  intrahash            = {b0644d7aa84be0df0f198d586d341843},
  keywords             = {juergen},
  priority             = {2},
  timestamp            = {2008-02-26T11:59:46.000+0100},
}

@Comment{jabref-meta: databaseType:bibtex;}
