@TechReport{Kingma2013,
  author   = {Kingma, Diederik P. and Welling, Max},
  title    = {Auto-{Encoding} {Variational} {Bayes}},
  year     = {2013},
  month    = dec,
  note     = {arXiv:1312.6114 [cs, stat] version: 1 type: article},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  annote   = {Comment: Fixes a typo in the abstract, no other changes},
  doi      = {10.48550/arXiv.1312.6114},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1312.6114v1.pdf:application/pdf},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1312.6114},
  urldate  = {2023-03-04},
}

@Book{Goodfellow2016,
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  title     = {Deep Learning},
  year      = {2016},
  note      = {\url{http://www.deeplearningbook.org}},
}

@Article{Charte2018,
  author     = {Charte, David and Charte, Francisco and García, Salvador and del Jesus, María J. and Herrera, Francisco},
  journal    = {Information Fusion},
  title      = {A practical tutorial on autoencoders for nonlinear feature fusion: {Taxonomy}, models, software and guidelines},
  year       = {2018},
  issn       = {15662535},
  month      = nov,
  note       = {arXiv:1801.01586 [cs]},
  pages      = {78--96},
  volume     = {44},
  abstract   = {Many of the existing machine learning algorithms, both supervised and unsupervised, depend on the quality of the input characteristics to generate a good model. The amount of these variables is also important, since performance tends to decline as the input dimensionality increases, hence the interest in using feature fusion techniques, able to produce feature sets that are more compact and higher level. A plethora of procedures to fuse original variables for producing new ones has been developed in the past decades. The most basic ones use linear combinations of the original variables, such as PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis), while others find manifold embeddings of lower dimensionality based on non-linear combinations, such as Isomap or LLE (Linear Locally Embedding) techniques. More recently, autoencoders (AEs) have emerged as an alternative to manifold learning for conducting nonlinear feature fusion. Dozens of AE models have been proposed lately, each with its own specific traits. Although many of them can be used to generate reduced feature sets through the fusion of the original ones, there also AEs designed with other applications in mind. The goal of this paper is to provide the reader with a broad view of what an AE is, how they are used for feature fusion, a taxonomy gathering a broad range of models, and how they relate to other classical techniques. In addition, a set of didactic guidelines on how to choose the proper AE for a given task is supplied, together with a discussion of the software tools available. Finally, two case studies illustrate the usage of AEs with datasets of handwritten digits and breast cancer.},
  doi        = {10.1016/j.inffus.2017.12.007},
  file       = {:Charte2018 - A Practical Tutorial on Autoencoders for Nonlinear Feature Fusion_ Taxonomy, Models, Software and Guidelines.pdf:PDF},
  keywords   = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  shorttitle = {A practical tutorial on autoencoders for nonlinear feature fusion},
  url        = {http://arxiv.org/abs/1801.01586},
  urldate    = {2023-03-04},
}

@Book{Phillips2021,
  author    = {Phillips, Jeff M.},
  publisher = {Springer International Publishing},
  title     = {Mathematical {Foundations} for {Data} {Analysis}},
  year      = {2021},
  isbn      = {9783030623401},
  month     = mar,
  note      = {Google-Books-ID: AUDYzQEACAAJ},
  abstract  = {This textbook, suitable for an early undergraduate up to a graduate course, provides an overview of many basic principles and techniques needed for modern data analysis. In particular, this book was designed and written as preparation for students planning to take rigorous Machine Learning and Data Mining courses. It introduces key conceptual tools necessary for data analysis, including concentration of measure and PAC bounds, cross validation, gradient descent, and principal component analysis. It also surveys basic techniques in supervised (regression and classification) and unsupervised learning (dimensionality reduction and clustering) through an accessible, simplified presentation. Students are recommended to have some background in calculus, probability, and linear algebra. Some familiarity with programming and algorithms is useful to understand advanced topics on computational techniques.},
  file      = {:Phillips2021 - Mathematical Foundations for Data Analysis.html:URL},
  keywords  = {Mathematics / Counting \& Numeration, Mathematics / Graphic Methods, Mathematics / Numerical Analysis, Mathematics / Probability \& Statistics / Stochastic Processes, Mathematics / Combinatorics},
  language  = {en},
}

@Book{Bellman1957,
  author    = {Bellman, Richard},
  publisher = {Princeton University Press},
  title     = {Dynamic {Programming}},
  year      = {1957},
  isbn      = {9780691079516},
  note      = {Google-Books-ID: wdtoPwAACAAJ},
  abstract  = {A multi-stage allocation process; A stochastic multi-stage decision process; The structure of dynamic programming processes; Existence and uniqueness theorems; The optimal inventory equation; Bottleneck problems in multi-stage production processes; Bottleneck problems; A continuous stochastic decision process; A new formalism in the calculus of variations; Multi-stages games; Markovian decision processes.},
  file      = {Google Books Link:https\://books.google.cz/books?id=wdtoPwAACAAJ:text/html},
  language  = {en},
}

@InProceedings{Liu2006,
  author    = {Weifeng Liu and Pokharel, P.P. and Principe, J.C.},
  booktitle = {The 2006 IEEE International Joint Conference on Neural Network Proceedings},
  title     = {Correntropy: A Localized Similarity Measure},
  year      = {2006},
  pages     = {4919-4924},
  doi       = {10.1109/IJCNN.2006.247192},
}

@Book{Stanczyk2015,
  editor    = {Stańczyk, Urszula and Jain, Lakhmi C.},
  publisher = {Springer Berlin Heidelberg},
  title     = {Feature {Selection} for {Data} and {Pattern} {Recognition}},
  year      = {2015},
  address   = {Berlin, Heidelberg},
  isbn      = {9783662456194 9783662456200},
  series    = {Studies in {Computational} {Intelligence}},
  volume    = {584},
  doi       = {10.1007/978-3-662-45620-0},
  language  = {en},
  url       = {https://link.springer.com/10.1007/978-3-662-45620-0},
  urldate   = {2023-03-05},
}

@Book{Liu1998,
  editor    = {Liu, Huan and Motoda, Hiroshi},
  publisher = {Springer US},
  title     = {Feature {Extraction}, {Construction} and {Selection}},
  year      = {1998},
  address   = {Boston, MA},
  isbn      = {9781461376224 9781461557258},
  doi       = {10.1007/978-1-4615-5725-8},
  file      = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2F978-1-4615-5725-8.pdf:application/pdf},
  keywords  = {algorithms, data analysis, data mining, genetic algorithms, knowledge discovery, learning, machine learning, robot},
  url       = {http://link.springer.com/10.1007/978-1-4615-5725-8},
  urldate   = {2023-03-05},
}

@Comment{jabref-meta: databaseType:bibtex;}
