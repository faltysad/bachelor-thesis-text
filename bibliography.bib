@Book{Goodfellow2016,
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  title     = {Deep Learning},
  year      = {2016},
  note      = {\url{http://www.deeplearningbook.org}},
}

@Article{Charte2018,
  author     = {Charte, David and Charte, Francisco and García, Salvador and del Jesus, María J. and Herrera, Francisco},
  journal    = {Information Fusion},
  title      = {A practical tutorial on autoencoders for nonlinear feature fusion: {Taxonomy}, models, software and guidelines},
  year       = {2018},
  issn       = {15662535},
  month      = nov,
  note       = {arXiv:1801.01586 [cs]},
  pages      = {78--96},
  volume     = {44},
  abstract   = {Many of the existing machine learning algorithms, both supervised and unsupervised, depend on the quality of the input characteristics to generate a good model. The amount of these variables is also important, since performance tends to decline as the input dimensionality increases, hence the interest in using feature fusion techniques, able to produce feature sets that are more compact and higher level. A plethora of procedures to fuse original variables for producing new ones has been developed in the past decades. The most basic ones use linear combinations of the original variables, such as PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis), while others find manifold embeddings of lower dimensionality based on non-linear combinations, such as Isomap or LLE (Linear Locally Embedding) techniques. More recently, autoencoders (AEs) have emerged as an alternative to manifold learning for conducting nonlinear feature fusion. Dozens of AE models have been proposed lately, each with its own specific traits. Although many of them can be used to generate reduced feature sets through the fusion of the original ones, there also AEs designed with other applications in mind. The goal of this paper is to provide the reader with a broad view of what an AE is, how they are used for feature fusion, a taxonomy gathering a broad range of models, and how they relate to other classical techniques. In addition, a set of didactic guidelines on how to choose the proper AE for a given task is supplied, together with a discussion of the software tools available. Finally, two case studies illustrate the usage of AEs with datasets of handwritten digits and breast cancer.},
  doi        = {10.1016/j.inffus.2017.12.007},
  file       = {:Charte2018 - A Practical Tutorial on Autoencoders for Nonlinear Feature Fusion_ Taxonomy, Models, Software and Guidelines.pdf:PDF},
  keywords   = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  shorttitle = {A practical tutorial on autoencoders for nonlinear feature fusion},
  url        = {http://arxiv.org/abs/1801.01586},
  urldate    = {2023-03-04},
}

@Book{Phillips2021,
  author    = {Phillips, Jeff M.},
  publisher = {Springer International Publishing},
  title     = {Mathematical {Foundations} for {Data} {Analysis}},
  year      = {2021},
  isbn      = {9783030623401},
  month     = mar,
  note      = {Google-Books-ID: AUDYzQEACAAJ},
  abstract  = {This textbook, suitable for an early undergraduate up to a graduate course, provides an overview of many basic principles and techniques needed for modern data analysis. In particular, this book was designed and written as preparation for students planning to take rigorous Machine Learning and Data Mining courses. It introduces key conceptual tools necessary for data analysis, including concentration of measure and PAC bounds, cross validation, gradient descent, and principal component analysis. It also surveys basic techniques in supervised (regression and classification) and unsupervised learning (dimensionality reduction and clustering) through an accessible, simplified presentation. Students are recommended to have some background in calculus, probability, and linear algebra. Some familiarity with programming and algorithms is useful to understand advanced topics on computational techniques.},
  file      = {:Phillips2021 - Mathematical Foundations for Data Analysis.html:URL},
  keywords  = {Mathematics / Counting \& Numeration, Mathematics / Graphic Methods, Mathematics / Numerical Analysis, Mathematics / Probability \& Statistics / Stochastic Processes, Mathematics / Combinatorics},
  language  = {en},
}

@Book{2010,
  title    = {Dynamic {Programming}},
  year     = {2010},
  isbn     = {9780691146683},
  month    = jul,
  language = {en},
  url      = {https://press.princeton.edu/books/paperback/9780691146683/dynamic-programming},
  urldate  = {2023-03-05},
}

@Book{Bellman1957,
  author    = {Bellman, Richard},
  publisher = {Princeton University Press},
  title     = {Dynamic {Programming}},
  year      = {1957},
  isbn      = {9780691079516},
  note      = {Google-Books-ID: wdtoPwAACAAJ},
  abstract  = {A multi-stage allocation process; A stochastic multi-stage decision process; The structure of dynamic programming processes; Existence and uniqueness theorems; The optimal inventory equation; Bottleneck problems in multi-stage production processes; Bottleneck problems; A continuous stochastic decision process; A new formalism in the calculus of variations; Multi-stages games; Markovian decision processes.},
  file      = {Google Books Link:https\://books.google.cz/books?id=wdtoPwAACAAJ:text/html},
  language  = {en},
}

@InProceedings{Liu2006,
  author    = {Weifeng Liu and Pokharel, P.P. and Principe, J.C.},
  booktitle = {The 2006 IEEE International Joint Conference on Neural Network Proceedings},
  title     = {Correntropy: A Localized Similarity Measure},
  year      = {2006},
  pages     = {4919-4924},
  doi       = {10.1109/IJCNN.2006.247192},
}

@Book{Stanczyk2015,
  editor    = {Stańczyk, Urszula and Jain, Lakhmi C.},
  publisher = {Springer Berlin Heidelberg},
  title     = {Feature {Selection} for {Data} and {Pattern} {Recognition}},
  year      = {2015},
  address   = {Berlin, Heidelberg},
  isbn      = {9783662456194 9783662456200},
  series    = {Studies in {Computational} {Intelligence}},
  volume    = {584},
  doi       = {10.1007/978-3-662-45620-0},
  language  = {en},
  url       = {https://link.springer.com/10.1007/978-3-662-45620-0},
  urldate   = {2023-03-05},
}

@Book{Liu1998,
  editor    = {Liu, Huan and Motoda, Hiroshi},
  publisher = {Springer US},
  title     = {Feature {Extraction}, {Construction} and {Selection}},
  year      = {1998},
  address   = {Boston, MA},
  isbn      = {9781461376224 9781461557258},
  doi       = {10.1007/978-1-4615-5725-8},
  file      = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2F978-1-4615-5725-8.pdf:application/pdf},
  keywords  = {algorithms, data analysis, data mining, genetic algorithms, knowledge discovery, learning, machine learning, robot},
  url       = {http://link.springer.com/10.1007/978-1-4615-5725-8},
  urldate   = {2023-03-05},
}

@Book{Mitchell1997,
  author    = {Mitchell, Tom M.},
  publisher = {McGraw-Hill},
  title     = {Machine Learning},
  year      = {1997},
  address   = {New York},
  isbn      = {978-0-07-042807-2},
  abstract  = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. This exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning---including probability and statistics, artificial intelligence, and neural networks---unifying them all in a logical and coherent manner.},
  added-at  = {2017-05-08T14:37:30.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/23e79734ee1a6e49aee02ffd108224d1c/flint63},
  file      = {eBook:1900-99/Mitchell97.pdf:PDF;McGraw-Hill Product page:http\://www.mhprofessional.com/product.php?isbn=0070428077:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/0070428077/:URL},
  groups    = {public},
  interhash = {479a66c32badb3a455fbdcf8e6633a5d},
  intrahash = {3e79734ee1a6e49aee02ffd108224d1c},
  keywords  = {01624 105 book shelf ai learn algorithm},
  timestamp = {2017-07-13T17:10:10.000+0200},
  username  = {flint63},
}

@Book{Chollet2017,
  author    = {Chollet, François},
  publisher = {Manning},
  title     = {Deep Learning with Python},
  year      = {2017},
  isbn      = {9781617294433},
  month     = nov,
  added-at  = {2018-08-01T08:16:18.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/231f94815ebbd65d3a31e4a69e818573e/jaeschke},
  interhash = {cfbfd3f93853a469e5e6978f61a74a0a},
  intrahash = {31f94815ebbd65d3a31e4a69e818573e},
  keywords  = {ai deep deeplearning learning ml},
  timestamp = {2021-05-19T08:35:34.000+0200},
}

@Article{Erhan2010,
  author   = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
  journal  = {Journal of Machine Learning Research},
  title    = {Why {Does} {Unsupervised} {Pre}-training {Help} {Deep} {Learning}?},
  year     = {2010},
  issn     = {1533-7928},
  number   = {19},
  pages    = {625--660},
  volume   = {11},
  abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
  file     = {Full Text PDF:http\://jmlr.org/papers/volume11/erhan10a/erhan10a.pdf:application/pdf},
  url      = {http://jmlr.org/papers/v11/erhan10a.html},
  urldate  = {2023-03-06},
}

@InProceedings{Ranzato2007,
  author   = {Ranzato, M.A. and Huang, Fu and Boureau, Y-Lan and Lecun, Yann},
  title    = {Unsupervised {Learning} of {Invariant} {Feature} {Hierarchies} with {Applications} to {Object} {Recognition}},
  year     = {2007},
  month    = jul,
  pages    = {1--8},
  abstract = {We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64\% error on MNIST, and 54\% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.},
  doi      = {10.1109/CVPR.2007.383157},
  file     = {Full Text PDF:https\://www.researchgate.net/profile/Yann-Lecun/publication/224716259_Unsupervised_Learning_of_Invariant_Feature_Hierarchies_with_Applications_to_Object_Recognition/links/0912f50f9e762562f3000000/Unsupervised-Learning-of-Invariant-Feature-Hierarchies-with-Applications-to-Object-Recognition.pdf:application/pdf;ResearchGate Link:https\://www.researchgate.net/publication/224716259_Unsupervised_Learning_of_Invariant_Feature_Hierarchies_with_Applications_to_Object_Recognition:},
}

@InProceedings{Bengio2006,
  author    = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Greedy {Layer}-{Wise} {Training} of {Deep} {Networks}},
  year      = {2006},
  publisher = {MIT Press},
  volume    = {19},
  abstract  = {Recent analyses (Bengio, Delalleau, \& Le Roux, 2006; Bengio \& Le Cun, 2007) of modern nonparametric machine learning algorithms that are kernel machines, such as Support Vector Machines (SVMs), graph-based manifold and semi-supervised learning algorithms suggest fundamental limitations of some learning algorithms. The problem is clear in kernel-based approaches when the kernel is "local" (e.g., the Gaussian kernel), i.e., K (x, y ) converges to a constant when {\textbar}{\textbar}x - y {\textbar}{\textbar} increases. These analyses point to the difficulty of learning "highly-varying functions", i.e., functions that have a large number of "variations" in the domain of interest, e.g., they would require a large number of pieces to be well represented by a piecewise-linear approximation. Since the number of pieces can be made to grow exponentially with the number of factors of variations in the input, this is connected with the well-known curse of dimensionality for classical non-parametric learning algorithms (for regression, classification and density estimation). If the shapes of all these pieces are unrelated, one needs enough examples for each piece in order to generalize properly. However, if these shapes are related and can be predicted from each other, "non-local" learning algorithms have the potential to generalize to pieces not covered by the training set. Such ability would seem necessary for learning in complex domains such as Artificial Intelligence tasks (e.g., related to vision, language, speech, robotics). Kernel machines (not only those with a local kernel) have a shallow architecture, i.e., only two levels of data-dependent computational elements. This is also true of feedforward neural networks with a single hidden layer (which can become SVMs when the number of hidden units becomes large (Bengio, Le Roux, Vincent, Delalleau, \& Marcotte, 2006)). A serious problem with shallow architectures is that they can be very inefficient in terms of the number of computational units (e.g., bases, hidden units), and thus in terms of required examples (Bengio \& Le Cun, 2007). One way to represent a highly-varying function compactly (with few parameters) is through the composition of many non-linearities, i.e., with a deep architecture. For example, the parity function with d inputs requires O(2d ) examples and parameters to be represented by a Gaussian SVM (Bengio et al., 2006), O(d2 ) parameters for a one-hidden-layer neural network, O(d) parameters and units for a multi-layer network with O(log2 d) layers, and O(1) parameters with a recurrent neural network. More generally,},
  file      = {Full Text PDF:https\://proceedings.neurips.cc/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf:application/pdf},
  url       = {https://proceedings.neurips.cc/paper/2006/hash/5da713a690c067105aeb2fae32403405-Abstract.html},
  urldate   = {2023-03-06},
}

@Article{Valiant1984,
  author   = {Valiant, L. G.},
  journal  = {Communications of the ACM},
  title    = {A theory of the learnable},
  year     = {1984},
  issn     = {0001-0782},
  month    = nov,
  number   = {11},
  pages    = {1134--1142},
  volume   = {27},
  doi      = {10.1145/1968.1972},
  file     = {Full Text PDF:https\://dl.acm.org/doi/pdf/10.1145/1968.1972:application/pdf},
  keywords = {probabilistic models of learning, inductive inference, propositional expressions},
  url      = {https://doi.org/10.1145/1968.1972},
  urldate  = {2023-03-08},
}

@Article{Wolpert1996,
  author   = {Wolpert, David H.},
  journal  = {Neural Computation},
  title    = {{The Lack of A Priori Distinctions Between Learning Algorithms}},
  year     = {1996},
  issn     = {0899-7667},
  month    = {10},
  number   = {7},
  pages    = {1341-1390},
  volume   = {8},
  abstract = {{This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.}},
  doi      = {10.1162/neco.1996.8.7.1341},
  eprint   = {https://direct.mit.edu/neco/article-pdf/8/7/1341/813495/neco.1996.8.7.1341.pdf},
  url      = {https://doi.org/10.1162/neco.1996.8.7.1341},
}

@Book{Geron2019,
  author     = {Geron, Aurelien},
  publisher  = {O'Reilly Media, Inc.},
  title      = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: {Concepts}, {Tools}, and {Techniques} to {Build} {Intelligent} {Systems}},
  year       = {2019},
  edition    = {2nd},
  isbn       = {9781492032649},
  abstract   = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks-Scikit-Learn and TensorFlow-author Aurelien Geron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use Scikit-Learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the TensorFlow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets},
  shorttitle = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn}, {Keras}, and {TensorFlow}},
}

@TechReport{Rosenblatt1957,
  author                      = {Rosenblatt, F.},
  title                       = {The perceptron - A perceiving and recognizing automaton},
  year                        = {1957},
  address                     = {Ithaca, New York},
  month                       = {January},
  number                      = {85-460-1},
  school                      = {Cornell Aeronautical Laboratory},
  title_with_no_special_chars = {The Perceptron A perceiving and recognizing automaton},
}

@Book{Hebb1949,
  author               = {Hebb, Donald O.},
  publisher            = {Wiley},
  title                = {The organization of behavior: {A} neuropsychological theory},
  year                 = {1949},
  address              = {New York},
  isbn                 = {0-8058-4300-0},
  month                = jun,
  abstract             = {{Donald Hebb pioneered many current themes in
                 behavioural neuroscience. He saw psychology as a
                 biological science, but one in which the organization
                 of behaviour must remain the central concern. Through
                 penetrating theoretical concepts, including the "cell
                 assembly," "phase sequence," and "Hebb synapse," he
                 offered a way to bridge the gap between cells, circuits
                 and behaviour. He saw the brain as a dynamically
                 organized system of multiple distributed parts, with
                 roots that extend into foundations of development and
                 evolutionary heritage. He understood that behaviour, as
                 brain, can be sliced at various levels and that one of
                 our challenges is to bring these levels into both
                 conceptual and empirical register. He could move
                 between theory and fact with an ease that continues to
                 inspire both students and professional investigators.
                 Although facts continue to accumulate at an
                 accelerating rate in both psychology and neuroscience,
                 and although these facts continue to force revision in
                 the details of Hebb's earlier contributions, his
                 overall insistence that we look at behaviour and brain
                 together â within a dynamic, relational and
                 multilayered framework â remains. His work touches
                 upon current studies of population coding, contextual
                 factors in brain representations, synaptic plasticity,
                 developmental construction of brain/behaviour
                 relations, clinical syndromes, deterioration of
                 performance with age and disease, and the formal
                 construction of connectionist models. The collection of
                 papers in this volume represent these and related
                 themes that Hebb inspired. We also acknowledge our
                 appreciation for Don Hebb as teacher, colleague and
                 friend.}},
  added-at             = {2011-06-02T00:21:57.000+0200},
  biburl               = {https://www.bibsonomy.org/bibtex/26432ae617e6db0127c8b197bf760d99e/mhwombat},
  citeulike-article-id = {500649},
  citeulike-linkout-0  = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0805843000},
  citeulike-linkout-1  = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0805843000},
  citeulike-linkout-2  = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0805843000},
  citeulike-linkout-3  = {http://www.amazon.jp/exec/obidos/ASIN/0805843000},
  citeulike-linkout-4  = {http://www.amazon.co.uk/exec/obidos/ASIN/0805843000/citeulike00-21},
  citeulike-linkout-5  = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0805843000},
  citeulike-linkout-6  = {http://www.worldcat.org/isbn/0805843000},
  citeulike-linkout-7  = {http://books.google.com/books?vid=ISBN0805843000},
  citeulike-linkout-8  = {http://www.amazon.com/gp/search?keywords=0805843000\&index=books\&linkCode=qs},
  citeulike-linkout-9  = {http://www.librarything.com/isbn/0805843000},
  day                  = {15},
  file                 = {:neural_nets/Hebb 1949.pdf:PDF},
  groups               = {public},
  howpublished         = {Hardcover},
  interhash            = {ba8f8b92a0de2c83bdbcc9d742235a59},
  intrahash            = {6432ae617e6db0127c8b197bf760d99e},
  keywords             = {MSc checked network neural seminal},
  posted-at            = {2006-02-10 16:35:34},
  priority             = {2},
  timestamp            = {2016-07-12T19:25:30.000+0200},
  username             = {mhwombat},
}

@Book{Minsky1969,
  author    = {Minsky, M. and Papert, S.},
  publisher = {MIT Press},
  title     = {Perceptrons},
  year      = {1969},
  address   = {Cambridge, MA},
  added-at  = {2013-12-05T10:32:26.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/24587aec0472c41d00c38bf3e888304ba/prlz77},
  interhash = {9ad5b73f68093070d73e54312145eca2},
  intrahash = {4587aec0472c41d00c38bf3e888304ba},
  keywords  = {critic minsky papert perceptron},
  timestamp = {2013-12-05T10:32:26.000+0100},
}

@InBook{Rumelhart1987,
  author    = {Rumelhart, David E. and McClelland, James L.},
  pages     = {318-362},
  title     = {Learning Internal Representations by Error Propagation},
  year      = {1987},
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
}

@PhdThesis{Werbos1974,
  author               = {Werbos, P. J.},
  school               = {Harvard University},
  title                = {Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
  year                 = {1974},
  added-at             = {2008-02-26T11:58:58.000+0100},
  biburl               = {https://www.bibsonomy.org/bibtex/2b0644d7aa84be0df0f198d586d341843/schaul},
  citeulike-article-id = {2381655},
  description          = {idsia},
  interhash            = {4165e2708a0468e89f8305f21ee2c711},
  intrahash            = {b0644d7aa84be0df0f198d586d341843},
  keywords             = {juergen},
  priority             = {2},
  timestamp            = {2008-02-26T11:59:46.000+0100},
}

@Article{Hornik1989,
  author  = {Kurt Hornik and Maxwell B. Stinchcombe and Halbert L. White},
  journal = {Neural Networks},
  title   = {Multilayer feedforward networks are universal approximators},
  year    = {1989},
  pages   = {359-366},
  volume  = {2},
}

@Article{Cybenko1989,
  author   = {Cybenko, G.},
  journal  = {Mathematics of Control, Signals and Systems},
  title    = {Approximation by superpositions of a sigmoidal function},
  year     = {1989},
  issn     = {1435-568X},
  month    = dec,
  number   = {4},
  pages    = {303--314},
  volume   = {2},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  doi      = {10.1007/BF02551274},
  file     = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2FBF02551274.pdf:application/pdf},
  language = {en},
  url      = {https://doi.org/10.1007/BF02551274},
  urldate  = {2023-03-08},
}

@Article{Hornik1990,
  author   = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal  = {Neural Networks},
  title    = {Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks},
  year     = {1990},
  issn     = {0893-6080},
  month    = jan,
  number   = {5},
  pages    = {551--560},
  volume   = {3},
  abstract = {We give conditions ensuring that multilayer feedforward networks with as few as a single hidden layer and an appropriately smooth hidden layer activation function are capable of arbitrarily accurate approximation to an arbitrary function and its derivatives. In fact, these networks can approximate functions that are not differentiable in the classical sense, but possess only a generalized derivative, as is the case for certain piecewise differentiable functions. The conditions imposed on the hidden layer activation function are relatively mild; the conditions imposed on the domain of the function to be approximated have practical implications. Our approximation results provide a previously missing theoretical justification for the use of multilayer feedforward networks in applications requiring simultaneous approximation of a function and its derivatives.},
  doi      = {10.1016/0893-6080(90)90005-6},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/0893608090900056/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords = {Approximation, Derivatives, Sobolev space, Feedforward networks},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/0893608090900056},
  urldate  = {2023-03-08},
}

 
@Book{Murphy2022,
  author    = {Kevin P. Murphy},
  publisher = {MIT Press},
  title     = {Probabilistic Machine Learning: An introduction},
  year      = {2022},
  url       = {probml.ai},
}

@Article{Kingma2019,
  author   = {Kingma, Diederik P. and Welling, Max},
  journal  = {Foundations and Trends® in Machine Learning},
  title    = {An {Introduction} to {Variational} {Autoencoders}},
  year     = {2019},
  issn     = {1935-8237, 1935-8245},
  note     = {arXiv:1906.02691 [cs, stat]},
  number   = {4},
  pages    = {307--392},
  volume   = {12},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  doi      = {10.1561/2200000056},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1906.02691.pdf:application/pdf},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  url      = {http://arxiv.org/abs/1906.02691},
  urldate  = {2023-03-09},
}

 
@Book{Murphy2023,
  author    = {Kevin P. Murphy},
  publisher = {MIT Press},
  title     = {Probabilistic Machine Learning: Advanced Topics},
  year      = {2023},
  url       = {http://probml.github.io/book2},
}

@Article{Kullback1951,
  author    = {Kullback, S. and Leibler, R. A.},
  journal   = {The Annals of Mathematical Statistics},
  title     = {On {Information} and {Sufficiency}},
  year      = {1951},
  issn      = {0003-4851},
  number    = {1},
  pages     = {79--86},
  volume    = {22},
  file      = {JSTOR Full Text PDF:https\://www.jstor.org/stable/pdfplus/10.2307/2236703.pdf?acceptTC=true:application/pdf},
  publisher = {Institute of Mathematical Statistics},
  url       = {https://www.jstor.org/stable/2236703},
  urldate   = {2023-03-09},
}

@Article{Samuel1967,
  author  = {Arthur L. Samuel},
  journal = {IBM J. Res. Dev.},
  title   = {Some Studies in Machine Learning Using the Game of Checkers},
  year    = {1967},
  pages   = {206-227},
  volume  = {44},
}

@Article{Hochreiter1998,
  author    = {Hochreiter, Sepp},
  journal   = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  title     = {The {Vanishing} {Gradient} {Problem} {During} {Learning} {Recurrent} {Neural} {Nets} and {Problem} {Solutions}},
  year      = {1998},
  issn      = {0218-4885},
  month     = apr,
  number    = {02},
  pages     = {107--116},
  volume    = {06},
  abstract  = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
  doi       = {10.1142/S0218488598000094},
  file      = {Full Text PDF:https\://www.worldscientific.com/doi/pdf/10.1142/S0218488598000094:application/pdf},
  keywords  = {Recurrent neural nets, vanishing gradient, long-term dependencies, long short-term memory},
  publisher = {World Scientific Publishing Co.},
  url       = {https://www.worldscientific.com/doi/abs/10.1142/S0218488598000094},
  urldate   = {2023-03-09},
}

@Article{LeCun1989,
  author   = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal  = {Neural Computation},
  title    = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
  year     = {1989},
  issn     = {0899-7667},
  month    = {12},
  number   = {4},
  pages    = {541-551},
  volume   = {1},
  abstract = {{The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.}},
  doi      = {10.1162/neco.1989.1.4.541},
  eprint   = {https://direct.mit.edu/neco/article-pdf/1/4/541/811941/neco.1989.1.4.541.pdf},
  url      = {https://doi.org/10.1162/neco.1989.1.4.541},
}

@Article{Baldi1989,
  author     = {Baldi, Pierre and Hornik, Kurt},
  journal    = {Neural Networks},
  title      = {Neural networks and principal component analysis: {Learning} from examples without local minima},
  year       = {1989},
  issn       = {0893-6080},
  month      = jan,
  number     = {1},
  pages      = {53--58},
  volume     = {2},
  abstract   = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.},
  doi        = {10.1016/0893-6080(89)90014-2},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/0893608089900142/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords   = {Neural networks, Principal component analysis, Learning, Back propagation},
  language   = {en},
  shorttitle = {Neural networks and principal component analysis},
  url        = {https://www.sciencedirect.com/science/article/pii/0893608089900142},
  urldate    = {2023-03-09},
}

@InProceedings{Kamyshanska2013,
  author  = {Kamyshanska, Hanna and Memisevic, Roland},
  title   = {On autoencoder scoring},
  year    = {2013},
  month   = {01},
  journal = {30th International Conference on Machine Learning, ICML 2013},
}

@Article{Olshausen1997,
  author     = {Olshausen, Bruno A. and Field, David J.},
  journal    = {Vision Research},
  title      = {Sparse coding with an overcomplete basis set: {A} strategy employed by {V1}?},
  year       = {1997},
  issn       = {0042-6989},
  month      = dec,
  number     = {23},
  pages      = {3311--3325},
  volume     = {37},
  abstract   = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
  doi        = {10.1016/S0042-6989(97)00169-7},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0042698997001697/pdf?md5=4da7cc24881b6697abdf982e3bf98d46&pid=1-s2.0-S0042698997001697-main.pdf&isDTMRedir=Y:application/pdf},
  keywords   = {Coding, V1, Gabor-wavelet, Natural images},
  language   = {en},
  shorttitle = {Sparse coding with an overcomplete basis set},
  url        = {https://www.sciencedirect.com/science/article/pii/S0042698997001697},
  urldate    = {2023-03-09},
}

@TechReport{Rifai2012,
  author   = {Rifai, Salah and Bengio, Yoshua and Dauphin, Yann and Vincent, Pascal},
  title    = {A {Generative} {Process} for {Sampling} {Contractive} {Auto}-{Encoders}},
  year     = {2012},
  month    = jun,
  note     = {arXiv:1206.6434 [cs, stat] type: article},
  abstract = {The contractive auto-encoder learns a representation of the input data that captures the local manifold structure around each data point, through the leading singular vectors of the Jacobian of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a high-density region of the input space. This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto-encoder. The associated stochastic process defines a distribution from which one can sample, and which experimentally appears to converge quickly and mix well between modes, compared to Restricted Boltzmann Machines and Deep Belief Networks. The intuitions behind this procedure can also be used to train the second layer of contraction that pools lower-level features and learns to be invariant to the local directions of variation discovered in the first layer. We show that this can help learn and represent invariances present in the data and improve classification error.},
  annote   = {Comment: Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)},
  doi      = {10.48550/arXiv.1206.6434},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1206.6434.pdf:application/pdf},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1206.6434},
  urldate  = {2023-03-09},
}

@TechReport{Goodfellow2014,
  author   = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  title    = {Generative {Adversarial} {Networks}},
  year     = {2014},
  month    = jun,
  note     = {arXiv:1406.2661 [cs, stat] type: article},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  doi      = {10.48550/arXiv.1406.2661},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1406.2661.pdf:application/pdf},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1406.2661},
  urldate  = {2023-03-09},
}

@Article{Kingma2014,
  author    = {Kingma, D. P. and Welling, M.},
  title     = {Auto-{Encoding} {Variational} {Bayes}},
  year      = {2014},
  file      = {Full Text PDF:https\://pure.uva.nl/ws/files/2511146/162970_1312.6114v10.pd.pdf:application/pdf},
  language  = {en},
  publisher = {Ithaca, NYarXiv.org},
  url       = {https://dare.uva.nl/search?identifier=cf65ba0f-d88f-4a49-8ebd-3a7fce86edd7},
  urldate   = {2023-03-24},
}

@TechReport{Rezende2014,
  author   = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  title    = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},
  year     = {2014},
  month    = may,
  note     = {arXiv:1401.4082 [cs, stat] type: article},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  annote   = {Comment: Appears In Proceedings of the 31st International Conference on Machine Learning (ICML), JMLR: W{\textbackslash}\&CP volume 32, 2014},
  doi      = {10.48550/arXiv.1401.4082},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1401.4082.pdf:application/pdf},
  keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Methodology},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1401.4082},
  urldate  = {2023-03-24},
}

@InCollection{Banerjee2007,
  author     = {Banerjee, Arindam},
  publisher  = {Society for Industrial and Applied Mathematics},
  title      = {An {Analysis} of {Logistic} {Models}: {Exponential} {Family} {Connections} and {Online} {Performance}},
  year       = {2007},
  isbn       = {9780898716306},
  month      = apr,
  pages      = {204--215},
  series     = {Proceedings},
  abstract   = {Logistic models are arguably one of the most widely used data analysis techniques. In this paper, we present analyses focussing on two important aspects of logistic models—its relationship with exponential family based generative models, and its performance in online and potentially adversarial settings. In particular, we present two new theoretical results on logistic models focusing on the above two aspects. First, we establish an exact connection between logistic models and exponential family based generative models, resolving a long-standing ambiguity over their relationship. Second, we show that online Bayesian logistic models are competitive to the best batch models, even in potentially adversarial settings. Further, we discuss relevant connections of our analysis to the literature on integral transforms, and also present a new optimality result for Bayesian models. The analysis makes a strong case for using logistic models and partly explains the success of such models for a wide range of practical problems.},
  doi        = {10.1137/1.9781611972771.19},
  file       = {Full Text PDF:https\://epubs.siam.org/doi/pdf/10.1137/1.9781611972771.19:application/pdf},
  shorttitle = {An {Analysis} of {Logistic} {Models}},
  url        = {https://epubs.siam.org/doi/abs/10.1137/1.9781611972771.19},
  urldate    = {2023-03-25},
}

@Article{Soenderby2016,
  author = {Sønderby, Casper and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren and Winther, Ole},
  title  = {How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks},
  year   = {2016},
  month  = {02},
}

@Article{Dayan1995,
  author   = {Dayan, P. and Hinton, G. E. and Neal, R. M. and Zemel, R. S.},
  journal  = {Neural Computation},
  title    = {The {Helmholtz} machine},
  year     = {1995},
  issn     = {0899-7667},
  month    = sep,
  number   = {5},
  pages    = {889--904},
  volume   = {7},
  abstract = {Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.},
  doi      = {10.1162/neco.1995.7.5.889},
  file     = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/7584891:text/html},
  keywords = {Algorithms, Feedback, Humans, Models, Psychological, Pattern Recognition, Automated, Pattern Recognition, Visual, Perception, Stochastic Processes},
  language = {eng},
  pmid     = {7584891},
}

@Misc{LeCun2022,
  author   = {Yann LeCun},
  month    = jun,
  title    = {A {Path} {Towards} {Autonomous} {Machine} {Intelligence}},
  year     = {2022},
  abstract = {How could machines learn as efficiently as humans and animals?  How could machines learn to reason and plan?  How could machines learn representations of percepts and action plans at multiple...},
  journal  = {OpenReview},
  language = {en},
  url      = {https://openreview.net/forum?id=BZ5a1r-kVsf},
  urldate  = {2023-03-25},
}

@TechReport{Bengio2014,
  author     = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  title      = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
  year       = {2014},
  month      = apr,
  note       = {arXiv:1206.5538 [cs] type: article},
  abstract   = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  doi        = {10.48550/arXiv.1206.5538},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1206.5538.pdf:application/pdf},
  keywords   = {Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {Representation {Learning}},
  url        = {http://arxiv.org/abs/1206.5538},
  urldate    = {2023-03-25},
}

@Comment{jabref-meta: databaseType:bibtex;}
